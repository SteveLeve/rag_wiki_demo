{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836cfd7d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. ✅ Run `foundation/00-setup-postgres-schema.ipynb` to create tables\n",
    "2. ✅ Run `foundation/02-rag-postgresql-persistent.ipynb` to generate embeddings\n",
    "3. ✅ Run `evaluation-lab/01-create-ground-truth-human-in-loop.ipynb` to create test queries\n",
    "\n",
    "These setup notebooks will populate the registry and create ground-truth test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08614e2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these parameters to experiment with different strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb313a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDING_MODEL_ALIAS = \"all-minilm-l6-v2\"  # From registry\n",
    "RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # HuggingFace model\n",
    "TOP_K_INITIAL = 20  # How many initial results to get\n",
    "TOP_K_FINAL = 5     # How many to rerank and return\n",
    "RERANKING_BATCH_SIZE = 32\n",
    "\n",
    "# Experiment tracking\n",
    "EXPERIMENT_NAME = \"reranking-cross-encoder\"\n",
    "TECHNIQUES_APPLIED = [\"vector_retrieval\", \"cross_encoder_reranking\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc518c1",
   "metadata": {},
   "source": [
    "## Load Embeddings from Registry\n",
    "\n",
    "This section demonstrates the registry discovery pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c4497",
   "metadata": {},
   "outputs": [],
   "source": "import psycopg2\nimport psycopg2.extras\nimport ollama\nimport json\nimport pandas as pd\nimport numpy as np\nimport hashlib\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Optional\nimport os\n\n# PostgreSQL connection\nPOSTGRES_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'postgres',\n    'password': 'postgres',\n}\n\n# Create database connection\ntry:\n    db_connection = psycopg2.connect(\n        host=POSTGRES_CONFIG['host'],\n        port=POSTGRES_CONFIG['port'],\n        database=POSTGRES_CONFIG['database'],\n        user=POSTGRES_CONFIG['user'],\n        password=POSTGRES_CONFIG['password']\n    )\n    print(\"✓ Connected to PostgreSQL\")\nexcept psycopg2.OperationalError as e:\n    print(f\"✗ Failed to connect to PostgreSQL: {e}\")\n    raise\n\n# ============================================================================\n# PART 1: REGISTRY DISCOVERY & LOAD-OR-GENERATE PATTERN\n# ============================================================================\n# Inline from foundation/00-registry-and-tracking-utilities.ipynb\n\ndef list_available_embeddings(db_connection) -> pd.DataFrame:\n    \"\"\"Query embedding_registry to show available models with metadata.\n\n    Returns:\n        DataFrame with columns: model_alias, model_name, dimension, embedding_count,\n                                 chunk_source_dataset, created_at, chunk_size_config\n    \"\"\"\n    query = '''\n        SELECT\n            model_alias,\n            model_name,\n            dimension,\n            embedding_count,\n            chunk_source_dataset,\n            chunk_size_config,\n            created_at,\n            last_accessed\n        FROM embedding_registry\n        ORDER BY created_at DESC\n    '''\n    return pd.read_sql(query, db_connection)\n\n\ndef get_embedding_metadata(db_connection, model_alias: str) -> Optional[Dict]:\n    \"\"\"Fetch metadata_json and other info for a specific model.\n\n    Args:\n        db_connection: PostgreSQL connection\n        model_alias: The model alias (e.g., 'bge_base_en_v1_5')\n\n    Returns:\n        Dict with: dimension, embedding_count, config_hash (if stored),\n                   chunk_source_dataset, created_at, metadata_json\n    \"\"\"\n    with db_connection.cursor() as cur:\n        cur.execute('''\n            SELECT\n                dimension,\n                embedding_count,\n                chunk_source_dataset,\n                chunk_size_config,\n                created_at,\n                metadata_json\n            FROM embedding_registry\n            WHERE model_alias = %s\n        ''', (model_alias,))\n        result = cur.fetchone()\n\n        if not result:\n            return None\n\n        return {\n            'dimension': result[0],\n            'embedding_count': result[1],\n            'chunk_source_dataset': result[2],\n            'chunk_size_config': result[3],\n            'created_at': result[4],\n            'metadata_json': result[5] or {}\n        }\n\n\nclass PostgreSQLVectorDB:\n    \"\"\"Helper to load embeddings from PostgreSQL without regeneration.\"\"\"\n\n    def __init__(self, config, table_name, preserve_existing=True):\n        self.config = config\n        self.table_name = table_name\n        self.conn = psycopg2.connect(\n            host=config['host'],\n            port=config['port'],\n            database=config['database'],\n            user=config['user'],\n            password=config['password']\n        )\n        print(f'✓ Connected to table: {table_name}')\n\n    def get_chunk_count(self):\n        \"\"\"How many embeddings are stored?\"\"\"\n        with self.conn.cursor() as cur:\n            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n            return cur.fetchone()[0]\n\n    def similarity_search(self, query_embedding, top_n=5):\n        \"\"\"Retrieve most similar chunks using pgvector.\n\n        Args:\n            query_embedding: Query embedding vector\n            top_n: Number of results to return\n\n        Returns:\n            List of tuples: (chunk_text, similarity_score, chunk_id)\n        \"\"\"\n        with self.conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n            cur.execute(f'''\n                SELECT id,\n                       content as chunk_text,\n                       1 - (embedding <=> %s::vector) as similarity\n                FROM {self.table_name}\n                ORDER BY embedding <=> %s::vector\n                LIMIT %s\n            ''', (query_embedding, query_embedding, top_n))\n\n            results = cur.fetchall()\n            return [(row['chunk_text'], row['similarity'], row['id']) for row in results]\n\n    def close(self):\n        if self.conn:\n            self.conn.close()\n\n\ndef load_or_generate(db_connection, embedding_model_alias, preserve_existing=True):\n    \"\"\"Load embeddings from registry OR show instructions if not available.\n\n    This is the CORE PATTERN for fast iteration: check registry first,\n    load existing embeddings instantly (<1 second), avoid 50+ minute regeneration.\n\n    Args:\n        db_connection: PostgreSQL connection object\n        embedding_model_alias: Model identifier (e.g., 'all_minilm_l6_v2')\n        preserve_existing: If True, always load. If False, regenerate (requires manual run).\n                          If None, prompt user interactively.\n\n    Returns:\n        PostgreSQLVectorDB instance ready for use, or None if no embeddings available\n    \"\"\"\n\n    print(f\"\\n{'='*70}\")\n    print(f\"Checking for embeddings: '{embedding_model_alias}'...\")\n    print(f\"{'='*70}\\n\")\n\n    try:\n        with db_connection.cursor() as cur:\n            cur.execute('''\n                SELECT id, dimension, embedding_count, created_at, metadata_json\n                FROM embedding_registry\n                WHERE model_alias = %s\n            ''', (embedding_model_alias,))\n            registry_entry = cur.fetchone()\n    except Exception as e:\n        print(f\"Could not query registry: {e}\")\n        print(\"Make sure foundation/00-setup-postgres-schema.ipynb has been run.\")\n        return None\n\n    # Case A: Embeddings exist\n    if registry_entry:\n        reg_id, dimension, embedding_count, created_at, metadata_json = registry_entry\n\n        print(f\"✓ FOUND EXISTING EMBEDDINGS\")\n        print(f\"  Model:      {embedding_model_alias}\")\n        print(f\"  Count:      {embedding_count:,} embeddings\")\n        print(f\"  Dimension:  {dimension}\")\n        print(f\"  Created:    {created_at}\")\n        print(f\"\\n  TIME SAVINGS:\")\n        print(f\"    Loading:       <1 second\")\n        print(f\"    Regenerating:  ~50+ minutes\")\n        print(f\"    ➜ You save 50+ minutes by loading!\\n\")\n\n        if preserve_existing:\n            # Auto-load (for scripts/notebooks)\n            print(\"Loading existing embeddings...\\n\")\n\n            try:\n                table_name = f'embeddings_{embedding_model_alias.replace(\".\", \"_\")}'\n\n                db_instance = PostgreSQLVectorDB(\n                    config=POSTGRES_CONFIG,\n                    table_name=table_name,\n                    preserve_existing=True\n                )\n\n                count = db_instance.get_chunk_count()\n                print(f\"✓ LOADED SUCCESSFULLY\")\n                print(f\"  Embeddings: {count:,}\")\n                print(f\"  Table: {table_name}\")\n                print(f\"  Status: Ready for retrieval\\n\")\n\n                return db_instance\n\n            except Exception as e:\n                print(f\"\\n✗ Error loading embeddings: {e}\")\n                print(f\"\\nTroubleshooting:\")\n                print(f\"  1. Verify PostgreSQL is running\")\n                print(f\"  2. Check POSTGRES_CONFIG settings\")\n                print(f\"  3. Run foundation/02 to generate embeddings first\")\n                return None\n\n    # Case B: No embeddings found\n    else:\n        print(f\"✗ NO EMBEDDINGS FOUND\")\n        print(f\"  Model: {embedding_model_alias}\")\n        print(f\"\\nTo create embeddings, run:\")\n        print(f\"  foundation/02-rag-postgresql-persistent.ipynb\")\n        print(f\"\\nThen come back and re-run this cell.\\n\")\n        return None\n\n\n# Discover and load embeddings\nprint(\"Step 1: Discovering available embeddings...\\n\")\navailable = list_available_embeddings(db_connection)\n\nif available.empty:\n    print(\"⚠️  No embeddings found in registry yet.\")\n    print(\"Run foundation/02-rag-postgresql-persistent.ipynb first.\\n\")\nelse:\n    print(\"Available embeddings:\")\n    print(available.to_string(index=False))\n    print()\n\n# Load embeddings using the pattern\nprint(\"\\nStep 2: Loading embeddings using load-or-generate pattern...\\n\")\nembeddings_db = load_or_generate(\n    db_connection=db_connection,\n    embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n    preserve_existing=True  # Auto-load if available\n)\n\nif embeddings_db:\n    print(\"✓ Success! Embeddings loaded and ready for retrieval.\")\nelse:\n    print(\"⚠️  Could not load embeddings. See instructions above.\")\n    # Continue anyway for structure, but operations will fail\n    embeddings_db = None"
  },
  {
   "cell_type": "markdown",
   "id": "463b19f4",
   "metadata": {},
   "source": [
    "## Implement Cross-Encoder Reranking\n",
    "\n",
    "Core reranking implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688bd17",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 2: CROSS-ENCODER RERANKING IMPLEMENTATION\n# ============================================================================\n\nfrom sentence_transformers import CrossEncoder\n\ndef rerank_with_crossencoder(query: str, \n                             candidates: List[Tuple[str, float, int]], \n                             reranker_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n                             top_k: int = 5) -> List[Tuple[str, float, int]]:\n    \"\"\"Two-stage retrieval: dense retrieval → cross-encoder reranking.\n\n    Stage 1: Dense retrieval (already done - these are candidates)\n    Stage 2: Cross-encoder scoring (more accurate, slower)\n\n    Args:\n        query: User question\n        candidates: List of (chunk_text, initial_score, chunk_id) from vector search\n        reranker_model: CrossEncoder model name\n        top_k: Return top K after reranking\n\n    Returns:\n        List of (chunk_text, rerank_score, chunk_id) sorted by rerank score\n    \"\"\"\n    if not candidates:\n        return []\n\n    # Load cross-encoder model\n    print(f\"Loading cross-encoder model: {reranker_model}\")\n    model = CrossEncoder(reranker_model)\n\n    # Create query-document pairs for the cross-encoder\n    chunk_texts = [chunk_text for chunk_text, _, _ in candidates]\n    pairs = [[query, chunk_text] for chunk_text in chunk_texts]\n\n    # Score all pairs with cross-encoder (more accurate but slower than dense retrieval)\n    print(f\"Scoring {len(pairs)} candidates with cross-encoder...\")\n    scores = model.predict(pairs)\n\n    # Combine original data with reranking scores\n    reranked = [\n        (chunk_text, float(score), chunk_id)\n        for (chunk_text, _, chunk_id), score in zip(candidates, scores)\n    ]\n\n    # Sort by rerank score (descending)\n    reranked.sort(key=lambda x: x[1], reverse=True)\n\n    return reranked[:top_k]\n\n\ndef retrieve_with_reranking(query: str,\n                           embeddings_db: PostgreSQLVectorDB,\n                           embedding_model: str,\n                           top_k_initial: int = 20,\n                           top_k_final: int = 5,\n                           reranker_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2') -> List[Tuple[str, float, int]]:\n    \"\"\"Complete retrieval pipeline with reranking.\n\n    Args:\n        query: User question\n        embeddings_db: PostgreSQLVectorDB instance\n        embedding_model: Model name for embedding the query\n        top_k_initial: How many candidates to get from dense retrieval\n        top_k_final: How many to return after reranking\n        reranker_model: Cross-encoder model to use\n\n    Returns:\n        List of top_k_final results after reranking\n    \"\"\"\n    # Step 1: Dense retrieval (broad recall from vector similarity)\n    print(f\"\\nStep 1: Dense retrieval (getting top {top_k_initial} candidates)...\")\n    query_emb = ollama.embed(model=embedding_model, input=query)['embeddings'][0]\n    candidates = embeddings_db.similarity_search(query_emb, top_n=top_k_initial)\n    print(f\"✓ Retrieved {len(candidates)} candidates via dense retrieval\")\n\n    # Step 2: Cross-encoder reranking (high precision, slower)\n    print(f\"\\nStep 2: Cross-encoder reranking (selecting top {top_k_final})...\")\n    reranked = rerank_with_crossencoder(query, candidates, reranker_model, top_k=top_k_final)\n    print(f\"✓ Reranked to top {len(reranked)} results\")\n\n    return reranked\n\n\n# Load test questions from ground truth\nprint(\"\\nLoading ground truth test set...\")\nground_truth_questions = []\n\nwith db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n    cur.execute('''\n        SELECT\n            id,\n            question,\n            relevant_chunk_ids,\n            quality_rating,\n            source_type\n        FROM evaluation_groundtruth\n        WHERE quality_rating = 'good'\n        ORDER BY id\n    ''')\n\n    for row in cur.fetchall():\n        ground_truth_questions.append({\n            'id': row['id'],\n            'question': row['question'],\n            'relevant_chunk_ids': row['relevant_chunk_ids'] or [],\n            'quality_rating': row['quality_rating'],\n            'source_type': row['source_type']\n        })\n\nprint(f\"✓ Loaded {len(ground_truth_questions)} ground truth questions\\n\")\n\nif ground_truth_questions:\n    print(f\"Sample question: {ground_truth_questions[0]['question'][:80]}...\")\n    print(f\"Relevant chunks: {ground_truth_questions[0]['relevant_chunk_ids'][:3]}\\n\")\n\n# Demonstrate reranking on first test question (if available)\nif embeddings_db and ground_truth_questions:\n    print(\"=\" * 70)\n    print(\"DEMONSTRATING RERANKING\")\n    print(\"=\" * 70)\n\n    test_question = ground_truth_questions[0]['question']\n    print(f\"\\nQuery: {test_question}\\n\")\n\n    # Baseline: vector-only retrieval\n    print(\"BASELINE (Vector Retrieval Only):\")\n    query_emb = ollama.embed(model=EMBEDDING_MODEL_ALIAS, input=test_question)['embeddings'][0]\n    baseline_results = embeddings_db.similarity_search(query_emb, top_n=TOP_K_FINAL)\n\n    print(\"Top 5 results (vector similarity only):\")\n    for i, (chunk_text, score, chunk_id) in enumerate(baseline_results, 1):\n        preview = chunk_text[:100].replace('\\n', ' ') + '...'\n        print(f\"  [{i}] (score: {score:.4f}) {preview}\")\n\n    # With reranking\n    print(\"\\nWITH RERANKING (CrossEncoder):\")\n    reranked_results = retrieve_with_reranking(\n        test_question,\n        embeddings_db,\n        EMBEDDING_MODEL_ALIAS,\n        top_k_initial=TOP_K_INITIAL,\n        top_k_final=TOP_K_FINAL,\n        reranker_model=RERANKER_MODEL\n    )\n\n    print(\"Top 5 results (after cross-encoder reranking):\")\n    for i, (chunk_text, score, chunk_id) in enumerate(reranked_results, 1):\n        preview = chunk_text[:100].replace('\\n', ' ') + '...'\n        print(f\"  [{i}] (rerank score: {score:.4f}) {preview}\")\n\n    print(\"\\nNote: Reranking may reorder results, prioritizing semantic relevance over surface similarity.\\n\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "7aa9e315",
   "metadata": {},
   "source": [
    "## Evaluate Impact\n",
    "\n",
    "Compare reranked results to baseline vector retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7acb5",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 3: EVALUATE IMPACT - BASELINE VS RERANKING\n# ============================================================================\n# Inline metric functions from evaluation-lab/02-evaluation-metrics-framework.ipynb\n\ndef precision_at_k(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int], k: int = 5) -> float:\n    \"\"\"Precision@K: What % of top-K results are relevant?\n    \n    Formula: |{relevant in top-K}| / K\n    \n    Good for: Understanding result quality from user perspective\n    \"\"\"\n    if k == 0:\n        return 0.0\n\n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n\n    num_relevant_in_k = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n\n    return num_relevant_in_k / k\n\n\ndef recall_at_k(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int], k: int = 5) -> float:\n    \"\"\"Recall@K: What % of all relevant chunks were found in top-K?\n    \n    Formula: |{relevant in top-K}| / |all relevant|\n    \n    Good for: Understanding coverage\n    \"\"\"\n    if len(relevant_chunk_ids) == 0:\n        return 0.0\n\n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n\n    num_relevant_found = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n\n    return num_relevant_found / len(relevant_set)\n\n\ndef mean_reciprocal_rank(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int]) -> float:\n    \"\"\"MRR: How quickly do we find the first relevant result?\n    \n    Formula: 1 / (rank of first relevant result)\n    \n    Good for: Understanding user satisfaction\n    \"\"\"\n    relevant_set = set(relevant_chunk_ids)\n\n    for rank, chunk_id in enumerate(retrieved_chunk_ids, start=1):\n        if chunk_id in relevant_set:\n            return 1.0 / rank\n\n    return 0.0\n\n\ndef ndcg_at_k(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int], k: int = 5) -> float:\n    \"\"\"NDCG@K: Normalized Discounted Cumulative Gain\n    \n    How well-ranked are the results? (rewards relevant results at top)\n    \n    Good for: Understanding ranking quality\n    \"\"\"\n    def dcg_score(relevance_scores: List[float]) -> float:\n        \"\"\"Compute DCG from relevance scores.\"\"\"\n        return sum(\n            (2**rel - 1) / np.log2(rank + 2)\n            for rank, rel in enumerate(relevance_scores)\n        )\n\n    if k == 0 or len(relevant_chunk_ids) == 0:\n        return 0.0\n\n    # Get top-K retrieved\n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n\n    # Binary relevance: 1 if relevant, 0 if not\n    relevance = [1 if chunk_id in relevant_set else 0 for chunk_id in retrieved_k]\n\n    # Compute DCG for retrieved ranking\n    dcg = dcg_score(relevance)\n\n    # Compute ideal DCG (perfect ranking)\n    ideal_relevance = sorted(relevance, reverse=True)\n    idcg = dcg_score(ideal_relevance)\n\n    if idcg == 0:\n        return 0.0\n\n    return dcg / idcg\n\n\ndef evaluate_retrieval_with_and_without_reranking(test_questions: List[Dict],\n                                                   embeddings_db: PostgreSQLVectorDB,\n                                                   embedding_model: str,\n                                                   reranker_model: str = RERANKER_MODEL) -> Dict:\n    \"\"\"Compare baseline vs reranking on test set.\n\n    Args:\n        test_questions: List of dicts from evaluation_groundtruth table\n        embeddings_db: PostgreSQLVectorDB instance\n        embedding_model: Model name for query embedding\n        reranker_model: Cross-encoder model for reranking\n\n    Returns:\n        dict: {\n            'baseline': {metric: value},\n            'reranked': {metric: value},\n            'improvements_pct': {metric: improvement_percentage},\n            'per_query': [per-query results]\n        }\n    \"\"\"\n    baseline_results = []\n    reranked_results = []\n    per_query_metrics = []\n\n    print(f\"\\nEvaluating {len(test_questions)} test queries...\")\n    print(\"=\" * 70)\n\n    for i, q in enumerate(test_questions, 1):\n        query = q['question']\n        relevant_ids = q['relevant_chunk_ids']\n\n        if not relevant_ids:\n            print(f\"Skipping query {i} (no relevant chunks)\")\n            continue\n\n        # Progress indicator\n        if i % max(1, len(test_questions) // 10) == 0:\n            print(f\"Progress: {i}/{len(test_questions)} queries processed...\")\n\n        # ================================================================\n        # BASELINE: Vector-only retrieval (no reranking)\n        # ================================================================\n        query_emb = ollama.embed(model=embedding_model, input=query)['embeddings'][0]\n        baseline_chunks = embeddings_db.similarity_search(query_emb, top_n=TOP_K_FINAL)\n        baseline_ids = [chunk_id for _, _, chunk_id in baseline_chunks]\n\n        # ================================================================\n        # WITH RERANKING: Dense retrieval + cross-encoder reranking\n        # ================================================================\n        reranked_chunks = retrieve_with_reranking(\n            query,\n            embeddings_db,\n            embedding_model,\n            top_k_initial=TOP_K_INITIAL,\n            top_k_final=TOP_K_FINAL,\n            reranker_model=reranker_model\n        )\n        reranked_ids = [chunk_id for _, _, chunk_id in reranked_chunks]\n\n        # ================================================================\n        # COMPUTE METRICS\n        # ================================================================\n        baseline_metrics = {\n            'precision@5': precision_at_k(baseline_ids, relevant_ids, k=5),\n            'recall@5': recall_at_k(baseline_ids, relevant_ids, k=5),\n            'mrr': mean_reciprocal_rank(baseline_ids, relevant_ids),\n            'ndcg@5': ndcg_at_k(baseline_ids, relevant_ids, k=5)\n        }\n\n        reranked_metrics = {\n            'precision@5': precision_at_k(reranked_ids, relevant_ids, k=5),\n            'recall@5': recall_at_k(reranked_ids, relevant_ids, k=5),\n            'mrr': mean_reciprocal_rank(reranked_ids, relevant_ids),\n            'ndcg@5': ndcg_at_k(reranked_ids, relevant_ids, k=5)\n        }\n\n        baseline_results.append(baseline_metrics)\n        reranked_results.append(reranked_metrics)\n\n        # Store per-query details\n        per_query_metrics.append({\n            'question': query[:80],\n            'baseline': baseline_metrics,\n            'reranked': reranked_metrics\n        })\n\n    print(\"=\" * 70)\n    print(f\"✓ Evaluation complete ({len(baseline_results)} queries)\\n\")\n\n    # ================================================================\n    # AGGREGATE METRICS\n    # ================================================================\n    def aggregate(results):\n        if not results:\n            return {'precision@5': 0, 'recall@5': 0, 'mrr': 0, 'ndcg@5': 0}\n        return {\n            metric: np.mean([r[metric] for r in results])\n            for metric in results[0].keys()\n        }\n\n    baseline_agg = aggregate(baseline_results)\n    reranked_agg = aggregate(reranked_results)\n\n    # ================================================================\n    # COMPUTE IMPROVEMENTS\n    # ================================================================\n    improvements = {}\n    for metric in baseline_agg.keys():\n        if baseline_agg[metric] > 0:\n            improvement_pct = (\n                (reranked_agg[metric] - baseline_agg[metric]) / baseline_agg[metric] * 100\n            )\n        else:\n            improvement_pct = 0\n        improvements[metric] = improvement_pct\n\n    return {\n        'baseline': baseline_agg,\n        'reranked': reranked_agg,\n        'improvements_pct': improvements,\n        'per_query': per_query_metrics,\n        'num_queries_evaluated': len(baseline_results)\n    }\n\n\n# Run evaluation\nprint(\"\\n\" + \"=\" * 70)\nprint(\"EVALUATING RERANKING IMPACT\")\nprint(\"=\" * 70)\n\nif embeddings_db and ground_truth_questions:\n    evaluation_results = evaluate_retrieval_with_and_without_reranking(\n        ground_truth_questions,\n        embeddings_db,\n        EMBEDDING_MODEL_ALIAS,\n        reranker_model=RERANKER_MODEL\n    )\n\n    # Display results\n    print(\"\\n\" + \"=\" * 70)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\" * 70)\n\n    baseline = evaluation_results['baseline']\n    reranked = evaluation_results['reranked']\n    improvements = evaluation_results['improvements_pct']\n\n    print(f\"\\nQueries Evaluated: {evaluation_results['num_queries_evaluated']}\\n\")\n\n    print(\"BASELINE (Vector Retrieval Only):\")\n    print(f\"  Precision@5: {baseline['precision@5']:.4f}\")\n    print(f\"  Recall@5:    {baseline['recall@5']:.4f}\")\n    print(f\"  MRR:         {baseline['mrr']:.4f}\")\n    print(f\"  NDCG@5:      {baseline['ndcg@5']:.4f}\")\n\n    print(\"\\nWITH RERANKING (CrossEncoder):\")\n    print(f\"  Precision@5: {reranked['precision@5']:.4f}\")\n    print(f\"  Recall@5:    {reranked['recall@5']:.4f}\")\n    print(f\"  MRR:         {reranked['mrr']:.4f}\")\n    print(f\"  NDCG@5:      {reranked['ndcg@5']:.4f}\")\n\n    print(\"\\nIMPROVEMENTS:\")\n    for metric, improvement in improvements.items():\n        sign = \"+\" if improvement > 0 else \"\"\n        print(f\"  {metric:<15} {sign}{improvement:>6.2f}%\")\n\n    # Visualize per-query improvements\n    print(\"\\n\" + \"=\" * 70)\n    print(\"PER-QUERY ANALYSIS\")\n    print(\"=\" * 70)\n\n    import matplotlib.pyplot as plt\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    fig.suptitle('Reranking Impact: Baseline vs CrossEncoder', fontsize=14, fontweight='bold')\n\n    # Extract per-query metrics\n    baseline_p5 = [q['baseline']['precision@5'] for q in evaluation_results['per_query']]\n    reranked_p5 = [q['reranked']['precision@5'] for q in evaluation_results['per_query']]\n\n    baseline_recall = [q['baseline']['recall@5'] for q in evaluation_results['per_query']]\n    reranked_recall = [q['reranked']['recall@5'] for q in evaluation_results['per_query']]\n\n    baseline_mrr = [q['baseline']['mrr'] for q in evaluation_results['per_query']]\n    reranked_mrr = [q['reranked']['mrr'] for q in evaluation_results['per_query']]\n\n    baseline_ndcg = [q['baseline']['ndcg@5'] for q in evaluation_results['per_query']]\n    reranked_ndcg = [q['reranked']['ndcg@5'] for q in evaluation_results['per_query']]\n\n    # Plot 1: Precision@5\n    ax = axes[0, 0]\n    x = np.arange(len(baseline_p5))\n    ax.bar(x - 0.2, baseline_p5, 0.4, label='Baseline', alpha=0.8, color='#2E86AB')\n    ax.bar(x + 0.2, reranked_p5, 0.4, label='Reranked', alpha=0.8, color='#06A77D')\n    ax.set_ylabel('Precision@5', fontweight='bold')\n    ax.set_title('Precision@5 by Query')\n    ax.legend()\n    ax.set_ylim(0, 1.0)\n    ax.grid(True, alpha=0.3, axis='y')\n\n    # Plot 2: Recall@5\n    ax = axes[0, 1]\n    ax.bar(x - 0.2, baseline_recall, 0.4, label='Baseline', alpha=0.8, color='#2E86AB')\n    ax.bar(x + 0.2, reranked_recall, 0.4, label='Reranked', alpha=0.8, color='#06A77D')\n    ax.set_ylabel('Recall@5', fontweight='bold')\n    ax.set_title('Recall@5 by Query')\n    ax.legend()\n    ax.set_ylim(0, 1.0)\n    ax.grid(True, alpha=0.3, axis='y')\n\n    # Plot 3: MRR\n    ax = axes[1, 0]\n    ax.bar(x - 0.2, baseline_mrr, 0.4, label='Baseline', alpha=0.8, color='#2E86AB')\n    ax.bar(x + 0.2, reranked_mrr, 0.4, label='Reranked', alpha=0.8, color='#06A77D')\n    ax.set_ylabel('MRR', fontweight='bold')\n    ax.set_title('Mean Reciprocal Rank by Query')\n    ax.legend()\n    ax.set_ylim(0, 1.0)\n    ax.grid(True, alpha=0.3, axis='y')\n\n    # Plot 4: NDCG@5\n    ax = axes[1, 1]\n    ax.bar(x - 0.2, baseline_ndcg, 0.4, label='Baseline', alpha=0.8, color='#2E86AB')\n    ax.bar(x + 0.2, reranked_ndcg, 0.4, label='Reranked', alpha=0.8, color='#06A77D')\n    ax.set_ylabel('NDCG@5', fontweight='bold')\n    ax.set_title('NDCG@5 by Query')\n    ax.legend()\n    ax.set_ylim(0, 1.0)\n    ax.grid(True, alpha=0.3, axis='y')\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n✓ Visualizations displayed above\")\n\nelse:\n    print(\"⚠️  Cannot evaluate: embeddings or test questions not available\")\n    evaluation_results = None\n"
  },
  {
   "cell_type": "markdown",
   "id": "cdd87d6f",
   "metadata": {},
   "source": [
    "## Track Experiment\n",
    "\n",
    "Store results for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a47f9",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 4: EXPERIMENT TRACKING\n# ============================================================================\n# Inline from foundation/00-registry-and-tracking-utilities.ipynb\n\ndef compute_config_hash(config_dict: Dict) -> str:\n    \"\"\"Create deterministic SHA256 hash of a configuration dictionary.\n\n    This enables finding all experiments with identical configurations.\n\n    Args:\n        config_dict: Configuration parameters\n\n    Returns:\n        SHA256 hash string (first 12 characters for readability)\n    \"\"\"\n    config_str = json.dumps(config_dict, sort_keys=True)\n    hash_obj = hashlib.sha256(config_str.encode())\n    return hash_obj.hexdigest()[:12]\n\n\ndef start_experiment(db_connection, experiment_name: str,\n                     notebook_path: str = None,\n                     embedding_model_alias: str = None,\n                     config: Dict = None,\n                     techniques: List[str] = None,\n                     notes: str = None) -> int:\n    \"\"\"Start a new experiment and return its ID for tracking.\n\n    Args:\n        db_connection: PostgreSQL connection\n        experiment_name: Human-readable experiment name\n        notebook_path: Path to the notebook running this experiment\n        embedding_model_alias: Which embedding model is being used\n        config: Dict of configuration parameters\n        techniques: List of techniques being applied\n        notes: Optional notes about the experiment\n\n    Returns:\n        Experiment ID for use in save_metrics() and complete_experiment()\n    \"\"\"\n    if config is None:\n        config = {}\n    if techniques is None:\n        techniques = []\n\n    config_hash = compute_config_hash(config)\n\n    with db_connection.cursor() as cur:\n        cur.execute('''\n            INSERT INTO experiments (\n                experiment_name, notebook_path, embedding_model_alias,\n                config_hash, config_json, techniques_applied, notes, status\n            )\n            VALUES (%s, %s, %s, %s, %s, %s, %s, 'running')\n            RETURNING id\n        ''', (\n            experiment_name,\n            notebook_path,\n            embedding_model_alias,\n            config_hash,\n            json.dumps(config),\n            techniques,\n            notes\n        ))\n        exp_id = cur.fetchone()[0]\n    db_connection.commit()\n    print(f\"✓ Started experiment #{exp_id}: {experiment_name}\")\n    return exp_id\n\n\ndef save_metrics(db_connection, experiment_id: int, metrics_dict: Dict,\n                 export_to_file: bool = True,\n                 export_dir: str = 'data/experiment_results') -> Tuple[bool, str]:\n    \"\"\"Save experiment metrics to database and optionally to JSON file.\n\n    Args:\n        db_connection: PostgreSQL connection\n        experiment_id: ID from start_experiment()\n        metrics_dict: Dict of {metric_name: value, ...}\n        export_to_file: Whether to also save to filesystem JSON\n        export_dir: Directory for JSON exports\n\n    Returns:\n        Tuple of (success: bool, message: str)\n    \"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            for metric_name, metric_data in metrics_dict.items():\n                # Handle both simple floats and nested dicts with details\n                if isinstance(metric_data, dict):\n                    metric_value = metric_data.get('value', 0.0)\n                    metric_details = metric_data.get('details', {})\n                else:\n                    metric_value = metric_data\n                    metric_details = {}\n\n                cur.execute('''\n                    INSERT INTO evaluation_results (\n                        experiment_id, metric_name, metric_value, metric_details_json\n                    )\n                    VALUES (%s, %s, %s, %s)\n                ''', (\n                    experiment_id,\n                    metric_name,\n                    float(metric_value),\n                    json.dumps(metric_details) if metric_details else '{}'\n                ))\n        db_connection.commit()\n\n        # Export to file if requested\n        file_path = None\n        if export_to_file:\n            os.makedirs(export_dir, exist_ok=True)\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            file_path = os.path.join(export_dir, f'experiment_{experiment_id}_{timestamp}.json')\n            with open(file_path, 'w') as f:\n                json.dump({\n                    'experiment_id': experiment_id,\n                    'timestamp': timestamp,\n                    'metrics': metrics_dict\n                }, f, indent=2)\n\n        msg = f\"✓ Saved {len(metrics_dict)} metrics for experiment #{experiment_id}\"\n        if file_path:\n            msg += f\" to {file_path}\"\n        print(msg)\n        return True, msg\n    except Exception as e:\n        msg = f\"✗ Failed to save metrics: {e}\"\n        print(msg)\n        db_connection.rollback()\n        return False, msg\n\n\ndef complete_experiment(db_connection, experiment_id: int,\n                       status: str = 'completed',\n                       notes: str = None) -> bool:\n    \"\"\"Mark an experiment as complete.\n\n    Args:\n        db_connection: PostgreSQL connection\n        experiment_id: ID returned from start_experiment()\n        status: 'completed' or 'failed'\n        notes: Optional update to notes field\n\n    Returns:\n        True if successful\n    \"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            if notes:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, notes = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, notes, experiment_id))\n            else:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, experiment_id))\n        db_connection.commit()\n        print(f\"✓ Experiment #{experiment_id} marked as {status}\")\n        return True\n    except Exception as e:\n        print(f\"✗ Failed to complete experiment: {e}\")\n        db_connection.rollback()\n        return False\n\n\n# ============================================================================\n# RUN EXPERIMENT TRACKING\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRACKING EXPERIMENT\")\nprint(\"=\" * 70)\n\nif evaluation_results:\n    # Prepare configuration\n    config_dict = {\n        'embedding_model_alias': EMBEDDING_MODEL_ALIAS,\n        'reranker_model': RERANKER_MODEL,\n        'top_k_initial': TOP_K_INITIAL,\n        'top_k_final': TOP_K_FINAL,\n        'reranking_batch_size': RERANKING_BATCH_SIZE,\n        'num_test_queries': evaluation_results['num_queries_evaluated'],\n    }\n\n    config_hash = compute_config_hash(config_dict)\n\n    print(f\"\\nExperiment Configuration:\")\n    print(f\"  Name: {EXPERIMENT_NAME}\")\n    print(f\"  Embedding Model: {EMBEDDING_MODEL_ALIAS}\")\n    print(f\"  Reranker Model: {RERANKER_MODEL}\")\n    print(f\"  Config Hash: {config_hash}\")\n    print(f\"  Test Queries: {evaluation_results['num_queries_evaluated']}\\n\")\n\n    # Start experiment tracking\n    experiment_id = start_experiment(\n        db_connection,\n        experiment_name=EXPERIMENT_NAME,\n        notebook_path='advanced-techniques/05-reranking.ipynb',\n        embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n        config=config_dict,\n        techniques=TECHNIQUES_APPLIED,\n        notes=f'Cross-encoder reranking evaluation on {evaluation_results[\"num_queries_evaluated\"]} queries'\n    )\n\n    # Prepare metrics for storage\n    metrics_to_store = {}\n\n    # Baseline metrics\n    for metric_name, metric_value in evaluation_results['baseline'].items():\n        metrics_to_store[f'baseline_{metric_name}'] = metric_value\n\n    # Reranked metrics\n    for metric_name, metric_value in evaluation_results['reranked'].items():\n        metrics_to_store[f'reranked_{metric_name}'] = metric_value\n\n    # Improvement percentages\n    for metric_name, improvement_pct in evaluation_results['improvements_pct'].items():\n        metrics_to_store[f'improvement_pct_{metric_name}'] = improvement_pct\n\n    # Configuration and metadata\n    metrics_to_store['num_queries_evaluated'] = evaluation_results['num_queries_evaluated']\n    metrics_to_store['config_hash'] = config_hash\n\n    # Save metrics\n    print(\"\\nSaving metrics to database...\\n\")\n    success, message = save_metrics(db_connection, experiment_id, metrics_to_store, export_to_file=True)\n\n    # Complete experiment\n    if success:\n        notes = f\"Successfully evaluated reranking on {evaluation_results['num_queries_evaluated']} queries. \"\n        notes += f\"Precision@5 improved {evaluation_results['improvements_pct']['precision@5']:.2f}%\"\n\n        complete_experiment(db_connection, experiment_id, status='completed', notes=notes)\n\n        # Display results summary\n        print(\"\\n\" + \"=\" * 70)\n        print(\"EXPERIMENT RESULTS SUMMARY\")\n        print(\"=\" * 70)\n\n        print(f\"\\nExperiment ID: {experiment_id}\")\n        print(f\"Experiment Name: {EXPERIMENT_NAME}\")\n        print(f\"Status: Completed\")\n        print(f\"Config Hash: {config_hash}\")\n\n        print(f\"\\nKey Improvements:\")\n        for metric_name, improvement_pct in evaluation_results['improvements_pct'].items():\n            sign = \"+\" if improvement_pct > 0 else \"\"\n            baseline_val = evaluation_results['baseline'][metric_name]\n            reranked_val = evaluation_results['reranked'][metric_name]\n            print(f\"  {metric_name}:\")\n            print(f\"    Baseline:  {baseline_val:.4f}\")\n            print(f\"    Reranked:  {reranked_val:.4f}\")\n            print(f\"    Improvement: {sign}{improvement_pct:.2f}%\")\n\n        print(f\"\\nResults exported to:\")\n        print(f\"  Database: evaluation_results table (experiment_id={experiment_id})\")\n        print(f\"  JSON: data/experiment_results/experiment_{experiment_id}_*.json\")\n\n        print(\"\\n\" + \"=\" * 70)\n        print(\"NEXT STEPS\")\n        print(\"=\" * 70)\n        print(\"\\n1. Review the visualizations above to understand the impact\")\n        print(\"2. Compare results with other techniques:\")\n        print(\"   - evaluation-lab/03-compare-experiments.ipynb\")\n        print(\"   - evaluation-lab/04-plot-improvements.ipynb\")\n        print(\"3. Try other reranking models:\")\n        print(\"   - cross-encoder/ms-marco-TinyBERT-L-2-v2 (faster)\")\n        print(\"   - cross-encoder/qnli-distilroberta-base (alternative)\")\n        print(\"4. Experiment with different TOP_K_INITIAL values\")\n        print(\"5. Combine with other techniques (query expansion, hybrid search)\")\n\n    else:\n        print(\"\\n✗ Failed to track experiment\")\n        complete_experiment(db_connection, experiment_id, status='failed', notes='Failed to save metrics')\n\nelse:\n    print(\"⚠️  Cannot track experiment: evaluation_results not available\")\n\n# Close database connection\nprint(\"\\n\\nClosing database connection...\")\nif embeddings_db:\n    embeddings_db.close()\ndb_connection.close()\nprint(\"✓ All connections closed\")\n"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}