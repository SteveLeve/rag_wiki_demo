{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b1db79",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. ✅ foundation/00-setup-postgres-schema.ipynb\n",
    "2. ✅ foundation/02-rag-postgresql-persistent.ipynb\n",
    "3. ✅ evaluation-lab/01-create-ground-truth-human-in-loop.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c1de9",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b7c12",
   "metadata": {},
   "outputs": [],
   "source": "import ollama\nimport psycopg2\nimport psycopg2.extras\nimport json\nimport math\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Optional\nimport hashlib\n\n# Configuration\nEMBEDDING_MODEL_ALIAS = \"bge_base_en_v1_5\"\nEMBEDDING_MODEL = \"hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\"\nLLM_MODEL = \"llama3.2:1b\"  # Ollama model for query expansion\nNUM_EXPANSIONS = 4  # How many query variants to generate\nTOP_K_PER_QUERY = 5  # Results per variant\nTOP_K_FINAL = 5  # Final results to return\n\nEXPERIMENT_NAME = \"query-expansion-llm\"\nTECHNIQUES_APPLIED = [\"vector_retrieval\", \"llm_query_expansion\"]\n\n# PostgreSQL configuration\nPOSTGRES_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'postgres',\n    'password': 'postgres',\n}"
  },
  {
   "cell_type": "markdown",
   "id": "eaf1f7ee",
   "metadata": {},
   "source": [
    "## Load Embeddings from Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943cf377",
   "metadata": {},
   "outputs": [],
   "source": "# Connect to PostgreSQL\ndb_connection = psycopg2.connect(\n    host=POSTGRES_CONFIG['host'],\n    port=POSTGRES_CONFIG['port'],\n    database=POSTGRES_CONFIG['database'],\n    user=POSTGRES_CONFIG['user'],\n    password=POSTGRES_CONFIG['password']\n)\n\nprint(f\"Connected to PostgreSQL at {POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}\")\n\n# List available embeddings in registry\ndef list_available_embeddings(db_conn) -> pd.DataFrame:\n    \"\"\"Query embedding_registry to show available models with metadata.\"\"\"\n    query = '''\n        SELECT \n            model_alias,\n            model_name,\n            dimension,\n            embedding_count,\n            chunk_source_dataset,\n            chunk_size_config,\n            created_at,\n            last_accessed\n        FROM embedding_registry\n        ORDER BY created_at DESC\n    '''\n    return pd.read_sql(query, db_conn)\n\n# Load or verify embeddings exist\nprint(\"\\nLooking for embeddings in registry...\")\navailable_embeddings = list_available_embeddings(db_connection)\n\nif available_embeddings.empty:\n    print(\"✗ No embeddings found in registry\")\n    print(\"Please run foundation/02-rag-postgresql-persistent.ipynb first\")\n    raise ValueError(\"No embeddings available. Run foundation/02 first.\")\n\nprint(\"\\nAvailable embeddings:\")\nprint(available_embeddings[['model_alias', 'embedding_count', 'dimension']])\n\n# Check if our target embedding model is available\nembedding_found = available_embeddings[available_embeddings['model_alias'] == EMBEDDING_MODEL_ALIAS]\n\nif embedding_found.empty:\n    print(f\"\\n✗ Embedding model '{EMBEDDING_MODEL_ALIAS}' not found\")\n    print(\"Please regenerate embeddings with foundation/02 using this model alias\")\n    raise ValueError(f\"Embedding model {EMBEDDING_MODEL_ALIAS} not found\")\n\n# Load embedding metadata\nembedding_meta = embedding_found.iloc[0]\nembedding_count = embedding_meta['embedding_count']\nembedding_dimension = embedding_meta['dimension']\n\nprint(f\"\\n✓ Found embedding model: {EMBEDDING_MODEL_ALIAS}\")\nprint(f\"  Count: {embedding_count:,} embeddings\")\nprint(f\"  Dimension: {embedding_dimension}\")\nprint(f\"  Created: {embedding_meta['created_at']}\")\n\n# Load ground truth test questions\nprint(\"\\nLoading ground truth test questions...\")\nground_truth_questions = []\n\nwith db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n    cur.execute('''\n        SELECT \n            id,\n            question,\n            relevant_chunk_ids,\n            quality_rating,\n            source_type\n        FROM evaluation_groundtruth\n        WHERE quality_rating = 'good'\n        ORDER BY id\n        LIMIT 20\n    ''')\n    \n    for row in cur.fetchall():\n        ground_truth_questions.append({\n            'id': row['id'],\n            'question': row['question'],\n            'relevant_chunk_ids': row['relevant_chunk_ids'],\n            'quality_rating': row['quality_rating'],\n            'source_type': row['source_type']\n        })\n\nprint(f\"✓ Loaded {len(ground_truth_questions)} ground truth questions\")\nif ground_truth_questions:\n    print(f\"  Sample: {ground_truth_questions[0]['question'][:80]}...\")\n\n# Helper function to get embeddings table name\ndef get_embeddings_table_name(model_alias: str) -> str:\n    \"\"\"Convert model alias to table name\"\"\"\n    return f'embeddings_{model_alias.replace(\".\", \"_\")}'\n\nembeddings_table_name = get_embeddings_table_name(EMBEDDING_MODEL_ALIAS)\nprint(f\"\\nEmbeddings table: {embeddings_table_name}\")"
  },
  {
   "cell_type": "markdown",
   "id": "d771af3e",
   "metadata": {},
   "source": [
    "## Implement LLM-Based Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea24db1f",
   "metadata": {},
   "outputs": [],
   "source": "def expand_query_with_llm(query: str, num_expansions: int = 4, llm_model: str = 'llama3.2:1b') -> List[str]:\n    \"\"\"\n    Generate semantically similar query reformulations using an LLM.\n    \n    Args:\n        query: Original user question\n        num_expansions: How many variants to generate\n        llm_model: Ollama model name\n        \n    Returns:\n        List of [original_query, variant1, variant2, ...] (up to num_expansions + 1)\n    \"\"\"\n    prompt = f\"\"\"Generate {num_expansions} different ways to ask this question. Each variant should:\n- Mean the same thing as the original\n- Use different wording and phrasing\n- Be a complete question that can stand alone\n\nOriginal question: {query}\n\nGenerate exactly {num_expansions} variants. Format each on a new line starting with \"Q:\".\nExample format:\nQ: How is something done?\nQ: What is the process of something?\nQ: What are the steps involved in something?\n\nNow generate {num_expansions} variants for the original question:\"\"\"\n    \n    try:\n        response = ollama.chat(\n            model=llm_model,\n            messages=[{'role': 'user', 'content': prompt}]\n        )\n        \n        content = response['message']['content']\n        variants = [query]  # Start with original\n        \n        for line in content.split('\\n'):\n            line = line.strip()\n            if line.startswith('Q:'):\n                variant = line[2:].strip()\n                if variant and len(variant) > 10 and variant not in variants:  # Avoid duplicates\n                    variants.append(variant)\n            elif line and '?' in line and len(line) > 10:\n                # Handle variations where Q: might not be present\n                if not any(x in line[:5] for x in ['1.', '2.', '3.', '-']):\n                    # Likely a question, add it\n                    if line not in variants:\n                        variants.append(line)\n        \n        return variants[:num_expansions + 1]  # Original + up to num_expansions\n    \n    except Exception as e:\n        print(f\"  ✗ LLM expansion failed: {e}\")\n        return [query]  # Fallback to original only\n\n\ndef retrieve_multi_query(queries: List[str], \n                         db_conn, \n                         embeddings_table: str,\n                         embedding_model: str,\n                         top_k_per_query: int = 5) -> List[Tuple[str, float, int]]:\n    \"\"\"\n    Retrieve for each query variant, then merge and deduplicate results.\n    \n    Args:\n        queries: List of query variants\n        db_conn: PostgreSQL connection\n        embeddings_table: Name of the embeddings table\n        embedding_model: Model name for embeddings\n        top_k_per_query: Results per variant\n        \n    Returns:\n        List of (chunk_text, max_similarity_score, chunk_id) tuples, deduplicated and sorted by score\n    \"\"\"\n    # Retrieve for each variant\n    all_results = {}  # chunk_id → (chunk_text, max_similarity_score)\n    \n    for query_variant in queries:\n        # Generate embedding for this query variant\n        query_emb_response = ollama.embed(model=embedding_model, input=query_variant)\n        query_emb = query_emb_response['embeddings'][0]\n        \n        # Search for similar chunks\n        with db_conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n            cur.execute(f'''\n                SELECT \n                    id,\n                    chunk_text,\n                    1 - (embedding <=> %s::vector) as similarity\n                FROM {embeddings_table}\n                ORDER BY embedding <=> %s::vector\n                LIMIT %s\n            ''', (query_emb, query_emb, top_k_per_query))\n            \n            results = cur.fetchall()\n            \n            for chunk in results:\n                chunk_id = chunk['id']\n                chunk_text = chunk['chunk_text']\n                score = chunk['similarity']\n                \n                # Keep chunk with highest similarity score across all variants\n                if chunk_id not in all_results or score > all_results[chunk_id][1]:\n                    all_results[chunk_id] = (chunk_text, score)\n    \n    # Convert to list and sort by score (descending)\n    merged = [\n        (chunk_text, score, chunk_id)\n        for chunk_id, (chunk_text, score) in all_results.items()\n    ]\n    merged.sort(key=lambda x: x[1], reverse=True)\n    \n    return merged\n\n\ndef retrieve_with_query_expansion(query: str, \n                                  db_conn,\n                                  embeddings_table: str,\n                                  embedding_model: str,\n                                  num_expansions: int = 4, \n                                  top_k_per_query: int = 5,\n                                  top_k_final: int = 5,\n                                  verbose: bool = True) -> List[Tuple[str, float, int]]:\n    \"\"\"\n    Complete retrieval pipeline with query expansion.\n    \n    Args:\n        query: Original user question\n        db_conn: PostgreSQL connection\n        embeddings_table: Name of embeddings table\n        embedding_model: Model name for embeddings\n        num_expansions: Number of variants to generate\n        top_k_per_query: Results per variant\n        top_k_final: Final results to return\n        verbose: Whether to print debug info\n        \n    Returns:\n        Top K results after merging multi-query retrieval\n    \"\"\"\n    # Step 1: Expand query\n    query_variants = expand_query_with_llm(query, num_expansions)\n    \n    if verbose:\n        print(f\"✓ Generated {len(query_variants)} query variants:\")\n        for i, v in enumerate(query_variants):\n            prefix = \"[Original]\" if i == 0 else f\"[Variant {i}]\"\n            print(f\"  {prefix} {v[:80]}{'...' if len(v) > 80 else ''}\")\n    \n    # Step 2: Retrieve for each variant\n    merged_results = retrieve_multi_query(\n        query_variants, \n        db_conn,\n        embeddings_table,\n        embedding_model,\n        top_k_per_query\n    )\n    \n    if verbose:\n        print(f\"✓ Retrieved {len(merged_results)} deduplicated results across {len(query_variants)} variants\")\n    \n    # Step 3: Return top K\n    final_results = merged_results[:top_k_final]\n    \n    if verbose:\n        print(f\"✓ Returning top {len(final_results)} results\")\n    \n    return final_results\n\n\n# Test query expansion on first question\nif ground_truth_questions:\n    test_query = ground_truth_questions[0]['question']\n    print(\"\\n\" + \"=\"*70)\n    print(\"TESTING QUERY EXPANSION\")\n    print(\"=\"*70)\n    print(f\"\\nOriginal query: {test_query}\\n\")\n    \n    test_results = retrieve_with_query_expansion(\n        test_query,\n        db_connection,\n        embeddings_table_name,\n        EMBEDDING_MODEL,\n        num_expansions=NUM_EXPANSIONS,\n        top_k_per_query=TOP_K_PER_QUERY,\n        top_k_final=TOP_K_FINAL,\n        verbose=True\n    )\n    \n    print(f\"\\nTop {TOP_K_FINAL} results:\")\n    for i, (chunk_text, score, chunk_id) in enumerate(test_results):\n        preview = chunk_text[:150].replace('\\n', ' ') + '...'\n        print(f\"  [{i+1}] (score: {score:.4f}, id: {chunk_id}) {preview}\")"
  },
  {
   "cell_type": "markdown",
   "id": "1e5b93d5",
   "metadata": {},
   "source": [
    "## Evaluate Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e634377",
   "metadata": {},
   "outputs": [],
   "source": "# Metric computation functions\n\ndef precision_at_k(retrieved_chunk_ids: List[int], \n                   relevant_chunk_ids: List[int], \n                   k: int = 5) -> float:\n    \"\"\"Precision@K: What percentage of top-K results are relevant?\"\"\"\n    if k == 0:\n        return 0.0\n    \n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n    \n    num_relevant_in_k = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n    \n    return num_relevant_in_k / k\n\n\ndef recall_at_k(retrieved_chunk_ids: List[int], \n                relevant_chunk_ids: List[int], \n                k: int = 5) -> float:\n    \"\"\"Recall@K: What percentage of all relevant chunks were found in top-K?\"\"\"\n    if len(relevant_chunk_ids) == 0:\n        return 0.0\n    \n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n    \n    num_relevant_found = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n    \n    return num_relevant_found / len(relevant_set)\n\n\ndef mean_reciprocal_rank(retrieved_chunk_ids: List[int], \n                         relevant_chunk_ids: List[int]) -> float:\n    \"\"\"MRR: How quickly do we find the first relevant result?\"\"\"\n    relevant_set = set(relevant_chunk_ids)\n    \n    for rank, chunk_id in enumerate(retrieved_chunk_ids, start=1):\n        if chunk_id in relevant_set:\n            return 1.0 / rank\n    \n    return 0.0\n\n\ndef ndcg_at_k(retrieved_chunk_ids: List[int], \n              relevant_chunk_ids: List[int], \n              k: int = 5) -> float:\n    \"\"\"NDCG@K: How well-ranked are results? (rewards relevant results at top)\"\"\"\n    \n    def dcg_score(relevance_scores: List[float]) -> float:\n        \"\"\"Compute DCG from relevance scores.\"\"\"\n        return sum(\n            (2**rel - 1) / math.log2(rank + 2)\n            for rank, rel in enumerate(relevance_scores)\n        )\n    \n    if k == 0 or len(relevant_chunk_ids) == 0:\n        return 0.0\n    \n    # Get top-K retrieved\n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n    \n    # Binary relevance: 1 if relevant, 0 if not\n    relevance = [1 if chunk_id in relevant_set else 0 for chunk_id in retrieved_k]\n    \n    # Compute DCG for retrieved ranking\n    dcg = dcg_score(relevance)\n    \n    # Compute ideal DCG (perfect ranking)\n    ideal_relevance = sorted(relevance, reverse=True)\n    idcg = dcg_score(ideal_relevance)\n    \n    if idcg == 0:\n        return 0.0\n    \n    return dcg / idcg\n\n\ndef evaluate_query_expansion(test_questions: List[Dict], \n                             db_conn,\n                             embeddings_table: str,\n                             embedding_model: str) -> Dict:\n    \"\"\"\n    Compare baseline (single query) vs expansion (multi-query).\n    \n    Focus: Recall improvement (finding more relevant chunks)\n    \n    Args:\n        test_questions: List of question dicts with 'question' and 'relevant_chunk_ids'\n        db_conn: PostgreSQL connection\n        embeddings_table: Name of embeddings table\n        embedding_model: Model name for embeddings\n        \n    Returns:\n        Dict with baseline, expanded metrics, and improvements\n    \"\"\"\n    \n    baseline_results = []\n    expanded_results = []\n    query_details = []\n    \n    print(f\"\\nEvaluating query expansion on {len(test_questions)} test questions...\")\n    print(\"-\" * 70)\n    \n    for q_idx, q in enumerate(test_questions, 1):\n        query = q['question']\n        relevant_ids = q['relevant_chunk_ids']\n        \n        # Baseline: single query vector retrieval\n        query_emb_response = ollama.embed(model=embedding_model, input=query)\n        query_emb = query_emb_response['embeddings'][0]\n        \n        with db_conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n            cur.execute(f'''\n                SELECT \n                    id,\n                    1 - (embedding <=> %s::vector) as similarity\n                FROM {embeddings_table}\n                ORDER BY embedding <=> %s::vector\n                LIMIT 5\n            ''', (query_emb, query_emb))\n            \n            baseline_chunks = cur.fetchall()\n            baseline_ids = [chunk['id'] for chunk in baseline_chunks]\n        \n        # With expansion\n        expanded_chunks = retrieve_with_query_expansion(\n            query, \n            db_conn,\n            embeddings_table,\n            embedding_model, \n            num_expansions=NUM_EXPANSIONS, \n            top_k_per_query=TOP_K_PER_QUERY,\n            top_k_final=TOP_K_FINAL,\n            verbose=False\n        )\n        expanded_ids = [chunk_id for _, _, chunk_id in expanded_chunks]\n        \n        # Compute metrics\n        baseline_metrics = {\n            'precision@5': precision_at_k(baseline_ids, relevant_ids, k=5),\n            'recall@5': recall_at_k(baseline_ids, relevant_ids, k=5),\n            'mrr': mean_reciprocal_rank(baseline_ids, relevant_ids),\n            'ndcg@5': ndcg_at_k(baseline_ids, relevant_ids, k=5)\n        }\n        \n        expanded_metrics = {\n            'precision@5': precision_at_k(expanded_ids, relevant_ids, k=5),\n            'recall@5': recall_at_k(expanded_ids, relevant_ids, k=5),\n            'mrr': mean_reciprocal_rank(expanded_ids, relevant_ids),\n            'ndcg@5': ndcg_at_k(expanded_ids, relevant_ids, k=5)\n        }\n        \n        baseline_results.append(baseline_metrics)\n        expanded_results.append(expanded_metrics)\n        \n        # Track per-query details\n        query_details.append({\n            'question': query,\n            'relevant_count': len(relevant_ids),\n            'baseline': baseline_metrics,\n            'expanded': expanded_metrics,\n            'improvement_recall': expanded_metrics['recall@5'] - baseline_metrics['recall@5'],\n            'improvement_ndcg': expanded_metrics['ndcg@5'] - baseline_metrics['ndcg@5']\n        })\n        \n        # Progress output\n        if q_idx % 5 == 0 or q_idx == len(test_questions):\n            print(f\"  [{q_idx}/{len(test_questions)}] Evaluated\")\n    \n    # Aggregate metrics\n    def aggregate(results):\n        return {\n            metric: sum(r[metric] for r in results) / len(results)\n            for metric in results[0].keys()\n        }\n    \n    baseline_agg = aggregate(baseline_results)\n    expanded_agg = aggregate(expanded_results)\n    \n    # Compute improvements\n    improvements = {}\n    for metric in baseline_agg.keys():\n        baseline_val = baseline_agg[metric]\n        expanded_val = expanded_agg[metric]\n        \n        if baseline_val > 0:\n            improvements[metric] = ((expanded_val - baseline_val) / baseline_val * 100)\n        else:\n            improvements[metric] = 0.0\n    \n    return {\n        'baseline': baseline_agg,\n        'expanded': expanded_agg,\n        'improvements_pct': improvements,\n        'per_query': query_details,\n        'num_queries': len(test_questions)\n    }\n\n\n# Run evaluation\nprint(\"\\n\" + \"=\"*70)\nprint(\"EVALUATION: BASELINE VS QUERY EXPANSION\")\nprint(\"=\"*70)\n\neval_results = evaluate_query_expansion(\n    ground_truth_questions,\n    db_connection,\n    embeddings_table_name,\n    EMBEDDING_MODEL\n)\n\n# Display results\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULTS SUMMARY\")\nprint(\"=\"*70)\n\nprint(f\"\\nQueries evaluated: {eval_results['num_queries']}\")\nprint(f\"\\n{'Metric':<20} {'Baseline':<15} {'Expanded':<15} {'Improvement':<15}\")\nprint(\"-\" * 65)\n\nfor metric in ['precision@5', 'recall@5', 'ndcg@5', 'mrr']:\n    baseline_val = eval_results['baseline'][metric]\n    expanded_val = eval_results['expanded'][metric]\n    improvement = eval_results['improvements_pct'][metric]\n    \n    improvement_str = f\"+{improvement:.1f}%\" if improvement >= 0 else f\"{improvement:.1f}%\"\n    print(f\"{metric:<20} {baseline_val:<15.4f} {expanded_val:<15.4f} {improvement_str:<15}\")\n\n# Detailed per-query analysis\nprint(\"\\n\" + \"=\"*70)\nprint(\"ANALYSIS: WHICH QUERIES BENEFIT MOST FROM EXPANSION?\")\nprint(\"=\"*70)\n\n# Sort by recall improvement\ntop_improvements = sorted(\n    eval_results['per_query'],\n    key=lambda x: x['improvement_recall'],\n    reverse=True\n)\n\nprint(\"\\nTop 5 questions with highest Recall@5 improvement:\")\nfor i, q in enumerate(top_improvements[:5], 1):\n    baseline_recall = q['baseline']['recall@5']\n    expanded_recall = q['expanded']['recall@5']\n    improvement = q['improvement_recall'] * 100\n    \n    print(f\"\\n  [{i}] Improvement: {improvement:+.1f}% (Recall@5: {baseline_recall:.2f} → {expanded_recall:.2f})\")\n    print(f\"      Q: {q['question'][:80]}...\")\n    print(f\"      Relevant chunks to find: {q['relevant_count']}\")\n\n# Questions that don't benefit\nno_benefit = [q for q in eval_results['per_query'] if q['improvement_recall'] <= 0]\nif no_benefit:\n    print(f\"\\n  Queries with no Recall improvement: {len(no_benefit)}\")\n    print(\"  (Original single query was already optimal)\")\n\n# Distribution statistics\nrecall_improvements = [q['improvement_recall'] * 100 for q in eval_results['per_query']]\nprint(f\"\\nRecall@5 Improvement Statistics:\")\nprint(f\"  Mean: {np.mean(recall_improvements):+.1f}%\")\nprint(f\"  Median: {np.median(recall_improvements):+.1f}%\")\nprint(f\"  Min: {np.min(recall_improvements):+.1f}%\")\nprint(f\"  Max: {np.max(recall_improvements):+.1f}%\")\nprint(f\"  Std Dev: {np.std(recall_improvements):.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "2334b37f",
   "metadata": {},
   "source": [
    "## Track Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24456773",
   "metadata": {},
   "outputs": [],
   "source": "# Experiment tracking utilities (inline from foundation/00)\n\ndef compute_config_hash(config_dict: Dict) -> str:\n    \"\"\"Create deterministic SHA256 hash of configuration.\"\"\"\n    config_str = json.dumps(config_dict, sort_keys=True)\n    hash_obj = hashlib.sha256(config_str.encode())\n    return hash_obj.hexdigest()[:12]\n\n\ndef start_experiment(db_conn, experiment_name: str, \n                    embedding_model_alias: str = None,\n                    config: Dict = None,\n                    techniques: List[str] = None,\n                    notes: str = None) -> int:\n    \"\"\"Start a new experiment and return its ID for tracking.\"\"\"\n    if config is None:\n        config = {}\n    if techniques is None:\n        techniques = []\n    \n    config_hash = compute_config_hash(config)\n    \n    with db_conn.cursor() as cur:\n        cur.execute('''\n            INSERT INTO experiments (\n                experiment_name, notebook_path, embedding_model_alias,\n                config_hash, config_json, techniques_applied, notes, status\n            )\n            VALUES (%s, %s, %s, %s, %s, %s, %s, 'running')\n            RETURNING id\n        ''', (\n            experiment_name,\n            'advanced-techniques/06-query-expansion.ipynb',\n            embedding_model_alias,\n            config_hash,\n            json.dumps(config),\n            techniques,\n            notes\n        ))\n        exp_id = cur.fetchone()[0]\n    db_conn.commit()\n    print(f\"✓ Started experiment #{exp_id}: {experiment_name}\")\n    return exp_id\n\n\ndef save_metrics(db_conn, experiment_id: int, metrics_dict: Dict,\n                export_to_file: bool = True,\n                export_dir: str = 'data/experiment_results') -> Tuple[bool, str]:\n    \"\"\"Save experiment metrics to database and optionally to JSON file.\"\"\"\n    import os\n    \n    try:\n        with db_conn.cursor() as cur:\n            for metric_name, metric_data in metrics_dict.items():\n                # Handle both simple floats and nested dicts with details\n                if isinstance(metric_data, dict):\n                    metric_value = metric_data.get('value', 0.0)\n                    metric_details = metric_data.get('details', {})\n                else:\n                    metric_value = metric_data\n                    metric_details = {}\n                \n                cur.execute('''\n                    INSERT INTO evaluation_results (\n                        experiment_id, metric_name, metric_value, metric_details_json\n                    )\n                    VALUES (%s, %s, %s, %s)\n                ''', (\n                    experiment_id,\n                    metric_name,\n                    float(metric_value),\n                    json.dumps(metric_details) if metric_details else '{}'\n                ))\n        db_conn.commit()\n        \n        # Export to file if requested\n        file_path = None\n        if export_to_file:\n            os.makedirs(export_dir, exist_ok=True)\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            file_path = os.path.join(export_dir, f'experiment_{experiment_id}_{timestamp}.json')\n            with open(file_path, 'w') as f:\n                json.dump({\n                    'experiment_id': experiment_id,\n                    'timestamp': timestamp,\n                    'metrics': metrics_dict\n                }, f, indent=2)\n        \n        msg = f\"✓ Saved {len(metrics_dict)} metrics for experiment #{experiment_id}\"\n        if file_path:\n            msg += f\" to {file_path}\"\n        print(msg)\n        return True, msg\n    except Exception as e:\n        msg = f\"✗ Failed to save metrics: {e}\"\n        print(msg)\n        db_conn.rollback()\n        return False, msg\n\n\ndef complete_experiment(db_conn, experiment_id: int, \n                       status: str = 'completed',\n                       notes: str = None) -> bool:\n    \"\"\"Mark an experiment as complete.\"\"\"\n    try:\n        with db_conn.cursor() as cur:\n            update_notes = \", notes = %s\" if notes else \"\"\n            params = [status, experiment_id] if not notes else [status, notes, experiment_id]\n            \n            cur.execute(f'''\n                UPDATE experiments\n                SET status = %s{update_notes}, completed_at = CURRENT_TIMESTAMP\n                WHERE id = %s\n            ''', params)\n        db_connection.commit()\n        print(f\"✓ Experiment #{experiment_id} marked as {status}\")\n        return True\n    except Exception as e:\n        print(f\"✗ Failed to complete experiment: {e}\")\n        db_connection.rollback()\n        return False\n\n\n# Start experiment tracking\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRACKING EXPERIMENT\")\nprint(\"=\"*70)\n\n# Prepare experiment configuration\nconfig_dict = {\n    'embedding_model_alias': EMBEDDING_MODEL_ALIAS,\n    'embedding_model': EMBEDDING_MODEL,\n    'llm_model': LLM_MODEL,\n    'num_expansions': NUM_EXPANSIONS,\n    'top_k_per_query': TOP_K_PER_QUERY,\n    'top_k_final': TOP_K_FINAL,\n    'num_test_queries': len(ground_truth_questions),\n}\n\nprint(\"\\nExperiment Configuration:\")\nfor key, value in config_dict.items():\n    print(f\"  {key}: {value}\")\n\n# Start experiment in database\nexperiment_id = start_experiment(\n    db_connection,\n    experiment_name=EXPERIMENT_NAME,\n    embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n    config=config_dict,\n    techniques=TECHNIQUES_APPLIED,\n    notes=f\"Query expansion evaluation on {len(ground_truth_questions)} test questions. \"\n          f\"Expects +15-30% Recall improvement over baseline.\"\n)\n\n# Prepare metrics for storage\nmetrics_to_store = {}\n\n# Store baseline metrics\nfor metric_name, value in eval_results['baseline'].items():\n    metrics_to_store[f'baseline_{metric_name}'] = float(value)\n\n# Store expanded metrics\nfor metric_name, value in eval_results['expanded'].items():\n    metrics_to_store[f'expanded_{metric_name}'] = float(value)\n\n# Store improvements\nfor metric_name, value in eval_results['improvements_pct'].items():\n    metrics_to_store[f'improvement_pct_{metric_name}'] = float(value)\n\n# Store test count\nmetrics_to_store['num_test_queries'] = len(ground_truth_questions)\n\n# Store per-query details as JSON\nquery_improvements = [\n    {\n        'question': q['question'],\n        'recall_improvement_pct': q['improvement_recall'] * 100,\n        'ndcg_improvement_pct': q['improvement_ndcg'] * 100,\n    }\n    for q in eval_results['per_query']\n]\n\nmetrics_to_store['per_query_improvements'] = {\n    'value': 0.0,\n    'details': {'improvements': query_improvements}\n}\n\n# Save metrics to database and file\nprint(\"\\nSaving metrics...\")\nsuccess, msg = save_metrics(db_connection, experiment_id, metrics_to_store, export_to_file=True)\n\n# Complete the experiment\nprint(\"\\nMarking experiment as complete...\")\ncomplete_experiment(\n    db_connection,\n    experiment_id,\n    status='completed',\n    notes=f\"Evaluation complete. Recall@5 improvement: {eval_results['improvements_pct']['recall@5']:.1f}%\"\n)\n\n# Final summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"EXPERIMENT COMPLETE\")\nprint(\"=\"*70)\n\nprint(f\"\\nExperiment #{experiment_id}: {EXPERIMENT_NAME}\")\nprint(f\"Status: Completed\")\nprint(f\"Timestamp: {datetime.now().isoformat()}\")\n\nprint(f\"\\nKey Results:\")\nprint(f\"  Precision@5 improvement:  {eval_results['improvements_pct']['precision@5']:+.1f}%\")\nprint(f\"  Recall@5 improvement:     {eval_results['improvements_pct']['recall@5']:+.1f}%\")\nprint(f\"  NDCG@5 improvement:       {eval_results['improvements_pct']['ndcg@5']:+.1f}%\")\nprint(f\"  MRR improvement:          {eval_results['improvements_pct']['mrr']:+.1f}%\")\n\nprint(f\"\\nMetrics Stored:\")\nprint(f\"  {len(metrics_to_store)} metrics stored to database\")\nprint(f\"  Results exported to data/experiment_results/\")\n\nprint(f\"\\nNext Steps:\")\nprint(f\"  1. Review the detailed per-query improvements above\")\nprint(f\"  2. Compare with other techniques using evaluation-lab/04\")\nprint(f\"  3. Combine query expansion with reranking for further improvements\")\nprint(f\"  4. Evaluate on more diverse question types in evaluation-lab/01\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}