{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da85bd3d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. ✅ foundation/00-setup-postgres-schema.ipynb\n",
    "2. ✅ foundation/02-rag-postgresql-persistent.ipynb\n",
    "3. ✅ evaluation-lab/01-create-ground-truth-human-in-loop.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115b915",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_ALIAS = \"all-minilm-l6-v2\"\n",
    "CHUNK_SIZE = 256\n",
    "CHUNK_OVERLAP = 50\n",
    "SPLIT_STRATEGY = \"semantic\"  # vs. 'fixed' or 'paragraph'\n",
    "TOP_K = 5\n",
    "\n",
    "EXPERIMENT_NAME = \"semantic-chunking-metadata\"\n",
    "TECHNIQUES_APPLIED = [\"semantic_chunking\", \"metadata_extraction\", \"filtered_retrieval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f8fd5",
   "metadata": {},
   "source": [
    "## Load Embeddings from Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b837c79",
   "metadata": {},
   "outputs": [],
   "source": "import psycopg2\nimport psycopg2.extras\nimport ollama\nimport json\nimport pandas as pd\nimport numpy as np\nimport hashlib\nimport re\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Optional\nimport os\n\n# PostgreSQL connection\nPOSTGRES_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'postgres',\n    'password': 'postgres',\n}\n\n# Create database connection\ntry:\n    db_connection = psycopg2.connect(\n        host=POSTGRES_CONFIG['host'],\n        port=POSTGRES_CONFIG['port'],\n        database=POSTGRES_CONFIG['database'],\n        user=POSTGRES_CONFIG['user'],\n        password=POSTGRES_CONFIG['password']\n    )\n    print(\"✓ Connected to PostgreSQL\")\nexcept psycopg2.OperationalError as e:\n    print(f\"✗ Failed to connect to PostgreSQL: {e}\")\n    raise\n\n# ============================================================================\n# PART 1: REGISTRY DISCOVERY & LOAD-OR-GENERATE PATTERN\n# ============================================================================\n\ndef list_available_embeddings(db_connection) -> pd.DataFrame:\n    \"\"\"Query embedding_registry to show available models with metadata.\n\n    Returns:\n        DataFrame with columns: model_alias, model_name, dimension, embedding_count,\n                                 chunk_source_dataset, created_at, chunk_size_config\n    \"\"\"\n    query = '''\n        SELECT\n            model_alias,\n            model_name,\n            dimension,\n            embedding_count,\n            chunk_source_dataset,\n            chunk_size_config,\n            created_at,\n            last_accessed\n        FROM embedding_registry\n        ORDER BY created_at DESC\n    '''\n    return pd.read_sql(query, db_connection)\n\n\ndef get_embedding_metadata(db_connection, model_alias: str) -> Optional[Dict]:\n    \"\"\"Fetch metadata_json and other info for a specific model.\n\n    Args:\n        db_connection: PostgreSQL connection\n        model_alias: The model alias (e.g., 'all_minilm_l6_v2')\n\n    Returns:\n        Dict with: dimension, embedding_count, config_hash (if stored),\n                   chunk_source_dataset, created_at, metadata_json\n    \"\"\"\n    with db_connection.cursor() as cur:\n        cur.execute('''\n            SELECT\n                dimension,\n                embedding_count,\n                chunk_source_dataset,\n                chunk_size_config,\n                created_at,\n                metadata_json\n            FROM embedding_registry\n            WHERE model_alias = %s\n        ''', (model_alias,))\n        result = cur.fetchone()\n\n        if not result:\n            return None\n\n        return {\n            'dimension': result[0],\n            'embedding_count': result[1],\n            'chunk_source_dataset': result[2],\n            'chunk_size_config': result[3],\n            'created_at': result[4],\n            'metadata_json': result[5] or {}\n        }\n\n\nclass PostgreSQLVectorDB:\n    \"\"\"Helper to load embeddings from PostgreSQL without regeneration.\"\"\"\n\n    def __init__(self, config, table_name, preserve_existing=True):\n        self.config = config\n        self.table_name = table_name\n        self.conn = psycopg2.connect(\n            host=config['host'],\n            port=config['port'],\n            database=config['database'],\n            user=config['user'],\n            password=config['password']\n        )\n        print(f'✓ Connected to table: {table_name}')\n\n    def get_chunk_count(self):\n        \"\"\"How many embeddings are stored?\"\"\"\n        with self.conn.cursor() as cur:\n            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n            return cur.fetchone()[0]\n\n    def similarity_search(self, query_embedding, top_n=5):\n        \"\"\"Retrieve most similar chunks using pgvector.\n\n        Args:\n            query_embedding: Query embedding vector\n            top_n: Number of results to return\n\n        Returns:\n            List of tuples: (chunk_text, similarity_score, chunk_id, metadata)\n        \"\"\"\n        with self.conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n            cur.execute(f'''\n                SELECT id,\n                       content as chunk_text,\n                       metadata_json,\n                       1 - (embedding <=> %s::vector) as similarity\n                FROM {self.table_name}\n                ORDER BY embedding <=> %s::vector\n                LIMIT %s\n            ''', (query_embedding, query_embedding, top_n))\n\n            results = cur.fetchall()\n            return [(row['chunk_text'], row['similarity'], row['id'], row['metadata_json'] or {}) \n                    for row in results]\n\n    def close(self):\n        if self.conn:\n            self.conn.close()\n\n\ndef load_or_generate(db_connection, embedding_model_alias, preserve_existing=True):\n    \"\"\"Load embeddings from registry OR show instructions if not available.\n\n    This is the CORE PATTERN for fast iteration: check registry first,\n    load existing embeddings instantly (<1 second), avoid 50+ minute regeneration.\n\n    Args:\n        db_connection: PostgreSQL connection object\n        embedding_model_alias: Model identifier (e.g., 'all_minilm_l6_v2')\n        preserve_existing: If True, always load. If False, regenerate.\n\n    Returns:\n        PostgreSQLVectorDB instance ready for use, or None if no embeddings available\n    \"\"\"\n\n    print(f\"\\n{'='*70}\")\n    print(f\"Checking for embeddings: '{embedding_model_alias}'...\")\n    print(f\"{'='*70}\\n\")\n\n    try:\n        with db_connection.cursor() as cur:\n            cur.execute('''\n                SELECT id, dimension, embedding_count, created_at, metadata_json\n                FROM embedding_registry\n                WHERE model_alias = %s\n            ''', (embedding_model_alias,))\n            registry_entry = cur.fetchone()\n    except Exception as e:\n        print(f\"Could not query registry: {e}\")\n        print(\"Make sure foundation/00-setup-postgres-schema.ipynb has been run.\")\n        return None\n\n    # Case A: Embeddings exist\n    if registry_entry:\n        reg_id, dimension, embedding_count, created_at, metadata_json = registry_entry\n\n        print(f\"✓ FOUND EXISTING EMBEDDINGS\")\n        print(f\"  Model:      {embedding_model_alias}\")\n        print(f\"  Count:      {embedding_count:,} embeddings\")\n        print(f\"  Dimension:  {dimension}\")\n        print(f\"  Created:    {created_at}\")\n        print(f\"\\n  TIME SAVINGS:\")\n        print(f\"    Loading:       <1 second\")\n        print(f\"    Regenerating:  ~50+ minutes\")\n        print(f\"    ➜ You save 50+ minutes by loading!\\n\")\n\n        if preserve_existing:\n            # Auto-load (for scripts/notebooks)\n            print(\"Loading existing embeddings...\\n\")\n\n            try:\n                table_name = f'embeddings_{embedding_model_alias.replace(\".\", \"_\")}'\n\n                db_instance = PostgreSQLVectorDB(\n                    config=POSTGRES_CONFIG,\n                    table_name=table_name,\n                    preserve_existing=True\n                )\n\n                count = db_instance.get_chunk_count()\n                print(f\"✓ LOADED SUCCESSFULLY\")\n                print(f\"  Embeddings: {count:,}\")\n                print(f\"  Table: {table_name}\")\n                print(f\"  Status: Ready for retrieval\\n\")\n\n                return db_instance\n\n            except Exception as e:\n                print(f\"\\n✗ Error loading embeddings: {e}\")\n                print(f\"\\nTroubleshooting:\")\n                print(f\"  1. Verify PostgreSQL is running\")\n                print(f\"  2. Check POSTGRES_CONFIG settings\")\n                print(f\"  3. Run foundation/02 to generate embeddings first\")\n                return None\n\n    # Case B: No embeddings found\n    else:\n        print(f\"✗ NO EMBEDDINGS FOUND\")\n        print(f\"  Model: {embedding_model_alias}\")\n        print(f\"\\nTo create embeddings, run:\")\n        print(f\"  foundation/02-rag-postgresql-persistent.ipynb\")\n        print(f\"\\nThen come back and re-run this cell.\\n\")\n        return None\n\n\n# Discover and load embeddings\nprint(\"Step 1: Discovering available embeddings...\\n\")\navailable = list_available_embeddings(db_connection)\n\nif available.empty:\n    print(\"⚠️  No embeddings found in registry yet.\")\n    print(\"Run foundation/02-rag-postgresql-persistent.ipynb first.\\n\")\nelse:\n    print(\"Available embeddings:\")\n    print(available.to_string(index=False))\n    print()\n\n# Load embeddings using the pattern\nprint(\"\\nStep 2: Loading embeddings using load-or-generate pattern...\\n\")\nembeddings_db = load_or_generate(\n    db_connection=db_connection,\n    embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n    preserve_existing=True  # Auto-load if available\n)\n\nif embeddings_db:\n    print(\"✓ Success! Embeddings loaded and ready for retrieval.\")\nelse:\n    print(\"⚠️  Could not load embeddings. See instructions above.\")\n    embeddings_db = None"
  },
  {
   "cell_type": "markdown",
   "id": "c7c589ec",
   "metadata": {},
   "source": [
    "## Implement Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b69cef",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 2: IMPLEMENT SEMANTIC CHUNKING WITH METADATA EXTRACTION\n# ============================================================================\n\ndef semantic_chunk_split(text, max_chunk_size=512, overlap=50):\n    \"\"\"\n    Split text at semantic boundaries (paragraphs, sentences) not fixed chars.\n    \n    Args:\n        text: Full text to chunk\n        max_chunk_size: Target max characters per chunk\n        overlap: Characters to overlap between chunks\n        \n    Returns:\n        List of dicts with 'text', 'start_pos', 'end_pos', 'boundary_type'\n    \"\"\"\n    chunks = []\n    \n    # Split by double newline (paragraph boundaries)\n    paragraphs = text.split('\\n\\n')\n    \n    current_chunk = \"\"\n    current_start = 0\n    \n    for para in paragraphs:\n        # If adding paragraph exceeds max, save current chunk\n        if len(current_chunk) + len(para) > max_chunk_size and current_chunk:\n            chunks.append({\n                'text': current_chunk.strip(),\n                'start_pos': current_start,\n                'end_pos': current_start + len(current_chunk),\n                'boundary_type': 'paragraph'\n            })\n            \n            # Start new chunk with overlap\n            overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n            current_chunk = overlap_text + \"\\n\\n\" + para\n            current_start = current_start + len(current_chunk) - overlap - len(para) - 2\n        else:\n            current_chunk += \"\\n\\n\" + para if current_chunk else para\n    \n    # Add final chunk\n    if current_chunk:\n        chunks.append({\n            'text': current_chunk.strip(),\n            'start_pos': current_start,\n            'end_pos': current_start + len(current_chunk),\n            'boundary_type': 'paragraph'\n        })\n    \n    return chunks\n\n\ndef extract_metadata(chunk_text):\n    \"\"\"\n    Extract metadata from chunk text.\n    \n    Returns:\n        dict: {\n            'has_dates': bool,\n            'has_numbers': bool,\n            'has_entities': bool,\n            'word_count': int,\n            'has_questions': bool,\n            'entity_count': int\n        }\n    \"\"\"\n    metadata = {}\n    \n    # Detect dates (YYYY format)\n    metadata['has_dates'] = bool(re.search(r'\\b\\d{4}\\b', chunk_text))\n    \n    # Detect numbers\n    metadata['has_numbers'] = bool(re.search(r'\\d+', chunk_text))\n    \n    # Detect capitalized entities (simple heuristic)\n    entities = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', chunk_text)\n    metadata['has_entities'] = len(entities) > 0\n    metadata['entity_count'] = len(set(entities))\n    \n    # Word count\n    metadata['word_count'] = len(chunk_text.split())\n    \n    # Has questions\n    metadata['has_questions'] = '?' in chunk_text\n    \n    return metadata\n\n\n# Load documents from database for chunking demonstration\nprint(\"\\n\" + \"=\"*70)\nprint(\"LOADING DOCUMENTS FOR SEMANTIC CHUNKING DEMONSTRATION\")\nprint(\"=\"*70)\n\n# Get sample documents from embeddings table\nsample_docs = []\nembeddings_table_name = f'embeddings_{EMBEDDING_MODEL_ALIAS.replace(\".\", \"_\")}'\n\nif embeddings_db:\n    with db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n        cur.execute(f'''\n            SELECT DISTINCT content\n            FROM {embeddings_table_name}\n            LIMIT 5\n        ''')\n        for row in cur.fetchall():\n            sample_docs.append(row['content'])\n\nprint(f\"\\nLoaded {len(sample_docs)} sample documents for demonstration\\n\")\n\nif sample_docs:\n    # Demonstrate semantic chunking on first document\n    test_doc = sample_docs[0]\n    \n    print(f\"Sample document (first 200 chars):\")\n    print(f\"  {test_doc[:200]}...\\n\")\n    \n    # Apply semantic chunking\n    print(f\"Applying semantic chunking (max_chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})...\")\n    semantic_chunks = semantic_chunk_split(test_doc, max_chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n    \n    print(f\"✓ Created {len(semantic_chunks)} semantic chunks\\n\")\n    \n    # Extract and display metadata for each chunk\n    print(\"Chunks with extracted metadata:\")\n    print(\"-\" * 70)\n    \n    chunks_with_metadata = []\n    for i, chunk in enumerate(semantic_chunks):\n        metadata = extract_metadata(chunk['text'])\n        chunk['metadata'] = metadata\n        chunks_with_metadata.append(chunk)\n        \n        print(f\"\\n[Chunk {i+1}]\")\n        print(f\"  Length: {len(chunk['text'])} chars, {metadata['word_count']} words\")\n        print(f\"  Has dates: {metadata['has_dates']}\")\n        print(f\"  Has numbers: {metadata['has_numbers']}\")\n        print(f\"  Has entities: {metadata['has_entities']} ({metadata['entity_count']} unique)\")\n        print(f\"  Has questions: {metadata['has_questions']}\")\n        print(f\"  Preview: {chunk['text'][:80].replace(chr(10), ' ')}...\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"SEMANTIC CHUNKING SUMMARY\")\n    print(\"=\"*70)\n    \n    # Show statistics\n    total_chars = sum(len(c['text']) for c in chunks_with_metadata)\n    avg_chunk_size = total_chars / len(chunks_with_metadata) if chunks_with_metadata else 0\n    \n    print(f\"\\nChunking Statistics:\")\n    print(f\"  Total chunks: {len(chunks_with_metadata)}\")\n    print(f\"  Average chunk size: {avg_chunk_size:.0f} chars\")\n    print(f\"  Min chunk size: {min((len(c['text']) for c in chunks_with_metadata), default=0)} chars\")\n    print(f\"  Max chunk size: {max((len(c['text']) for c in chunks_with_metadata), default=0)} chars\")\n    \n    # Metadata statistics\n    chunks_with_dates = sum(1 for c in chunks_with_metadata if c['metadata']['has_dates'])\n    chunks_with_entities = sum(1 for c in chunks_with_metadata if c['metadata']['has_entities'])\n    chunks_with_questions = sum(1 for c in chunks_with_metadata if c['metadata']['has_questions'])\n    \n    print(f\"\\nMetadata Statistics:\")\n    print(f\"  Chunks with dates: {chunks_with_dates}/{len(chunks_with_metadata)}\")\n    print(f\"  Chunks with entities: {chunks_with_entities}/{len(chunks_with_metadata)}\")\n    print(f\"  Chunks with questions: {chunks_with_questions}/{len(chunks_with_metadata)}\")\n    \n    avg_entities = np.mean([c['metadata']['entity_count'] for c in chunks_with_metadata])\n    avg_words = np.mean([c['metadata']['word_count'] for c in chunks_with_metadata])\n    print(f\"  Average entities per chunk: {avg_entities:.1f}\")\n    print(f\"  Average words per chunk: {avg_words:.1f}\")\n    \nelse:\n    print(\"⚠️  No embeddings available for demonstration\")\n    chunks_with_metadata = []"
  },
  {
   "cell_type": "markdown",
   "id": "d8e4f9de",
   "metadata": {},
   "source": [
    "## Filtered Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2862f2d8",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 3: IMPLEMENT METADATA-FILTERED RETRIEVAL\n# ============================================================================\n\ndef filtered_retrieval(query, embeddings_db, embedding_model, filters=None, top_k=5):\n    \"\"\"\n    Retrieve with optional metadata filtering.\n    \n    Args:\n        query: User question\n        embeddings_db: PostgreSQLVectorDB instance\n        embedding_model: Model for query embedding\n        filters: Dict of metadata filters {'has_dates': True, 'min_word_count': 50}\n        top_k: Number of results\n        \n    Returns:\n        Filtered results matching criteria: list of (chunk_text, score, chunk_id, metadata)\n    \"\"\"\n    \n    # Step 1: Vector retrieval (broad recall)\n    query_emb = ollama.embed(model=embedding_model, input=query)['embeddings'][0]\n    candidates = embeddings_db.similarity_search(query_emb, top_n=top_k*3)  # Over-retrieve\n    \n    # Step 2: Apply metadata filters\n    if filters:\n        filtered = []\n        for chunk_text, score, chunk_id, metadata in candidates:\n            # Check all filter conditions\n            passes_filters = True\n            for key, value in filters.items():\n                if key.startswith('min_'):\n                    # Minimum value filter\n                    metric_key = key[4:]  # Remove 'min_' prefix\n                    if metadata.get(metric_key, 0) < value:\n                        passes_filters = False\n                        break\n                elif key.startswith('has_'):\n                    # Boolean filter\n                    if metadata.get(key, False) != value:\n                        passes_filters = False\n                        break\n            \n            if passes_filters:\n                filtered.append((chunk_text, score, chunk_id, metadata))\n        \n        candidates = filtered\n    \n    return candidates[:top_k]\n\n\n# Load ground truth test questions\nprint(\"\\n\" + \"=\"*70)\nprint(\"LOADING GROUND TRUTH TEST QUESTIONS\")\nprint(\"=\"*70)\n\nground_truth_questions = []\n\nwith db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n    cur.execute('''\n        SELECT\n            id,\n            question,\n            relevant_chunk_ids,\n            quality_rating,\n            source_type\n        FROM evaluation_groundtruth\n        WHERE quality_rating = 'good'\n        ORDER BY id\n        LIMIT 10\n    ''')\n\n    for row in cur.fetchall():\n        ground_truth_questions.append({\n            'id': row['id'],\n            'question': row['question'],\n            'relevant_chunk_ids': row['relevant_chunk_ids'] or [],\n            'quality_rating': row['quality_rating'],\n            'source_type': row['source_type']\n        })\n\nprint(f\"\\nLoaded {len(ground_truth_questions)} ground truth questions\")\n\nif ground_truth_questions:\n    print(f\"Sample question: {ground_truth_questions[0]['question'][:80]}...\\n\")\n\n    # Demonstrate filtered retrieval\n    print(\"=\"*70)\n    print(\"DEMONSTRATING FILTERED RETRIEVAL\")\n    print(\"=\"*70)\n\n    if embeddings_db:\n        test_question = ground_truth_questions[0]['question']\n        print(f\"\\nQuery: {test_question}\\n\")\n\n        # Baseline: no filters\n        print(\"BASELINE (No Filters):\")\n        baseline_results = filtered_retrieval(\n            test_question,\n            embeddings_db,\n            EMBEDDING_MODEL_ALIAS,\n            filters=None,\n            top_k=TOP_K\n        )\n\n        print(f\"Retrieved {len(baseline_results)} results:\")\n        for i, (chunk_text, score, chunk_id, metadata) in enumerate(baseline_results, 1):\n            preview = chunk_text[:80].replace('\\n', ' ')\n            print(f\"  [{i}] (score: {score:.4f}) {preview}...\")\n\n        # With entity filter\n        print(\"\\nWITH ENTITY FILTER (has_entities=True):\")\n        entity_filtered = filtered_retrieval(\n            test_question,\n            embeddings_db,\n            EMBEDDING_MODEL_ALIAS,\n            filters={'has_entities': True},\n            top_k=TOP_K\n        )\n\n        print(f\"Retrieved {len(entity_filtered)} results with entities:\")\n        for i, (chunk_text, score, chunk_id, metadata) in enumerate(entity_filtered, 1):\n            preview = chunk_text[:80].replace('\\n', ' ')\n            entity_count = metadata.get('entity_count', 0)\n            print(f\"  [{i}] (score: {score:.4f}, entities: {entity_count}) {preview}...\")\n\n        # With minimum word count filter\n        print(f\"\\nWITH MINIMUM WORD COUNT FILTER (min_word_count=50):\")\n        word_filtered = filtered_retrieval(\n            test_question,\n            embeddings_db,\n            EMBEDDING_MODEL_ALIAS,\n            filters={'min_word_count': 50},\n            top_k=TOP_K\n        )\n\n        print(f\"Retrieved {len(word_filtered)} results with at least 50 words:\")\n        for i, (chunk_text, score, chunk_id, metadata) in enumerate(word_filtered, 1):\n            preview = chunk_text[:80].replace('\\n', ' ')\n            word_count = metadata.get('word_count', 0)\n            print(f\"  [{i}] (score: {score:.4f}, words: {word_count}) {preview}...\")\n\n        # With combined filters\n        print(f\"\\nWITH COMBINED FILTERS (has_entities=True AND min_word_count=30):\")\n        combined_filtered = filtered_retrieval(\n            test_question,\n            embeddings_db,\n            EMBEDDING_MODEL_ALIAS,\n            filters={'has_entities': True, 'min_word_count': 30},\n            top_k=TOP_K\n        )\n\n        print(f\"Retrieved {len(combined_filtered)} results:\")\n        for i, (chunk_text, score, chunk_id, metadata) in enumerate(combined_filtered, 1):\n            preview = chunk_text[:80].replace('\\n', ' ')\n            entity_count = metadata.get('entity_count', 0)\n            word_count = metadata.get('word_count', 0)\n            print(f\"  [{i}] (score: {score:.4f}, entities: {entity_count}, words: {word_count}) {preview}...\")\n\n    else:\n        print(\"⚠️  No embeddings available for filtered retrieval demonstration\")"
  },
  {
   "cell_type": "markdown",
   "id": "8ce59a1d",
   "metadata": {},
   "source": [
    "## Evaluate Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97ca62",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 4: EVALUATE IMPACT - COHERENCE AND FILTERED RETRIEVAL\n# ============================================================================\n\n# Metric functions\ndef precision_at_k(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int], k: int = 5) -> float:\n    \"\"\"Precision@K: What % of top-K results are relevant?\"\"\"\n    if k == 0:\n        return 0.0\n\n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n\n    num_relevant_in_k = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n\n    return num_relevant_in_k / k\n\n\ndef recall_at_k(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int], k: int = 5) -> float:\n    \"\"\"Recall@K: What % of all relevant chunks were found in top-K?\"\"\"\n    if len(relevant_chunk_ids) == 0:\n        return 0.0\n\n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n\n    num_relevant_found = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n\n    return num_relevant_found / len(relevant_set)\n\n\ndef mean_reciprocal_rank(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int]) -> float:\n    \"\"\"MRR: How quickly do we find the first relevant result?\"\"\"\n    relevant_set = set(relevant_chunk_ids)\n\n    for rank, chunk_id in enumerate(retrieved_chunk_ids, start=1):\n        if chunk_id in relevant_set:\n            return 1.0 / rank\n\n    return 0.0\n\n\ndef ndcg_at_k(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int], k: int = 5) -> float:\n    \"\"\"NDCG@K: How well-ranked are results? (rewards relevant at top)\"\"\"\n    import math\n    \n    def dcg_score(relevance_scores: List[float]) -> float:\n        \"\"\"Compute DCG from relevance scores.\"\"\"\n        return sum(\n            (2**rel - 1) / math.log2(rank + 2)\n            for rank, rel in enumerate(relevance_scores)\n        )\n\n    if k == 0 or len(relevant_chunk_ids) == 0:\n        return 0.0\n\n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n\n    # Binary relevance: 1 if relevant, 0 if not\n    relevance = [1 if chunk_id in relevant_set else 0 for chunk_id in retrieved_k]\n\n    dcg = dcg_score(relevance)\n    ideal_relevance = sorted(relevance, reverse=True)\n    idcg = dcg_score(ideal_relevance)\n\n    if idcg == 0:\n        return 0.0\n\n    return dcg / idcg\n\n\ndef evaluate_filtering_impact(test_questions: List[Dict],\n                              embeddings_db,\n                              embedding_model: str,\n                              top_k: int = 5) -> Dict:\n    \"\"\"\n    Compare unfiltered vs filtered retrieval.\n    \n    Focus: Precision improvement from metadata filtering\n    \n    Args:\n        test_questions: List of question dicts with 'question' and 'relevant_chunk_ids'\n        embeddings_db: PostgreSQLVectorDB instance\n        embedding_model: Model name for embeddings\n        top_k: Results to return\n        \n    Returns:\n        Dict with baseline, filtered metrics, and improvements\n    \"\"\"\n    \n    unfiltered_results = []\n    filtered_results = []\n    per_query_details = []\n    \n    print(f\"\\nEvaluating metadata filtering on {len(test_questions)} test questions...\")\n    print(\"-\" * 70)\n    \n    for q_idx, q in enumerate(test_questions, 1):\n        query = q['question']\n        relevant_ids = q['relevant_chunk_ids']\n        \n        if not relevant_ids:\n            continue\n        \n        # Baseline: no filters\n        unfiltered_chunks = filtered_retrieval(\n            query,\n            embeddings_db,\n            embedding_model,\n            filters=None,\n            top_k=top_k\n        )\n        unfiltered_ids = [chunk_id for _, _, chunk_id, _ in unfiltered_chunks]\n        \n        # With filtering: require entities (more likely to be relevant)\n        filtered_chunks = filtered_retrieval(\n            query,\n            embeddings_db,\n            embedding_model,\n            filters={'has_entities': True, 'min_word_count': 20},\n            top_k=top_k\n        )\n        filtered_ids = [chunk_id for _, _, chunk_id, _ in filtered_chunks]\n        \n        # Compute metrics\n        unfiltered_metrics = {\n            'precision@5': precision_at_k(unfiltered_ids, relevant_ids, k=5),\n            'recall@5': recall_at_k(unfiltered_ids, relevant_ids, k=5),\n            'mrr': mean_reciprocal_rank(unfiltered_ids, relevant_ids),\n            'ndcg@5': ndcg_at_k(unfiltered_ids, relevant_ids, k=5)\n        }\n        \n        filtered_metrics = {\n            'precision@5': precision_at_k(filtered_ids, relevant_ids, k=5),\n            'recall@5': recall_at_k(filtered_ids, relevant_ids, k=5),\n            'mrr': mean_reciprocal_rank(filtered_ids, relevant_ids),\n            'ndcg@5': ndcg_at_k(filtered_ids, relevant_ids, k=5)\n        }\n        \n        unfiltered_results.append(unfiltered_metrics)\n        filtered_results.append(filtered_metrics)\n        \n        per_query_details.append({\n            'question': query,\n            'unfiltered': unfiltered_metrics,\n            'filtered': filtered_metrics,\n            'unfiltered_count': len(unfiltered_ids),\n            'filtered_count': len(filtered_ids)\n        })\n        \n        if q_idx % max(1, len(test_questions) // 5) == 0:\n            print(f\"  Progress: {q_idx}/{len(test_questions)} queries evaluated\")\n    \n    # Aggregate metrics\n    def aggregate(results):\n        if not results:\n            return {'precision@5': 0, 'recall@5': 0, 'mrr': 0, 'ndcg@5': 0}\n        return {\n            metric: np.mean([r[metric] for r in results])\n            for metric in results[0].keys()\n        }\n    \n    unfiltered_agg = aggregate(unfiltered_results)\n    filtered_agg = aggregate(filtered_results)\n    \n    # Compute improvements\n    improvements = {}\n    for metric in unfiltered_agg.keys():\n        baseline_val = unfiltered_agg[metric]\n        filtered_val = filtered_agg[metric]\n        \n        if baseline_val > 0:\n            improvements[metric] = ((filtered_val - baseline_val) / baseline_val * 100)\n        else:\n            improvements[metric] = 0.0\n    \n    return {\n        'unfiltered': unfiltered_agg,\n        'filtered': filtered_agg,\n        'improvements_pct': improvements,\n        'per_query': per_query_details,\n        'num_queries': len(unfiltered_results)\n    }\n\n\n# Run evaluation\nprint(\"\\n\" + \"=\"*70)\nprint(\"EVALUATION: UNFILTERED VS METADATA-FILTERED RETRIEVAL\")\nprint(\"=\"*70)\n\nif embeddings_db and ground_truth_questions:\n    eval_results = evaluate_filtering_impact(\n        ground_truth_questions,\n        embeddings_db,\n        EMBEDDING_MODEL_ALIAS,\n        top_k=TOP_K\n    )\n\n    # Display results\n    print(\"\\n\" + \"=\"*70)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\"*70)\n\n    print(f\"\\nQueries evaluated: {eval_results['num_queries']}\\n\")\n\n    print(f\"{'Metric':<20} {'Unfiltered':<15} {'Filtered':<15} {'Change':<15}\")\n    print(\"-\" * 65)\n\n    for metric in ['precision@5', 'recall@5', 'ndcg@5', 'mrr']:\n        unfiltered_val = eval_results['unfiltered'][metric]\n        filtered_val = eval_results['filtered'][metric]\n        improvement = eval_results['improvements_pct'][metric]\n        \n        improvement_str = f\"+{improvement:.1f}%\" if improvement >= 0 else f\"{improvement:.1f}%\"\n        print(f\"{metric:<20} {unfiltered_val:<15.4f} {filtered_val:<15.4f} {improvement_str:<15}\")\n\n    # Analysis of which filters matter most\n    print(\"\\n\" + \"=\"*70)\n    print(\"ANALYSIS: FILTERING EFFECTIVENESS\")\n    print(\"=\"*70)\n\n    # Count how many queries benefited from filtering\n    beneficial_queries = sum(\n        1 for q in eval_results['per_query']\n        if q['filtered']['precision@5'] >= q['unfiltered']['precision@5']\n    )\n    \n    print(f\"\\nQueries with improved/maintained Precision@5: {beneficial_queries}/{eval_results['num_queries']}\")\n    print(f\"  ({beneficial_queries/eval_results['num_queries']*100:.1f}% of queries)\")\n\n    # Queries where filtering reduced result count\n    reduced_queries = sum(\n        1 for q in eval_results['per_query']\n        if q['filtered_count'] < q['unfiltered_count']\n    )\n    \n    print(f\"\\nQueries where filtering reduced result count: {reduced_queries}/{eval_results['num_queries']}\")\n    \n    avg_reduction = np.mean([\n        q['unfiltered_count'] - q['filtered_count']\n        for q in eval_results['per_query']\n    ])\n    \n    print(f\"  Average reduction in results: {avg_reduction:.1f} chunks\")\n\n    # Show top improvement queries\n    top_improved = sorted(\n        eval_results['per_query'],\n        key=lambda x: x['filtered']['precision@5'] - x['unfiltered']['precision@5'],\n        reverse=True\n    )\n\n    print(f\"\\nTop 3 queries with largest Precision@5 improvement:\")\n    for i, q in enumerate(top_improved[:3], 1):\n        unf_p5 = q['unfiltered']['precision@5']\n        filt_p5 = q['filtered']['precision@5']\n        improvement = (filt_p5 - unf_p5) * 100\n        print(f\"  [{i}] +{improvement:.1f}% (Precision@5: {unf_p5:.2f} → {filt_p5:.2f})\")\n        print(f\"      Q: {q['question'][:70]}...\")\n\nelse:\n    print(\"⚠️  Cannot evaluate: embeddings or test questions not available\")\n    eval_results = None"
  },
  {
   "cell_type": "markdown",
   "id": "65069507",
   "metadata": {},
   "source": [
    "## Track Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5a186",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 5: EXPERIMENT TRACKING\n# ============================================================================\n\ndef compute_config_hash(config_dict: Dict) -> str:\n    \"\"\"Create deterministic SHA256 hash of a configuration dictionary.\n\n    This enables finding all experiments with identical configurations.\n\n    Args:\n        config_dict: Configuration parameters\n\n    Returns:\n        SHA256 hash string (first 12 characters for readability)\n    \"\"\"\n    config_str = json.dumps(config_dict, sort_keys=True)\n    hash_obj = hashlib.sha256(config_str.encode())\n    return hash_obj.hexdigest()[:12]\n\n\ndef start_experiment(db_connection, experiment_name: str,\n                     notebook_path: str = None,\n                     embedding_model_alias: str = None,\n                     config: Dict = None,\n                     techniques: List[str] = None,\n                     notes: str = None) -> int:\n    \"\"\"Start a new experiment and return its ID for tracking.\n\n    Args:\n        db_connection: PostgreSQL connection\n        experiment_name: Human-readable experiment name\n        notebook_path: Path to the notebook running this experiment\n        embedding_model_alias: Which embedding model is being used\n        config: Dict of configuration parameters\n        techniques: List of techniques being applied\n        notes: Optional notes about the experiment\n\n    Returns:\n        Experiment ID for use in save_metrics() and complete_experiment()\n    \"\"\"\n    if config is None:\n        config = {}\n    if techniques is None:\n        techniques = []\n\n    config_hash = compute_config_hash(config)\n\n    with db_connection.cursor() as cur:\n        cur.execute('''\n            INSERT INTO experiments (\n                experiment_name, notebook_path, embedding_model_alias,\n                config_hash, config_json, techniques_applied, notes, status\n            )\n            VALUES (%s, %s, %s, %s, %s, %s, %s, 'running')\n            RETURNING id\n        ''', (\n            experiment_name,\n            notebook_path,\n            embedding_model_alias,\n            config_hash,\n            json.dumps(config),\n            techniques,\n            notes\n        ))\n        exp_id = cur.fetchone()[0]\n    db_connection.commit()\n    print(f\"✓ Started experiment #{exp_id}: {experiment_name}\")\n    return exp_id\n\n\ndef save_metrics(db_connection, experiment_id: int, metrics_dict: Dict,\n                 export_to_file: bool = True,\n                 export_dir: str = 'data/experiment_results') -> Tuple[bool, str]:\n    \"\"\"Save experiment metrics to database and optionally to JSON file.\n\n    Args:\n        db_connection: PostgreSQL connection\n        experiment_id: ID from start_experiment()\n        metrics_dict: Dict of {metric_name: value, ...}\n        export_to_file: Whether to also save to filesystem JSON\n        export_dir: Directory for JSON exports\n\n    Returns:\n        Tuple of (success: bool, message: str)\n    \"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            for metric_name, metric_data in metrics_dict.items():\n                # Handle both simple floats and nested dicts with details\n                if isinstance(metric_data, dict):\n                    metric_value = metric_data.get('value', 0.0)\n                    metric_details = metric_data.get('details', {})\n                else:\n                    metric_value = metric_data\n                    metric_details = {}\n\n                cur.execute('''\n                    INSERT INTO evaluation_results (\n                        experiment_id, metric_name, metric_value, metric_details_json\n                    )\n                    VALUES (%s, %s, %s, %s)\n                ''', (\n                    experiment_id,\n                    metric_name,\n                    float(metric_value),\n                    json.dumps(metric_details) if metric_details else '{}'\n                ))\n        db_connection.commit()\n\n        # Export to file if requested\n        file_path = None\n        if export_to_file:\n            os.makedirs(export_dir, exist_ok=True)\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            file_path = os.path.join(export_dir, f'experiment_{experiment_id}_{timestamp}.json')\n            with open(file_path, 'w') as f:\n                json.dump({\n                    'experiment_id': experiment_id,\n                    'timestamp': timestamp,\n                    'metrics': metrics_dict\n                }, f, indent=2)\n\n        msg = f\"✓ Saved {len(metrics_dict)} metrics for experiment #{experiment_id}\"\n        if file_path:\n            msg += f\" to {file_path}\"\n        print(msg)\n        return True, msg\n    except Exception as e:\n        msg = f\"✗ Failed to save metrics: {e}\"\n        print(msg)\n        db_connection.rollback()\n        return False, msg\n\n\ndef complete_experiment(db_connection, experiment_id: int,\n                       status: str = 'completed',\n                       notes: str = None) -> bool:\n    \"\"\"Mark an experiment as complete.\n\n    Args:\n        db_connection: PostgreSQL connection\n        experiment_id: ID returned from start_experiment()\n        status: 'completed' or 'failed'\n        notes: Optional update to notes field\n\n    Returns:\n        True if successful\n    \"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            if notes:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, notes = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, notes, experiment_id))\n            else:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, experiment_id))\n        db_connection.commit()\n        print(f\"✓ Experiment #{experiment_id} marked as {status}\")\n        return True\n    except Exception as e:\n        print(f\"✗ Failed to complete experiment: {e}\")\n        db_connection.rollback()\n        return False\n\n\n# ============================================================================\n# RUN EXPERIMENT TRACKING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRACKING EXPERIMENT\")\nprint(\"=\"*70)\n\nif eval_results:\n    # Prepare configuration\n    config_dict = {\n        'embedding_model_alias': EMBEDDING_MODEL_ALIAS,\n        'chunk_size': CHUNK_SIZE,\n        'chunk_overlap': CHUNK_OVERLAP,\n        'split_strategy': SPLIT_STRATEGY,\n        'top_k': TOP_K,\n        'filtering_criteria': {\n            'has_entities': True,\n            'min_word_count': 20\n        },\n        'num_test_queries': eval_results['num_queries'],\n    }\n\n    config_hash = compute_config_hash(config_dict)\n\n    print(f\"\\nExperiment Configuration:\")\n    print(f\"  Name: {EXPERIMENT_NAME}\")\n    print(f\"  Embedding Model: {EMBEDDING_MODEL_ALIAS}\")\n    print(f\"  Chunk Size: {CHUNK_SIZE}\")\n    print(f\"  Chunk Overlap: {CHUNK_OVERLAP}\")\n    print(f\"  Split Strategy: {SPLIT_STRATEGY}\")\n    print(f\"  Config Hash: {config_hash}\")\n    print(f\"  Test Queries: {eval_results['num_queries']}\\n\")\n\n    # Start experiment tracking\n    experiment_id = start_experiment(\n        db_connection,\n        experiment_name=EXPERIMENT_NAME,\n        notebook_path='advanced-techniques/08-semantic-chunking-and-metadata.ipynb',\n        embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n        config=config_dict,\n        techniques=TECHNIQUES_APPLIED,\n        notes=f'Semantic chunking with metadata filtering evaluation on {eval_results[\"num_queries\"]} queries. '\n              f'Techniques: {\", \".join(TECHNIQUES_APPLIED)}'\n    )\n\n    # Prepare metrics for storage\n    metrics_to_store = {}\n\n    # Unfiltered baseline metrics\n    for metric_name, metric_value in eval_results['unfiltered'].items():\n        metrics_to_store[f'baseline_{metric_name}'] = float(metric_value)\n\n    # Filtered metrics\n    for metric_name, metric_value in eval_results['filtered'].items():\n        metrics_to_store[f'filtered_{metric_name}'] = float(metric_value)\n\n    # Improvement percentages\n    for metric_name, improvement_pct in eval_results['improvements_pct'].items():\n        metrics_to_store[f'improvement_pct_{metric_name}'] = float(improvement_pct)\n\n    # Configuration and metadata\n    metrics_to_store['num_queries_evaluated'] = eval_results['num_queries']\n    metrics_to_store['config_hash'] = config_hash\n    \n    # Store semantic chunking statistics\n    if chunks_with_metadata:\n        avg_chunk_size = np.mean([len(c['text']) for c in chunks_with_metadata])\n        avg_entities = np.mean([c['metadata']['entity_count'] for c in chunks_with_metadata])\n        avg_words = np.mean([c['metadata']['word_count'] for c in chunks_with_metadata])\n        \n        metrics_to_store['semantic_chunking_avg_chunk_size'] = float(avg_chunk_size)\n        metrics_to_store['semantic_chunking_avg_entities'] = float(avg_entities)\n        metrics_to_store['semantic_chunking_avg_words'] = float(avg_words)\n        metrics_to_store['semantic_chunking_num_chunks'] = len(chunks_with_metadata)\n\n    # Save metrics\n    print(\"\\nSaving metrics to database...\\n\")\n    success, message = save_metrics(db_connection, experiment_id, metrics_to_store, export_to_file=True)\n\n    # Complete experiment\n    if success:\n        notes = f\"Successfully evaluated semantic chunking with metadata filtering on {eval_results['num_queries']} queries. \"\n        notes += f\"Precision@5 improvement: {eval_results['improvements_pct']['precision@5']:.2f}%. \"\n        notes += f\"Techniques applied: {', '.join(TECHNIQUES_APPLIED)}\"\n\n        complete_experiment(db_connection, experiment_id, status='completed', notes=notes)\n\n        # Display results summary\n        print(\"\\n\" + \"=\"*70)\n        print(\"EXPERIMENT RESULTS SUMMARY\")\n        print(\"=\"*70)\n\n        print(f\"\\nExperiment ID: {experiment_id}\")\n        print(f\"Experiment Name: {EXPERIMENT_NAME}\")\n        print(f\"Status: Completed\")\n        print(f\"Config Hash: {config_hash}\")\n\n        print(f\"\\nTechniques Applied:\")\n        for technique in TECHNIQUES_APPLIED:\n            print(f\"  - {technique}\")\n\n        print(f\"\\nKey Improvements:\")\n        for metric_name, improvement_pct in eval_results['improvements_pct'].items():\n            sign = \"+\" if improvement_pct > 0 else \"\"\n            baseline_val = eval_results['unfiltered'][metric_name]\n            filtered_val = eval_results['filtered'][metric_name]\n            print(f\"  {metric_name}:\")\n            print(f\"    Baseline (unfiltered): {baseline_val:.4f}\")\n            print(f\"    With filtering:        {filtered_val:.4f}\")\n            print(f\"    Improvement:           {sign}{improvement_pct:.2f}%\")\n\n        if chunks_with_metadata:\n            print(f\"\\nSemantic Chunking Statistics:\")\n            print(f\"  Average chunk size: {avg_chunk_size:.0f} characters\")\n            print(f\"  Average entities: {avg_entities:.1f} per chunk\")\n            print(f\"  Average words: {avg_words:.1f} per chunk\")\n            print(f\"  Total chunks created: {len(chunks_with_metadata)}\")\n\n        print(f\"\\nResults exported to:\")\n        print(f\"  Database: evaluation_results table (experiment_id={experiment_id})\")\n        print(f\"  JSON: data/experiment_results/experiment_{experiment_id}_*.json\")\n\n        print(\"\\n\" + \"=\"*70)\n        print(\"NEXT STEPS\")\n        print(\"=\"*70)\n        print(\"\\n1. Review semantic chunking coherence improvements\")\n        print(\"2. Compare metadata filter effectiveness:\")\n        print(\"   - Try different combinations (has_dates, has_questions, etc.)\")\n        print(\"   - Adjust min_word_count threshold\")\n        print(\"3. Combine with other techniques:\")\n        print(\"   - Query expansion (notebook 06)\")\n        print(\"   - Reranking (notebook 05)\")\n        print(\"   - Hybrid search (notebook 07)\")\n        print(\"4. Evaluate on different question types in evaluation-lab/01\")\n        print(\"5. Compare all advanced techniques using evaluation-lab/04\")\n\n    else:\n        print(\"\\n✗ Failed to track experiment\")\n        complete_experiment(db_connection, experiment_id, status='failed', notes='Failed to save metrics')\n\nelse:\n    print(\"⚠️  Cannot track experiment: evaluation results not available\")\n\n# Close database connection\nprint(\"\\n\\nClosing database connection...\")\nif embeddings_db:\n    embeddings_db.close()\ndb_connection.close()\nprint(\"✓ All connections closed\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}