{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2a2f03",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. ✅ foundation/00-setup-postgres-schema.ipynb\n",
    "2. ✅ foundation/02-rag-postgresql-persistent.ipynb\n",
    "3. ✅ evaluation-lab/01-create-ground-truth-human-in-loop.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d9cb3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_ALIAS = \"all-minilm-l6-v2\"\n",
    "TOP_K = 5\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "EXPERIMENT_NAME = \"citation-tracking-transparency\"\n",
    "TECHNIQUES_APPLIED = [\"vector_retrieval\", \"citation_tracking\", \"confidence_scoring\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44c3aa",
   "metadata": {},
   "source": [
    "## Load Embeddings from Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19cbcb",
   "metadata": {},
   "outputs": [],
   "source": "import psycopg2\nimport psycopg2.extras\nimport ollama\nimport json\nimport pandas as pd\nimport numpy as np\nimport hashlib\nimport re\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Optional\nimport os\n\n# PostgreSQL connection\nPOSTGRES_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'postgres',\n    'password': 'postgres',\n}\n\n# Create database connection\ntry:\n    db_connection = psycopg2.connect(\n        host=POSTGRES_CONFIG['host'],\n        port=POSTGRES_CONFIG['port'],\n        database=POSTGRES_CONFIG['database'],\n        user=POSTGRES_CONFIG['user'],\n        password=POSTGRES_CONFIG['password']\n    )\n    print(\"✓ Connected to PostgreSQL\")\nexcept psycopg2.OperationalError as e:\n    print(f\"✗ Failed to connect to PostgreSQL: {e}\")\n    raise\n\n# ============================================================================\n# PART 1: LOAD EMBEDDINGS FROM REGISTRY\n# ============================================================================\n\ndef list_available_embeddings(db_connection) -> pd.DataFrame:\n    \"\"\"Query embedding_registry to show available models with metadata.\"\"\"\n    query = '''\n        SELECT\n            model_alias,\n            model_name,\n            dimension,\n            embedding_count,\n            chunk_source_dataset,\n            chunk_size_config,\n            created_at,\n            last_accessed\n        FROM embedding_registry\n        ORDER BY created_at DESC\n    '''\n    return pd.read_sql(query, db_connection)\n\n\nclass PostgreSQLVectorDB:\n    \"\"\"Helper to load embeddings from PostgreSQL without regeneration.\"\"\"\n\n    def __init__(self, config, table_name, preserve_existing=True):\n        self.config = config\n        self.table_name = table_name\n        self.conn = psycopg2.connect(\n            host=config['host'],\n            port=config['port'],\n            database=config['database'],\n            user=config['user'],\n            password=config['password']\n        )\n        print(f'✓ Connected to table: {table_name}')\n\n    def get_chunk_count(self):\n        \"\"\"How many embeddings are stored?\"\"\"\n        with self.conn.cursor() as cur:\n            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n            return cur.fetchone()[0]\n\n    def similarity_search(self, query_embedding, top_n=5):\n        \"\"\"Retrieve most similar chunks using pgvector.\n\n        Returns:\n            List of tuples: (chunk_text, similarity_score, chunk_id)\n        \"\"\"\n        with self.conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n            cur.execute(f'''\n                SELECT id,\n                       content as chunk_text,\n                       1 - (embedding <=> %s::vector) as similarity\n                FROM {self.table_name}\n                ORDER BY embedding <=> %s::vector\n                LIMIT %s\n            ''', (query_embedding, query_embedding, top_n))\n\n            results = cur.fetchall()\n            return [(row['chunk_text'], row['similarity'], row['id']) for row in results]\n\n    def close(self):\n        if self.conn:\n            self.conn.close()\n\n\ndef load_or_generate(db_connection, embedding_model_alias, preserve_existing=True):\n    \"\"\"Load embeddings from registry OR show instructions if not available.\"\"\"\n\n    print(f\"\\n{'='*70}\")\n    print(f\"Checking for embeddings: '{embedding_model_alias}'...\")\n    print(f\"{'='*70}\\n\")\n\n    try:\n        with db_connection.cursor() as cur:\n            cur.execute('''\n                SELECT id, dimension, embedding_count, created_at, metadata_json\n                FROM embedding_registry\n                WHERE model_alias = %s\n            ''', (embedding_model_alias,))\n            registry_entry = cur.fetchone()\n    except Exception as e:\n        print(f\"Could not query registry: {e}\")\n        print(\"Make sure foundation/00-setup-postgres-schema.ipynb has been run.\")\n        return None\n\n    # Case A: Embeddings exist\n    if registry_entry:\n        reg_id, dimension, embedding_count, created_at, metadata_json = registry_entry\n\n        print(f\"✓ FOUND EXISTING EMBEDDINGS\")\n        print(f\"  Model:      {embedding_model_alias}\")\n        print(f\"  Count:      {embedding_count:,} embeddings\")\n        print(f\"  Dimension:  {dimension}\")\n        print(f\"  Created:    {created_at}\")\n        print(f\"\\n  TIME SAVINGS:\")\n        print(f\"    Loading:       <1 second\")\n        print(f\"    Regenerating:  ~50+ minutes\")\n        print(f\"    ➜ You save 50+ minutes by loading!\\n\")\n\n        if preserve_existing:\n            # Auto-load (for scripts/notebooks)\n            print(\"Loading existing embeddings...\\n\")\n\n            try:\n                table_name = f'embeddings_{embedding_model_alias.replace(\".\", \"_\")}'\n\n                db_instance = PostgreSQLVectorDB(\n                    config=POSTGRES_CONFIG,\n                    table_name=table_name,\n                    preserve_existing=True\n                )\n\n                count = db_instance.get_chunk_count()\n                print(f\"✓ LOADED SUCCESSFULLY\")\n                print(f\"  Embeddings: {count:,}\")\n                print(f\"  Table: {table_name}\")\n                print(f\"  Status: Ready for retrieval\\n\")\n\n                return db_instance\n\n            except Exception as e:\n                print(f\"\\n✗ Error loading embeddings: {e}\")\n                print(f\"\\nTroubleshooting:\")\n                print(f\"  1. Verify PostgreSQL is running\")\n                print(f\"  2. Check POSTGRES_CONFIG settings\")\n                print(f\"  3. Run foundation/02 to generate embeddings first\")\n                return None\n\n    # Case B: No embeddings found\n    else:\n        print(f\"✗ NO EMBEDDINGS FOUND\")\n        print(f\"  Model: {embedding_model_alias}\")\n        print(f\"\\nTo create embeddings, run:\")\n        print(f\"  foundation/02-rag-postgresql-persistent.ipynb\")\n        print(f\"\\nThen come back and re-run this cell.\\n\")\n        return None\n\n\n# Discover and load embeddings\nprint(\"Step 1: Discovering available embeddings...\\n\")\navailable = list_available_embeddings(db_connection)\n\nif available.empty:\n    print(\"⚠️  No embeddings found in registry yet.\")\n    print(\"Run foundation/02-rag-postgresql-persistent.ipynb first.\\n\")\nelse:\n    print(\"Available embeddings:\")\n    print(available.to_string(index=False))\n    print()\n\n# Load embeddings using the pattern\nprint(\"\\nStep 2: Loading embeddings using load-or-generate pattern...\\n\")\nembeddings_db = load_or_generate(\n    db_connection=db_connection,\n    embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n    preserve_existing=True  # Auto-load if available\n)\n\nif embeddings_db:\n    print(\"✓ Success! Embeddings loaded and ready for retrieval.\")\nelse:\n    print(\"⚠️  Could not load embeddings. See instructions above.\")\n    embeddings_db = None"
  },
  {
   "cell_type": "markdown",
   "id": "2425ab5a",
   "metadata": {},
   "source": [
    "## Implement Citation Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ba163",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 2: CITATION TRACKING IMPLEMENTATION\n# ============================================================================\n\ndef retrieve_with_citations(query: str, \n                           embeddings_db: PostgreSQLVectorDB, \n                           embedding_model: str, \n                           top_k: int = 5) -> List[Dict]:\n    \"\"\"\n    Retrieve chunks and prepare citation data.\n    \n    For each retrieved chunk, extract source information and assign citation ID.\n    This enables tracking which chunks contributed to the answer.\n    \n    Args:\n        query: User question\n        embeddings_db: PostgreSQLVectorDB instance\n        embedding_model: Model alias for embeddings\n        top_k: Number of results to retrieve\n        \n    Returns:\n        List of dicts with citation info:\n        {\n            'chunk_id': int,\n            'chunk_text': str,\n            'similarity_score': float,\n            'citation_id': str,  # e.g., \"[1]\", \"[2]\"\n            'source': str,       # e.g., \"Article: Einstein\"\n            'rank': int          # Position in retrieval results\n        }\n    \"\"\"\n    # Embed the query\n    query_emb = ollama.embed(model=embedding_model, input=query)['embeddings'][0]\n    \n    # Retrieve most similar chunks\n    results = embeddings_db.similarity_search(query_emb, top_n=top_k)\n    \n    citations = []\n    for idx, (chunk_text, score, chunk_id) in enumerate(results, start=1):\n        # Extract source from chunk text (format: \"Article: Title\\n\\nContent...\")\n        source = \"Unknown Source\"\n        if chunk_text.startswith(\"Article:\"):\n            # Get first line as source\n            first_line = chunk_text.split('\\n')[0]\n            source = first_line  # \"Article: Title\"\n        elif chunk_text.startswith(\"Section:\"):\n            first_line = chunk_text.split('\\n')[0]\n            source = first_line\n        else:\n            # Try to extract from content\n            lines = chunk_text.split('\\n')\n            for line in lines:\n                if line.strip() and len(line.strip()) < 100:\n                    source = line.strip()[:80]\n                    break\n        \n        citations.append({\n            'chunk_id': chunk_id,\n            'chunk_text': chunk_text,\n            'similarity_score': float(score),\n            'citation_id': f\"[{idx}]\",\n            'source': source,\n            'rank': idx\n        })\n    \n    return citations\n\n\ndef generate_with_citations(query: str, \n                           citations: List[Dict], \n                           llm_model: str = 'llama3.2:1b') -> Dict:\n    \"\"\"\n    Generate answer with inline citations.\n    \n    Takes retrieved chunks with citation IDs and asks the LLM to answer\n    using only those sources, including citation markers [1], [2], etc.\n    \n    Args:\n        query: User question\n        citations: List of citation dicts from retrieve_with_citations()\n        llm_model: Ollama model to use for generation\n        \n    Returns:\n        dict with:\n        {\n            'answer': str,           # Answer with [1], [2] citation markers\n            'confidence': float,     # Min similarity score of retrieved chunks\n            'sources': List[str],    # Source references with citations\n            'citations_used': List   # Full citation data\n        }\n    \"\"\"\n    # Build context from citations with inline markers\n    context_parts = []\n    for c in citations:\n        # Include first 500 chars of chunk with citation marker\n        preview = c['chunk_text'][:500]\n        # Clean up preview\n        if len(c['chunk_text']) > 500:\n            preview = preview.rsplit(' ', 1)[0] + '...'\n        context_parts.append(f\"{c['citation_id']} {c['source']}\\n{preview}\")\n    \n    context = \"\\n\\n\".join(context_parts)\n    \n    # Build prompt requesting citations\n    prompt = f\"\"\"Answer the following question using ONLY the provided sources. \nYou MUST include citation markers [1], [2], [3], etc. in your answer to show which source each fact comes from.\nReference the sources by their citation ID in square brackets.\n\nQuestion: {query}\n\nSources (use [1], [2], [3], etc. to cite):\n{context}\n\nAnswer (include citation markers for each fact):\"\"\"\n    \n    # Generate answer with citations\n    response = ollama.chat(\n        model=llm_model,\n        messages=[{'role': 'user', 'content': prompt}]\n    )\n    \n    answer = response['message']['content']\n    \n    # Confidence: minimum similarity score (weakest link in the chain)\n    confidence = min(c['similarity_score'] for c in citations) if citations else 0.0\n    \n    # Build source list with citations\n    sources = [\n        f\"{c['citation_id']} {c['source']}\"\n        for c in citations\n    ]\n    \n    return {\n        'answer': answer,\n        'confidence': confidence,\n        'sources': sources,\n        'citations_used': citations\n    }\n\n\ndef validate_citations(answer: str, citations: List[Dict]) -> Dict:\n    \"\"\"\n    Check if citations in answer actually match provided sources.\n    \n    Validates that:\n    - All citations used in the answer refer to available sources\n    - Citation IDs are properly formatted [1], [2], etc.\n    - Helps detect hallucinated or incorrect citations\n    \n    Args:\n        answer: Generated answer text\n        citations: List of citation dicts with citation_id\n        \n    Returns:\n        dict with:\n        {\n            'valid_citations': List[str],     # Citations that match sources\n            'invalid_citations': List[str],   # Citations not in sources\n            'unused_sources': List[str],      # Sources not cited in answer\n            'citation_accuracy': float        # Fraction of citations that are valid\n        }\n    \"\"\"\n    # Extract all citation IDs from answer (e.g., [1], [2], [3])\n    cited_ids = set(re.findall(r'\\[\\d+\\]', answer))\n    \n    # Available citation IDs from sources\n    available_ids = set(c['citation_id'] for c in citations)\n    \n    # Find valid vs invalid citations\n    valid = cited_ids & available_ids  # Intersection\n    invalid = cited_ids - available_ids  # Cited but not available\n    unused = available_ids - cited_ids  # Available but not cited\n    \n    # Calculate accuracy: what fraction of citations are valid\n    citation_accuracy = 0.0\n    if cited_ids:\n        citation_accuracy = len(valid) / len(cited_ids)\n    \n    return {\n        'valid_citations': sorted(list(valid)),\n        'invalid_citations': sorted(list(invalid)),\n        'unused_sources': sorted(list(unused)),\n        'citation_accuracy': citation_accuracy\n    }\n\n\n# Load test questions from ground truth\nprint(\"\\nLoading ground truth test set...\")\nground_truth_questions = []\n\nwith db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n    cur.execute('''\n        SELECT\n            id,\n            question,\n            relevant_chunk_ids,\n            quality_rating,\n            source_type\n        FROM evaluation_groundtruth\n        WHERE quality_rating = 'good'\n        ORDER BY id\n    ''')\n\n    for row in cur.fetchall():\n        ground_truth_questions.append({\n            'id': row['id'],\n            'question': row['question'],\n            'relevant_chunk_ids': row['relevant_chunk_ids'] or [],\n            'quality_rating': row['quality_rating'],\n            'source_type': row['source_type']\n        })\n\nprint(f\"✓ Loaded {len(ground_truth_questions)} ground truth questions\\n\")\n\nif ground_truth_questions:\n    print(f\"Sample question: {ground_truth_questions[0]['question'][:80]}...\")\n    print(f\"Relevant chunks: {ground_truth_questions[0]['relevant_chunk_ids'][:3]}\\n\")\n\n# Demonstrate citation tracking on first test question\nif embeddings_db and ground_truth_questions:\n    print(\"=\" * 70)\n    print(\"DEMONSTRATING CITATION TRACKING\")\n    print(\"=\" * 70)\n    \n    test_question = ground_truth_questions[0]['question']\n    print(f\"\\nQuery: {test_question}\\n\")\n    \n    # Step 1: Retrieve with citations\n    print(\"Step 1: Retrieving chunks with citation data...\")\n    citations = retrieve_with_citations(test_question, embeddings_db, EMBEDDING_MODEL_ALIAS, top_k=TOP_K)\n    \n    print(f\"✓ Retrieved {len(citations)} chunks with citation IDs:\")\n    for c in citations:\n        source_preview = c['source'][:60]\n        print(f\"  {c['citation_id']} (score: {c['similarity_score']:.4f}) {source_preview}\")\n    \n    # Step 2: Generate answer with citations\n    print(\"\\nStep 2: Generating answer with inline citations...\")\n    result = generate_with_citations(test_question, citations)\n    \n    print(f\"\\nGenerated Answer:\\n{result['answer']}\")\n    print(f\"\\nConfidence (min similarity): {result['confidence']:.4f}\")\n    print(f\"\\nSources:\")\n    for source in result['sources']:\n        print(f\"  {source}\")\n    \n    # Step 3: Validate citations\n    print(\"\\nStep 3: Validating citations...\")\n    validation = validate_citations(result['answer'], citations)\n    \n    print(f\"  Valid citations: {validation['valid_citations']}\")\n    print(f\"  Invalid citations: {validation['invalid_citations']}\")\n    print(f\"  Unused sources: {validation['unused_sources']}\")\n    print(f\"  Citation accuracy: {validation['citation_accuracy']:.2%}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "id": "71a8e7ec",
   "metadata": {},
   "source": [
    "## Evaluate Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd331ce",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 3: EVALUATE CITATIONS\n# ============================================================================\n\ndef evaluate_citation_tracking(test_questions: List[Dict], \n                              embeddings_db: PostgreSQLVectorDB, \n                              embedding_model: str,\n                              llm_model: str = 'llama3.2:1b') -> Tuple[Dict, List[Dict]]:\n    \"\"\"\n    Evaluate citation quality across a test set.\n    \n    For each test question:\n    1. Retrieve chunks with citations\n    2. Generate answer with citations\n    3. Validate citation accuracy\n    4. Aggregate metrics\n    \n    Args:\n        test_questions: List of test question dicts\n        embeddings_db: PostgreSQLVectorDB instance\n        embedding_model: Model for embedding queries\n        llm_model: Model for answer generation\n        \n    Returns:\n        Tuple of (aggregated_metrics, per_query_results)\n        \n        aggregated_metrics dict with:\n        {\n            'avg_confidence': float,           # Average min similarity score\n            'avg_citation_accuracy': float,    # % of citations that are valid\n            'avg_sources_used': float,         # Avg number of sources cited\n            'avg_invalid_citations': float,    # Avg number of hallucinated citations\n        }\n        \n        per_query_results list with per-query metrics\n    \"\"\"\n    results = []\n    \n    print(f\"\\nEvaluating citation tracking on {len(test_questions)} test questions...\")\n    print(\"=\" * 70)\n    \n    for i, q in enumerate(test_questions, 1):\n        query = q['question']\n        \n        # Progress indicator\n        if i % max(1, len(test_questions) // 10) == 0:\n            print(f\"Progress: {i}/{len(test_questions)} queries processed...\")\n        \n        try:\n            # Step 1: Retrieve with citations\n            citations = retrieve_with_citations(query, embeddings_db, embedding_model, top_k=TOP_K)\n            \n            # Step 2: Generate with citations\n            generated = generate_with_citations(query, citations, llm_model=llm_model)\n            \n            # Step 3: Validate citations\n            validation = validate_citations(generated['answer'], citations)\n            \n            # Collect metrics\n            results.append({\n                'query': query[:100],  # Truncate for display\n                'confidence': generated['confidence'],\n                'citation_accuracy': validation['citation_accuracy'],\n                'sources_used': len(validation['valid_citations']),\n                'sources_provided': len(citations),\n                'invalid_citations': len(validation['invalid_citations']),\n                'unused_sources': len(validation['unused_sources']),\n                'answer_preview': generated['answer'][:150]\n            })\n        \n        except Exception as e:\n            print(f\"\\n  Warning: Failed to evaluate query {i}: {e}\")\n            continue\n    \n    print(\"=\" * 70)\n    print(f\"✓ Evaluation complete ({len(results)} queries)\\n\")\n    \n    # Aggregate metrics\n    if not results:\n        print(\"⚠️  No queries evaluated\")\n        return {\n            'avg_confidence': 0.0,\n            'avg_citation_accuracy': 0.0,\n            'avg_sources_used': 0.0,\n            'avg_invalid_citations': 0.0\n        }, []\n    \n    metrics = {\n        'avg_confidence': np.mean([r['confidence'] for r in results]),\n        'avg_citation_accuracy': np.mean([r['citation_accuracy'] for r in results]),\n        'avg_sources_used': np.mean([r['sources_used'] for r in results]),\n        'avg_invalid_citations': np.mean([r['invalid_citations'] for r in results]),\n        'min_confidence': np.min([r['confidence'] for r in results]),\n        'max_confidence': np.max([r['confidence'] for r in results]),\n        'total_queries_evaluated': len(results)\n    }\n    \n    return metrics, results\n\n\n# Run evaluation\nprint(\"\\n\" + \"=\" * 70)\nprint(\"EVALUATING CITATION TRACKING\")\nprint(\"=\" * 70)\n\nif embeddings_db and ground_truth_questions:\n    evaluation_metrics, per_query_results = evaluate_citation_tracking(\n        ground_truth_questions,\n        embeddings_db,\n        EMBEDDING_MODEL_ALIAS,\n        llm_model='llama3.2:1b'\n    )\n    \n    # Display results\n    print(\"\\n\" + \"=\" * 70)\n    print(\"CITATION TRACKING EVALUATION RESULTS\")\n    print(\"=\" * 70)\n    \n    print(f\"\\nQueries Evaluated: {evaluation_metrics['total_queries_evaluated']}\")\n    print(f\"\\nConfidence (Minimum Similarity Score):\")\n    print(f\"  Average:  {evaluation_metrics['avg_confidence']:.4f}\")\n    print(f\"  Min:      {evaluation_metrics['min_confidence']:.4f}\")\n    print(f\"  Max:      {evaluation_metrics['max_confidence']:.4f}\")\n    \n    print(f\"\\nCitation Accuracy:\")\n    print(f\"  Average:  {evaluation_metrics['avg_citation_accuracy']:.2%}\")\n    print(f\"  (Fraction of citations that match provided sources)\")\n    \n    print(f\"\\nSource Usage:\")\n    print(f\"  Avg sources cited:    {evaluation_metrics['avg_sources_used']:.2f} / {TOP_K}\")\n    print(f\"  Avg unused sources:   {TOP_K - evaluation_metrics['avg_sources_used']:.2f}\")\n    \n    print(f\"\\nCitation Validity:\")\n    print(f\"  Avg invalid citations: {evaluation_metrics['avg_invalid_citations']:.2f}\")\n    print(f\"  (Hallucinated citations not in retrieved sources)\")\n    \n    # Show confidence distribution\n    print(f\"\\n\" + \"=\" * 70)\n    print(\"CONFIDENCE DISTRIBUTION ANALYSIS\")\n    print(\"=\" * 70)\n    \n    confidences = [r['confidence'] for r in per_query_results]\n    \n    # Categorize by confidence threshold\n    high_confidence = sum(1 for c in confidences if c >= 0.7)\n    medium_confidence = sum(1 for c in confidences if 0.5 <= c < 0.7)\n    low_confidence = sum(1 for c in confidences if c < 0.5)\n    \n    print(f\"\\nHigh Confidence (>= 0.70):    {high_confidence} queries ({high_confidence/len(confidences)*100:.1f}%)\")\n    print(f\"Medium Confidence (0.50-0.70): {medium_confidence} queries ({medium_confidence/len(confidences)*100:.1f}%)\")\n    print(f\"Low Confidence (< 0.50):      {low_confidence} queries ({low_confidence/len(confidences)*100:.1f}%)\")\n    \n    # Recommendations\n    print(f\"\\n\" + \"=\" * 70)\n    print(\"TRANSPARENCY & USER TRUST ASSESSMENT\")\n    print(\"=\" * 70)\n    \n    # Accuracy insights\n    if evaluation_metrics['avg_citation_accuracy'] > 0.9:\n        print(f\"\\n✓ Excellent: {evaluation_metrics['avg_citation_accuracy']:.0%} of citations are valid\")\n        print(f\"  Users can trust the citations in answers\")\n    elif evaluation_metrics['avg_citation_accuracy'] > 0.75:\n        print(f\"\\n+ Good: {evaluation_metrics['avg_citation_accuracy']:.0%} of citations are valid\")\n        print(f\"  Most citations are accurate, minor hallucinations detected\")\n    else:\n        print(f\"\\n- Poor: {evaluation_metrics['avg_citation_accuracy']:.0%} of citations are valid\")\n        print(f\"  Consider adding citation validation to filter hallucinations\")\n    \n    # Confidence insights\n    if evaluation_metrics['avg_confidence'] > 0.7:\n        print(f\"\\n✓ High confidence retrievals ({evaluation_metrics['avg_confidence']:.3f})\")\n        print(f\"  Users can rely on the similarity scores\")\n    elif evaluation_metrics['avg_confidence'] > 0.5:\n        print(f\"\\n+ Medium confidence ({evaluation_metrics['avg_confidence']:.3f})\")\n        print(f\"  Some weaker retrievals - consider reranking or filtering\")\n    else:\n        print(f\"\\n- Low confidence ({evaluation_metrics['avg_confidence']:.3f})\")\n        print(f\"  Consider: lower TOP_K, apply reranking, or improve query expansion\")\n    \n    # Coverage insights\n    avg_coverage = evaluation_metrics['avg_sources_used'] / TOP_K\n    if avg_coverage > 0.8:\n        print(f\"\\n✓ High source coverage ({avg_coverage:.0%} of retrieved sources cited)\")\n        print(f\"  Answers use most available information\")\n    elif avg_coverage > 0.5:\n        print(f\"\\n+ Moderate coverage ({avg_coverage:.0%} of sources cited)\")\n        print(f\"  Some sources unused - check for redundancy\")\n    else:\n        print(f\"\\n- Low coverage ({avg_coverage:.0%} of sources cited)\")\n        print(f\"  Many unused sources - may indicate retrieval quality issues\")\n    \n    print(f\"\\n\" + \"=\" * 70)\n\nelse:\n    print(\"⚠️  Cannot evaluate: embeddings or test questions not available\")\n    evaluation_metrics = None\n    per_query_results = None"
  },
  {
   "cell_type": "markdown",
   "id": "a1ca38f2",
   "metadata": {},
   "source": [
    "## Track Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a702fc",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 4: EXPERIMENT TRACKING\n# ============================================================================\n\ndef compute_config_hash(config_dict: Dict) -> str:\n    \"\"\"Create deterministic SHA256 hash of a configuration dictionary.\n    \n    Enables finding all experiments with identical configurations.\n    \n    Args:\n        config_dict: Configuration parameters\n        \n    Returns:\n        SHA256 hash string (first 12 characters)\n    \"\"\"\n    config_str = json.dumps(config_dict, sort_keys=True)\n    hash_obj = hashlib.sha256(config_str.encode())\n    return hash_obj.hexdigest()[:12]\n\n\ndef start_experiment(db_connection, experiment_name: str,\n                     notebook_path: str = None,\n                     embedding_model_alias: str = None,\n                     config: Dict = None,\n                     techniques: List[str] = None,\n                     notes: str = None) -> int:\n    \"\"\"Start a new experiment and return its ID for tracking.\n    \n    Args:\n        db_connection: PostgreSQL connection\n        experiment_name: Human-readable experiment name\n        notebook_path: Path to the notebook running this experiment\n        embedding_model_alias: Which embedding model is being used\n        config: Dict of configuration parameters\n        techniques: List of techniques being applied\n        notes: Optional notes about the experiment\n        \n    Returns:\n        Experiment ID for use in save_metrics() and complete_experiment()\n    \"\"\"\n    if config is None:\n        config = {}\n    if techniques is None:\n        techniques = []\n\n    config_hash = compute_config_hash(config)\n\n    with db_connection.cursor() as cur:\n        cur.execute('''\n            INSERT INTO experiments (\n                experiment_name, notebook_path, embedding_model_alias,\n                config_hash, config_json, techniques_applied, notes, status\n            )\n            VALUES (%s, %s, %s, %s, %s, %s, %s, 'running')\n            RETURNING id\n        ''', (\n            experiment_name,\n            notebook_path,\n            embedding_model_alias,\n            config_hash,\n            json.dumps(config),\n            techniques,\n            notes\n        ))\n        exp_id = cur.fetchone()[0]\n    db_connection.commit()\n    print(f\"✓ Started experiment #{exp_id}: {experiment_name}\")\n    return exp_id\n\n\ndef save_metrics(db_connection, experiment_id: int, metrics_dict: Dict,\n                 export_to_file: bool = True,\n                 export_dir: str = 'data/experiment_results') -> Tuple[bool, str]:\n    \"\"\"Save experiment metrics to database and optionally to JSON file.\n    \n    Args:\n        db_connection: PostgreSQL connection\n        experiment_id: ID from start_experiment()\n        metrics_dict: Dict of {metric_name: value, ...}\n        export_to_file: Whether to also save to filesystem JSON\n        export_dir: Directory for JSON exports\n        \n    Returns:\n        Tuple of (success: bool, message: str)\n    \"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            for metric_name, metric_data in metrics_dict.items():\n                # Handle both simple floats and nested dicts with details\n                if isinstance(metric_data, dict):\n                    metric_value = metric_data.get('value', 0.0)\n                    metric_details = metric_data.get('details', {})\n                else:\n                    metric_value = metric_data\n                    metric_details = {}\n\n                cur.execute('''\n                    INSERT INTO evaluation_results (\n                        experiment_id, metric_name, metric_value, metric_details_json\n                    )\n                    VALUES (%s, %s, %s, %s)\n                ''', (\n                    experiment_id,\n                    metric_name,\n                    float(metric_value),\n                    json.dumps(metric_details) if metric_details else '{}'\n                ))\n        db_connection.commit()\n\n        # Export to file if requested\n        file_path = None\n        if export_to_file:\n            os.makedirs(export_dir, exist_ok=True)\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            file_path = os.path.join(export_dir, f'experiment_{experiment_id}_{timestamp}.json')\n            with open(file_path, 'w') as f:\n                json.dump({\n                    'experiment_id': experiment_id,\n                    'timestamp': timestamp,\n                    'metrics': metrics_dict\n                }, f, indent=2)\n\n        msg = f\"✓ Saved {len(metrics_dict)} metrics for experiment #{experiment_id}\"\n        if file_path:\n            msg += f\" to {file_path}\"\n        print(msg)\n        return True, msg\n    except Exception as e:\n        msg = f\"✗ Failed to save metrics: {e}\"\n        print(msg)\n        db_connection.rollback()\n        return False, msg\n\n\ndef complete_experiment(db_connection, experiment_id: int,\n                       status: str = 'completed',\n                       notes: str = None) -> bool:\n    \"\"\"Mark an experiment as complete.\n    \n    Args:\n        db_connection: PostgreSQL connection\n        experiment_id: ID returned from start_experiment()\n        status: 'completed' or 'failed'\n        notes: Optional update to notes field\n        \n    Returns:\n        True if successful\n    \"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            if notes:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, notes = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, notes, experiment_id))\n            else:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, experiment_id))\n        db_connection.commit()\n        print(f\"✓ Experiment #{experiment_id} marked as {status}\")\n        return True\n    except Exception as e:\n        print(f\"✗ Failed to complete experiment: {e}\")\n        db_connection.rollback()\n        return False\n\n\n# ============================================================================\n# RUN EXPERIMENT TRACKING\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRACKING EXPERIMENT\")\nprint(\"=\" * 70)\n\nif evaluation_metrics and evaluation_metrics.get('total_queries_evaluated', 0) > 0:\n    \n    # Prepare configuration\n    config_dict = {\n        'embedding_model_alias': EMBEDDING_MODEL_ALIAS,\n        'top_k': TOP_K,\n        'confidence_threshold': CONFIDENCE_THRESHOLD,\n        'llm_model': 'llama3.2:1b',\n        'num_test_queries': evaluation_metrics['total_queries_evaluated'],\n    }\n\n    config_hash = compute_config_hash(config_dict)\n\n    print(f\"\\nExperiment Configuration:\")\n    print(f\"  Name: {EXPERIMENT_NAME}\")\n    print(f\"  Embedding Model: {EMBEDDING_MODEL_ALIAS}\")\n    print(f\"  Top K: {TOP_K}\")\n    print(f\"  Confidence Threshold: {CONFIDENCE_THRESHOLD}\")\n    print(f\"  Config Hash: {config_hash}\")\n    print(f\"  Test Queries: {evaluation_metrics['total_queries_evaluated']}\\n\")\n\n    # Start experiment tracking\n    experiment_id = start_experiment(\n        db_connection,\n        experiment_name=EXPERIMENT_NAME,\n        notebook_path='advanced-techniques/09-citation-tracking.ipynb',\n        embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n        config=config_dict,\n        techniques=TECHNIQUES_APPLIED,\n        notes=f'Citation tracking and source attribution on {evaluation_metrics[\"total_queries_evaluated\"]} queries'\n    )\n\n    # Prepare metrics for storage\n    metrics_to_store = {}\n\n    # Core citation metrics\n    metrics_to_store['avg_confidence'] = evaluation_metrics['avg_confidence']\n    metrics_to_store['avg_citation_accuracy'] = evaluation_metrics['avg_citation_accuracy']\n    metrics_to_store['avg_sources_used'] = evaluation_metrics['avg_sources_used']\n    metrics_to_store['avg_invalid_citations'] = evaluation_metrics['avg_invalid_citations']\n    \n    # Distribution metrics\n    metrics_to_store['min_confidence'] = evaluation_metrics['min_confidence']\n    metrics_to_store['max_confidence'] = evaluation_metrics['max_confidence']\n\n    # Configuration and metadata\n    metrics_to_store['num_queries_evaluated'] = evaluation_metrics['total_queries_evaluated']\n    metrics_to_store['top_k_used'] = TOP_K\n    metrics_to_store['config_hash'] = config_hash\n\n    # Save metrics\n    print(\"\\nSaving metrics to database...\\n\")\n    success, message = save_metrics(db_connection, experiment_id, metrics_to_store, export_to_file=True)\n\n    # Complete experiment\n    if success:\n        notes = f\"Successfully evaluated citation tracking on {evaluation_metrics['total_queries_evaluated']} queries. \"\n        notes += f\"Citation accuracy: {evaluation_metrics['avg_citation_accuracy']:.2%}, \"\n        notes += f\"Avg confidence: {evaluation_metrics['avg_confidence']:.4f}\"\n\n        complete_experiment(db_connection, experiment_id, status='completed', notes=notes)\n\n        # Display results summary\n        print(\"\\n\" + \"=\" * 70)\n        print(\"EXPERIMENT RESULTS SUMMARY\")\n        print(\"=\" * 70)\n\n        print(f\"\\nExperiment ID: {experiment_id}\")\n        print(f\"Experiment Name: {EXPERIMENT_NAME}\")\n        print(f\"Status: Completed\")\n        print(f\"Config Hash: {config_hash}\")\n\n        print(f\"\\nKey Metrics:\")\n        print(f\"  Avg Confidence:        {evaluation_metrics['avg_confidence']:.4f}\")\n        print(f\"  Citation Accuracy:     {evaluation_metrics['avg_citation_accuracy']:.2%}\")\n        print(f\"  Avg Sources Used:      {evaluation_metrics['avg_sources_used']:.2f} / {TOP_K}\")\n        print(f\"  Avg Invalid Citations: {evaluation_metrics['avg_invalid_citations']:.2f}\")\n\n        print(f\"\\nResults exported to:\")\n        print(f\"  Database: evaluation_results table (experiment_id={experiment_id})\")\n        print(f\"  JSON: data/experiment_results/experiment_{experiment_id}_*.json\")\n\n        print(\"\\n\" + \"=\" * 70)\n        print(\"NEXT STEPS\")\n        print(\"=\" * 70)\n        print(\"\\n1. Review citation accuracy metrics above\")\n        print(\"2. Compare results with other techniques:\")\n        print(\"   - evaluation-lab/03-compare-experiments.ipynb\")\n        print(\"   - evaluation-lab/04-plot-improvements.ipynb\")\n        print(\"3. Improve citation quality:\")\n        print(\"   - Combine with reranking (advanced-techniques/05)\")\n        print(\"   - Use query expansion (advanced-techniques/06)\")\n        print(\"   - Apply hybrid search (advanced-techniques/07)\")\n        print(\"4. Analyze per-query results for patterns:\")\n        print(\"   - Which questions have low citation accuracy?\")\n        print(\"   - Which retrieval confidence levels most reliable?\")\n        print(\"5. Implement citation filtering:\")\n        print(\"   - Only show answers with citation_accuracy > 0.8\")\n        print(\"   - Only show retrievals with confidence > CONFIDENCE_THRESHOLD\")\n\n    else:\n        print(\"\\n✗ Failed to track experiment\")\n        complete_experiment(db_connection, experiment_id, status='failed', notes='Failed to save metrics')\n\nelse:\n    print(\"⚠️  Cannot track experiment: evaluation results not available\")\n\n# Close database connection\nprint(\"\\n\\nClosing database connection...\")\nif embeddings_db:\n    embeddings_db.close()\ndb_connection.close()\nprint(\"✓ All connections closed\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}