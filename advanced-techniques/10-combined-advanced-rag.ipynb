{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e06a73",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. ✅ foundation/00-setup-postgres-schema.ipynb\n",
    "2. ✅ foundation/02-rag-postgresql-persistent.ipynb\n",
    "3. ✅ evaluation-lab/01-create-ground-truth-human-in-loop.ipynb\n",
    "4. ✅ Run individual technique notebooks 05-09 to understand each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997e56a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea55251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature flags - enable/disable each technique\n",
    "ENABLE_SEMANTIC_CHUNKING = True\n",
    "ENABLE_QUERY_EXPANSION = True\n",
    "ENABLE_HYBRID_SEARCH = False  # Can be expensive\n",
    "ENABLE_RERANKING = True\n",
    "ENABLE_CITATION_TRACKING = True\n",
    "\n",
    "EMBEDDING_MODEL_ALIAS = \"all-minilm-l6-v2\"\n",
    "TOP_K = 5\n",
    "\n",
    "# Name this experiment based on enabled techniques\n",
    "ENABLED_TECHNIQUES = []\n",
    "if ENABLE_SEMANTIC_CHUNKING: ENABLED_TECHNIQUES.append(\"semantic_chunking\")\n",
    "if ENABLE_QUERY_EXPANSION: ENABLED_TECHNIQUES.append(\"query_expansion\")\n",
    "if ENABLE_HYBRID_SEARCH: ENABLED_TECHNIQUES.append(\"hybrid_search\")\n",
    "if ENABLE_RERANKING: ENABLED_TECHNIQUES.append(\"reranking\")\n",
    "if ENABLE_CITATION_TRACKING: ENABLED_TECHNIQUES.append(\"citation_tracking\")\n",
    "\n",
    "EXPERIMENT_NAME = f\"combined-advanced-rag-{'-'.join(ENABLED_TECHNIQUES)}\"\n",
    "TECHNIQUES_APPLIED = ENABLED_TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99baab00",
   "metadata": {},
   "source": [
    "## Load Embeddings from Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c7964",
   "metadata": {},
   "outputs": [],
   "source": "import psycopg2\nimport psycopg2.extras\nimport ollama\nimport json\nimport pandas as pd\nimport numpy as np\nimport hashlib\nimport math\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Optional\nimport os\n\n# PostgreSQL connection\nPOSTGRES_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'postgres',\n    'password': 'postgres',\n}\n\n# Create database connection\ntry:\n    db_connection = psycopg2.connect(\n        host=POSTGRES_CONFIG['host'],\n        port=POSTGRES_CONFIG['port'],\n        database=POSTGRES_CONFIG['database'],\n        user=POSTGRES_CONFIG['user'],\n        password=POSTGRES_CONFIG['password']\n    )\n    print(\"✓ Connected to PostgreSQL\")\nexcept psycopg2.OperationalError as e:\n    print(f\"✗ Failed to connect to PostgreSQL: {e}\")\n    raise\n\n# ============================================================================\n# REGISTRY DISCOVERY & LOAD-OR-GENERATE PATTERN\n# ============================================================================\n\ndef list_available_embeddings(db_connection) -> pd.DataFrame:\n    \"\"\"Query embedding_registry to show available models with metadata.\"\"\"\n    query = '''\n        SELECT\n            model_alias,\n            model_name,\n            dimension,\n            embedding_count,\n            chunk_source_dataset,\n            chunk_size_config,\n            created_at,\n            last_accessed\n        FROM embedding_registry\n        ORDER BY created_at DESC\n    '''\n    return pd.read_sql(query, db_connection)\n\n\ndef get_embedding_metadata(db_connection, model_alias: str) -> Optional[Dict]:\n    \"\"\"Fetch metadata for a specific embedding model.\"\"\"\n    with db_connection.cursor() as cur:\n        cur.execute('''\n            SELECT\n                dimension,\n                embedding_count,\n                chunk_source_dataset,\n                chunk_size_config,\n                created_at,\n                metadata_json\n            FROM embedding_registry\n            WHERE model_alias = %s\n        ''', (model_alias,))\n        result = cur.fetchone()\n\n        if not result:\n            return None\n\n        return {\n            'dimension': result[0],\n            'embedding_count': result[1],\n            'chunk_source_dataset': result[2],\n            'chunk_size_config': result[3],\n            'created_at': result[4],\n            'metadata_json': result[5] or {}\n        }\n\n\nclass PostgreSQLVectorDB:\n    \"\"\"Helper to load embeddings from PostgreSQL without regeneration.\"\"\"\n\n    def __init__(self, config, table_name, preserve_existing=True):\n        self.config = config\n        self.table_name = table_name\n        self.conn = psycopg2.connect(\n            host=config['host'],\n            port=config['port'],\n            database=config['database'],\n            user=config['user'],\n            password=config['password']\n        )\n        print(f'✓ Connected to table: {table_name}')\n\n    def get_chunk_count(self):\n        \"\"\"How many embeddings are stored?\"\"\"\n        with self.conn.cursor() as cur:\n            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n            return cur.fetchone()[0]\n\n    def similarity_search(self, query_embedding, top_n=5):\n        \"\"\"Retrieve most similar chunks using pgvector.\"\"\"\n        with self.conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n            cur.execute(f'''\n                SELECT id,\n                       content as chunk_text,\n                       1 - (embedding <=> %s::vector) as similarity\n                FROM {self.table_name}\n                ORDER BY embedding <=> %s::vector\n                LIMIT %s\n            ''', (query_embedding, query_embedding, top_n))\n\n            results = cur.fetchall()\n            return [(row['chunk_text'], row['similarity'], row['id']) for row in results]\n\n    def close(self):\n        if self.conn:\n            self.conn.close()\n\n\ndef load_or_generate(db_connection, embedding_model_alias, preserve_existing=True):\n    \"\"\"Load embeddings from registry OR show instructions if not available.\"\"\"\n\n    print(f\"\\n{'='*70}\")\n    print(f\"Checking for embeddings: '{embedding_model_alias}'...\")\n    print(f\"{'='*70}\\n\")\n\n    try:\n        with db_connection.cursor() as cur:\n            cur.execute('''\n                SELECT id, dimension, embedding_count, created_at, metadata_json\n                FROM embedding_registry\n                WHERE model_alias = %s\n            ''', (embedding_model_alias,))\n            registry_entry = cur.fetchone()\n    except Exception as e:\n        print(f\"Could not query registry: {e}\")\n        print(\"Make sure foundation/00-setup-postgres-schema.ipynb has been run.\")\n        return None\n\n    if registry_entry:\n        reg_id, dimension, embedding_count, created_at, metadata_json = registry_entry\n\n        print(f\"✓ FOUND EXISTING EMBEDDINGS\")\n        print(f\"  Model:      {embedding_model_alias}\")\n        print(f\"  Count:      {embedding_count:,} embeddings\")\n        print(f\"  Dimension:  {dimension}\")\n        print(f\"  Created:    {created_at}\\n\")\n\n        if preserve_existing:\n            print(\"Loading existing embeddings...\\n\")\n\n            try:\n                table_name = f'embeddings_{embedding_model_alias.replace(\".\", \"_\")}'\n\n                db_instance = PostgreSQLVectorDB(\n                    config=POSTGRES_CONFIG,\n                    table_name=table_name,\n                    preserve_existing=True\n                )\n\n                count = db_instance.get_chunk_count()\n                print(f\"✓ LOADED SUCCESSFULLY\")\n                print(f\"  Embeddings: {count:,}\")\n                print(f\"  Table: {table_name}\")\n                print(f\"  Status: Ready for retrieval\\n\")\n\n                return db_instance\n\n            except Exception as e:\n                print(f\"\\n✗ Error loading embeddings: {e}\")\n                return None\n\n    else:\n        print(f\"✗ NO EMBEDDINGS FOUND\")\n        print(f\"  Model: {embedding_model_alias}\")\n        print(f\"\\nTo create embeddings, run:\")\n        print(f\"  foundation/02-rag-postgresql-persistent.ipynb\\n\")\n        return None\n\n\n# Discover and load embeddings\nprint(\"Step 1: Discovering available embeddings...\\n\")\navailable = list_available_embeddings(db_connection)\n\nif available.empty:\n    print(\"⚠️  No embeddings found in registry yet.\")\n    print(\"Run foundation/02-rag-postgresql-persistent.ipynb first.\\n\")\nelse:\n    print(\"Available embeddings:\")\n    print(available.to_string(index=False))\n    print()\n\n# Load embeddings using the pattern\nprint(\"\\nStep 2: Loading embeddings using load-or-generate pattern...\\n\")\nembeddings_db = load_or_generate(\n    db_connection=db_connection,\n    embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n    preserve_existing=True\n)\n\nif embeddings_db:\n    print(\"✓ Success! Embeddings loaded and ready for retrieval.\")\nelse:\n    print(\"⚠️  Could not load embeddings. See instructions above.\")\n    embeddings_db = None\n\n# Load ground truth test questions\nprint(\"\\nLoading ground truth test questions...\")\nground_truth_questions = []\n\nwith db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n    cur.execute('''\n        SELECT\n            id,\n            question,\n            relevant_chunk_ids,\n            quality_rating,\n            source_type\n        FROM evaluation_groundtruth\n        WHERE quality_rating = 'good'\n        ORDER BY id\n    ''')\n\n    for row in cur.fetchall():\n        ground_truth_questions.append({\n            'id': row['id'],\n            'question': row['question'],\n            'relevant_chunk_ids': row['relevant_chunk_ids'] or [],\n            'quality_rating': row['quality_rating'],\n            'source_type': row['source_type']\n        })\n\nprint(f\"✓ Loaded {len(ground_truth_questions)} ground truth questions\\n\")\n\nif ground_truth_questions:\n    print(f\"Sample question: {ground_truth_questions[0]['question'][:80]}...\")"
  },
  {
   "cell_type": "markdown",
   "id": "b29ebc5a",
   "metadata": {},
   "source": [
    "## Unified RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714fecf",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 2: COMBINED PIPELINE WITH FEATURE FLAGS\n# ============================================================================\n\ndef expand_query_with_llm(query: str, num_expansions: int = 4, llm_model: str = 'llama3.2:1b') -> List[str]:\n    \"\"\"Generate semantically similar query reformulations using an LLM.\"\"\"\n    prompt = f\"\"\"Generate {num_expansions} different ways to ask this question. Each variant should:\n- Mean the same thing as the original\n- Use different wording and phrasing\n- Be a complete question that can stand alone\n\nOriginal question: {query}\n\nGenerate exactly {num_expansions} variants. Format each on a new line starting with \"Q:\".\nExample format:\nQ: How is something done?\nQ: What is the process of something?\n\nNow generate {num_expansions} variants for the original question:\"\"\"\n    \n    try:\n        response = ollama.chat(\n            model=llm_model,\n            messages=[{'role': 'user', 'content': prompt}]\n        )\n        \n        content = response['message']['content']\n        variants = [query]  # Start with original\n        \n        for line in content.split('\\n'):\n            line = line.strip()\n            if line.startswith('Q:'):\n                variant = line[2:].strip()\n                if variant and len(variant) > 10 and variant not in variants:\n                    variants.append(variant)\n            elif line and '?' in line and len(line) > 10:\n                if not any(x in line[:5] for x in ['1.', '2.', '3.', '-']):\n                    if line not in variants:\n                        variants.append(line)\n        \n        return variants[:num_expansions + 1]\n    \n    except Exception as e:\n        print(f\"  LLM expansion failed: {e}\")\n        return [query]\n\n\ndef bm25_search_postgresql(query: str, db_connection, table_name: str, top_k: int = 10):\n    \"\"\"Keyword-based retrieval using PostgreSQL full-text search.\"\"\"\n    with db_connection.cursor() as cur:\n        try:\n            cur.execute(f'''\n                SELECT content,\n                       ts_rank(to_tsvector('english', content),\n                              plainto_tsquery('english', %s)) as relevance,\n                       id\n                FROM {table_name}\n                WHERE to_tsvector('english', content) @@ plainto_tsquery('english', %s)\n                ORDER BY relevance DESC\n                LIMIT %s\n            ''', (query, query, top_k))\n            \n            results = cur.fetchall()\n            return [(chunk, float(score), chunk_id) for chunk, score, chunk_id in results]\n        except Exception as e:\n            # Full-text search may not be configured\n            return []\n\n\ndef reciprocal_rank_fusion(dense_results: List[Tuple],\n                          sparse_results: List[Tuple],\n                          rrf_k: int = 60,\n                          top_k: int = 5) -> List[Tuple]:\n    \"\"\"Fuse rankings from multiple sources using Reciprocal Rank Fusion.\"\"\"\n    dense_ranks = {chunk_id: rank + 1 for rank, (_, _, chunk_id) in enumerate(dense_results)}\n    sparse_ranks = {chunk_id: rank + 1 for rank, (_, _, chunk_id) in enumerate(sparse_results)}\n    \n    all_chunk_ids = set(dense_ranks.keys()) | set(sparse_ranks.keys())\n    \n    chunk_texts = {}\n    for chunk_text, _, chunk_id in dense_results + sparse_results:\n        if chunk_id not in chunk_texts:\n            chunk_texts[chunk_id] = chunk_text\n    \n    fused_scores = {}\n    for chunk_id in all_chunk_ids:\n        rrf_score = 0.0\n        \n        if chunk_id in dense_ranks:\n            rrf_score += 1.0 / (rrf_k + dense_ranks[chunk_id])\n        \n        if chunk_id in sparse_ranks:\n            rrf_score += 1.0 / (rrf_k + sparse_ranks[chunk_id])\n        \n        fused_scores[chunk_id] = rrf_score\n    \n    fused = [\n        (chunk_texts[chunk_id], score, chunk_id)\n        for chunk_id, score in fused_scores.items()\n    ]\n    fused.sort(key=lambda x: x[1], reverse=True)\n    \n    return fused[:top_k]\n\n\ndef rerank_with_crossencoder(query: str,\n                             candidates: List[Tuple],\n                             reranker_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n                             top_k: int = 5) -> List[Tuple]:\n    \"\"\"Two-stage retrieval: dense retrieval -> cross-encoder reranking.\"\"\"\n    if not candidates:\n        return []\n    \n    from sentence_transformers import CrossEncoder\n    \n    print(f\"Loading cross-encoder model: {reranker_model}\")\n    model = CrossEncoder(reranker_model)\n    \n    chunk_texts = [chunk_text for chunk_text, _, _ in candidates]\n    pairs = [[query, chunk_text] for chunk_text in chunk_texts]\n    \n    print(f\"Scoring {len(pairs)} candidates with cross-encoder...\")\n    scores = model.predict(pairs)\n    \n    reranked = [\n        (chunk_text, float(score), chunk_id)\n        for (chunk_text, _, chunk_id), score in zip(candidates, scores)\n    ]\n    \n    reranked.sort(key=lambda x: x[1], reverse=True)\n    \n    return reranked[:top_k]\n\n\ndef combined_retrieval_pipeline(query: str,\n                               embeddings_db: PostgreSQLVectorDB,\n                               embedding_model: str,\n                               db_connection,\n                               config: Dict) -> Dict:\n    \"\"\"\n    Apply all enabled techniques in optimal order.\n    \n    Optimal ordering (from experiments):\n    1. Query Expansion (improves recall)\n    2. Hybrid Search (combines dense + sparse)\n    3. Reranking (improves precision)\n    4. Citation Tracking (adds transparency)\n    \n    Returns:\n        dict with: results, citations, query_variants, techniques_applied\n    \"\"\"\n    techniques_applied = []\n    query_variants = [query]\n    \n    # Step 1: Query Expansion (optional)\n    if config.get('enable_query_expansion', False):\n        query_variants = expand_query_with_llm(query, num_expansions=config.get('num_expansions', 4))\n        techniques_applied.append('query_expansion')\n        print(f\"  Generated {len(query_variants)} query variants\")\n    \n    # Step 2: Hybrid Search or Vector-only\n    table_name = f'embeddings_{embedding_model.replace(\".\", \"_\")}'\n    results = []\n    \n    if config.get('enable_hybrid_search', False):\n        # Hybrid: combine dense + sparse\n        for q_variant in query_variants:\n            # Dense retrieval\n            q_emb = ollama.embed(model=embedding_model, input=q_variant)['embeddings'][0]\n            with embeddings_db.conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n                cur.execute(f'''\n                    SELECT id, content as chunk_text,\n                           1 - (embedding <=> %s::vector) as similarity\n                    FROM {table_name}\n                    ORDER BY embedding <=> %s::vector\n                    LIMIT %s\n                ''', (q_emb, q_emb, config.get('top_k_initial', 20)))\n                dense_results_raw = cur.fetchall()\n                dense_results = [(row['chunk_text'], float(row['similarity']), row['id'])\n                                for row in dense_results_raw]\n            \n            # Sparse retrieval\n            sparse_results = bm25_search_postgresql(q_variant, db_connection, table_name,\n                                                   top_k=config.get('top_k_initial', 20))\n            \n            # Fuse with RRF\n            fused = reciprocal_rank_fusion(dense_results, sparse_results,\n                                         rrf_k=config.get('rrf_k', 60),\n                                         top_k=config.get('top_k_initial', 20))\n            results.extend(fused)\n        \n        techniques_applied.append('hybrid_search')\n        print(f\"  Retrieved with hybrid search (vector + BM25 + RRF)\")\n    \n    else:\n        # Vector-only retrieval\n        for q_variant in query_variants:\n            q_emb = ollama.embed(model=embedding_model, input=q_variant)['embeddings'][0]\n            vector_results = embeddings_db.similarity_search(q_emb, top_n=config.get('top_k_initial', 20))\n            results.extend(vector_results)\n        \n        techniques_applied.append('vector_search')\n        print(f\"  Retrieved with vector-only search\")\n    \n    # Deduplicate merged results\n    unique_results = {}\n    for chunk, score, chunk_id in results:\n        if chunk_id not in unique_results or score > unique_results[chunk_id][1]:\n            unique_results[chunk_id] = (chunk, score)\n    \n    results = [(chunk, score, chunk_id) for chunk_id, (chunk, score) in unique_results.items()]\n    results.sort(key=lambda x: x[1], reverse=True)\n    \n    # Step 3: Reranking (optional)\n    if config.get('enable_reranking', False):\n        results = rerank_with_crossencoder(query, results[:config.get('top_k_initial', 20)],\n                                          top_k=config.get('top_k_final', 5))\n        techniques_applied.append('reranking')\n        print(f\"  Applied cross-encoder reranking\")\n    else:\n        results = results[:config.get('top_k_final', 5)]\n    \n    # Step 4: Citation Tracking (optional)\n    citations = None\n    if config.get('enable_citation_tracking', False):\n        citations = [\n            {\n                'citation_id': f\"[{i+1}]\",\n                'chunk_text': chunk,\n                'score': score,\n                'chunk_id': chunk_id\n            }\n            for i, (chunk, score, chunk_id) in enumerate(results)\n        ]\n        techniques_applied.append('citation_tracking')\n        print(f\"  Prepared citation tracking\")\n    \n    return {\n        'results': results,\n        'citations': citations,\n        'query_variants': query_variants if len(query_variants) > 1 else None,\n        'techniques_applied': techniques_applied\n    }\n\n\nprint(\"\\nTesting combined pipeline with current configuration...\")\nif embeddings_db and ground_truth_questions:\n    test_query = ground_truth_questions[0]['question']\n    print(f\"\\nQuery: {test_query}\\n\")\n    \n    pipeline_result = combined_retrieval_pipeline(\n        test_query,\n        embeddings_db,\n        EMBEDDING_MODEL_ALIAS,\n        db_connection,\n        {\n            'enable_query_expansion': ENABLE_QUERY_EXPANSION,\n            'enable_hybrid_search': ENABLE_HYBRID_SEARCH,\n            'enable_reranking': ENABLE_RERANKING,\n            'enable_citation_tracking': ENABLE_CITATION_TRACKING,\n            'num_expansions': 4,\n            'top_k_initial': 20,\n            'top_k_final': 5,\n            'rrf_k': 60\n        }\n    )\n    \n    print(f\"\\nTechniques applied: {', '.join(pipeline_result['techniques_applied'])}\")\n    print(f\"\\nTop {len(pipeline_result['results'])} results:\")\n    for i, (chunk, score, chunk_id) in enumerate(pipeline_result['results'], 1):\n        preview = chunk[:100].replace('\\n', ' ') + '...'\n        print(f\"  [{i}] (score: {score:.4f}) {preview}\")"
  },
  {
   "cell_type": "markdown",
   "id": "1ca26469",
   "metadata": {},
   "source": [
    "## Evaluate System Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for combined advanced RAG\n",
    "NUM_EXPANSIONS = 3  # Number of query variations to generate\n",
    "TOP_K_INITIAL = 10  # Initial retrieval count before reranking\n",
    "TOP_K_FINAL = 5  # Final results after reranking\n",
    "ENABLE_RERANKING = True\n",
    "ENABLE_CITATION_TRACKING = True\n",
    "\n",
    "\n",
    "# ============================================================================",
    "# PART 3: EVALUATE SYSTEM PERFORMANCE",
    "# ============================================================================",
    "",
    "# Metric computation functions",
    "def precision_at_k(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int], k: int = 5) -> float:",
    "    \"\"\"Precision@K: What % of top-K results are relevant?\"\"\"",
    "    if k == 0:",
    "        return 0.0",
    "    ",
    "    retrieved_k = retrieved_chunk_ids[:k]",
    "    relevant_set = set(relevant_chunk_ids)",
    "    ",
    "    num_relevant_in_k = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)",
    "    return num_relevant_in_k / k",
    "",
    "",
    "def recall_at_k(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int], k: int = 5) -> float:",
    "    \"\"\"Recall@K: What % of all relevant chunks were found in top-K?\"\"\"",
    "    if len(relevant_chunk_ids) == 0:",
    "        return 0.0",
    "    ",
    "    retrieved_k = retrieved_chunk_ids[:k]",
    "    relevant_set = set(relevant_chunk_ids)",
    "    ",
    "    num_relevant_found = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)",
    "    return num_relevant_found / len(relevant_set)",
    "",
    "",
    "def mean_reciprocal_rank(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int]) -> float:",
    "    \"\"\"MRR: How quickly do we find the first relevant result?\"\"\"",
    "    relevant_set = set(relevant_chunk_ids)",
    "    ",
    "    for rank, chunk_id in enumerate(retrieved_chunk_ids, start=1):",
    "        if chunk_id in relevant_set:",
    "            return 1.0 / rank",
    "    ",
    "    return 0.0",
    "",
    "",
    "def ndcg_at_k(retrieved_chunk_ids: List[int], relevant_chunk_ids: List[int], k: int = 5) -> float:",
    "    \"\"\"NDCG@K: Normalized Discounted Cumulative Gain (ranking quality)\"\"\"",
    "    ",
    "    def dcg_score(relevance_scores: List[float]) -> float:",
    "        return sum(",
    "            (2**rel - 1) / math.log2(rank + 2)",
    "            for rank, rel in enumerate(relevance_scores)",
    "        )",
    "    ",
    "    if k == 0 or len(relevant_chunk_ids) == 0:",
    "        return 0.0",
    "    ",
    "    retrieved_k = retrieved_chunk_ids[:k]",
    "    relevant_set = set(relevant_chunk_ids)",
    "    ",
    "    relevance = [1 if chunk_id in relevant_set else 0 for chunk_id in retrieved_k]",
    "    ",
    "    dcg = dcg_score(relevance)",
    "    ideal_relevance = sorted(relevance, reverse=True)",
    "    idcg = dcg_score(ideal_relevance)",
    "    ",
    "    if idcg == 0:",
    "        return 0.0",
    "    ",
    "    return dcg / idcg",
    "",
    "",
    "def evaluate_with_config(test_questions: List[Dict],",
    "                        embeddings_db: PostgreSQLVectorDB,",
    "                        embedding_model: str,",
    "                        db_connection,",
    "                        config: Dict) -> Dict:",
    "    \"\"\"Evaluate pipeline with given configuration.\"\"\"",
    "    ",
    "    results_list = []",
    "    ",
    "    print(f\"\\nEvaluating configuration on {len(test_questions)} test questions...\")",
    "    print(\"-\" * 70)",
    "    ",
    "    for i, q in enumerate(test_questions, 1):",
    "        query = q['question']",
    "        relevant_ids = q['relevant_chunk_ids']",
    "        ",
    "        if not relevant_ids:",
    "            continue",
    "        ",
    "        # Run pipeline",
    "        pipeline_result = combined_retrieval_pipeline(query, embeddings_db, embedding_model,",
    "                                                     db_connection, config)",
    "        retrieved_ids = [chunk_id for _, _, chunk_id in pipeline_result['results']]",
    "        ",
    "        # Compute metrics",
    "        metrics = {",
    "            'precision@5': precision_at_k(retrieved_ids, relevant_ids, k=5),",
    "            'recall@5': recall_at_k(retrieved_ids, relevant_ids, k=5),",
    "            'mrr': mean_reciprocal_rank(retrieved_ids, relevant_ids),",
    "            'ndcg@5': ndcg_at_k(retrieved_ids, relevant_ids, k=5)",
    "        }",
    "        ",
    "        results_list.append(metrics)",
    "        ",
    "        if i % max(1, len(test_questions) // 10) == 0:",
    "            print(f\"  Progress: {i}/{len(test_questions)} queries evaluated\")",
    "    ",
    "    print(\"-\" * 70)",
    "    ",
    "    # Aggregate metrics",
    "    if not results_list:",
    "        return {'precision@5': 0, 'recall@5': 0, 'mrr': 0, 'ndcg@5': 0}",
    "    ",
    "    return {",
    "        'precision@5': np.mean([r['precision@5'] for r in results_list]),",
    "        'recall@5': np.mean([r['recall@5'] for r in results_list]),",
    "        'mrr': np.mean([r['mrr'] for r in results_list]),",
    "        'ndcg@5': np.mean([r['ndcg@5'] for r in results_list]),",
    "        'num_queries': len(results_list)",
    "    }",
    "",
    "",
    "print(\"\\n\" + \"=\" * 70)",
    "print(\"EVALUATING CURRENT CONFIGURATION\")",
    "print(\"=\" * 70)",
    "",
    "config = {",
    "    'enable_query_expansion': ENABLE_QUERY_EXPANSION,",
    "    'enable_hybrid_search': ENABLE_HYBRID_SEARCH,",
    "    'enable_reranking': ENABLE_RERANKING,",
    "    'enable_citation_tracking': ENABLE_CITATION_TRACKING,",
    "    'num_expansions': NUM_EXPANSIONS,",
    "    'top_k_initial': TOP_K_INITIAL,",
    "    'top_k_final': TOP_K_FINAL,",
    "    'rrf_k': RRF_K",
    "}",
    "",
    "if embeddings_db and ground_truth_questions:",
    "    eval_metrics = evaluate_with_config(",
    "        ground_truth_questions,",
    "        embeddings_db,",
    "        EMBEDDING_MODEL_ALIAS,",
    "        db_connection,",
    "        config",
    "    )",
    "    ",
    "    print(\"\\n\" + \"=\" * 70)",
    "    print(\"CURRENT CONFIGURATION RESULTS\")",
    "    print(\"=\" * 70)",
    "    ",
    "    print(f\"\\nEnabled Techniques: {', '.join(ENABLED_TECHNIQUES)}\")",
    "    print(f\"Queries Evaluated: {eval_metrics.get('num_queries', 0)}\\n\")",
    "    ",
    "    print(f\"{'Metric':<20} {'Score':<15}\")",
    "    print(\"-\" * 35)",
    "    ",
    "    for metric in ['precision@5', 'recall@5', 'mrr', 'ndcg@5']:",
    "        value = eval_metrics.get(metric, 0)",
    "        print(f\"{metric:<20} {value:<15.4f}\")",
    "    ",
    "else:",
    "    print(\"⚠️  Cannot evaluate: embeddings or test questions not available\")",
    "    eval_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74369a",
   "metadata": {},
   "source": [
    "## Compare All Technique Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb304006",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 4: COMPARE ALL TECHNIQUE COMBINATIONS\n# ============================================================================\n\ndef evaluate_all_combinations(test_questions: List[Dict],\n                             embeddings_db: PostgreSQLVectorDB,\n                             embedding_model: str,\n                             db_connection) -> Dict:\n    \"\"\"\n    Test all technique combinations to find optimal configuration.\n    \n    Combinations to test:\n    1. Baseline (no techniques)\n    2. Query Expansion only\n    3. Hybrid Search only\n    4. Reranking only\n    5. Query Expansion + Reranking\n    6. Hybrid + Reranking\n    7. All techniques combined\n    \n    Returns:\n        DataFrame with results for all combinations\n    \"\"\"\n    \n    combinations = [\n        {\n            'name': 'baseline',\n            'enable_query_expansion': False,\n            'enable_hybrid_search': False,\n            'enable_reranking': False,\n            'enable_citation_tracking': False\n        },\n        {\n            'name': 'expansion_only',\n            'enable_query_expansion': True,\n            'enable_hybrid_search': False,\n            'enable_reranking': False,\n            'enable_citation_tracking': False\n        },\n        {\n            'name': 'hybrid_only',\n            'enable_query_expansion': False,\n            'enable_hybrid_search': True,\n            'enable_reranking': False,\n            'enable_citation_tracking': False\n        },\n        {\n            'name': 'reranking_only',\n            'enable_query_expansion': False,\n            'enable_hybrid_search': False,\n            'enable_reranking': True,\n            'enable_citation_tracking': False\n        },\n        {\n            'name': 'expansion+reranking',\n            'enable_query_expansion': True,\n            'enable_hybrid_search': False,\n            'enable_reranking': True,\n            'enable_citation_tracking': False\n        },\n        {\n            'name': 'hybrid+reranking',\n            'enable_query_expansion': False,\n            'enable_hybrid_search': True,\n            'enable_reranking': True,\n            'enable_citation_tracking': False\n        },\n        {\n            'name': 'all_combined',\n            'enable_query_expansion': True,\n            'enable_hybrid_search': True,\n            'enable_reranking': True,\n            'enable_citation_tracking': True\n        }\n    ]\n    \n    results = {}\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"EVALUATING ALL TECHNIQUE COMBINATIONS\")\n    print(\"=\" * 70)\n    \n    for combo in combinations:\n        combo_name = combo['name']\n        print(f\"\\nTesting: {combo_name}\")\n        \n        # Create config from combination\n        config = {\n            'enable_query_expansion': combo['enable_query_expansion'],\n            'enable_hybrid_search': combo['enable_hybrid_search'],\n            'enable_reranking': combo['enable_reranking'],\n            'enable_citation_tracking': combo['enable_citation_tracking'],\n            'num_expansions': NUM_EXPANSIONS,\n            'top_k_initial': TOP_K_INITIAL,\n            'top_k_final': TOP_K_FINAL,\n            'rrf_k': RRF_K\n        }\n        \n        metrics = evaluate_with_config(test_questions, embeddings_db, embedding_model,\n                                      db_connection, config)\n        results[combo_name] = metrics\n    \n    return results\n\n\n# Run all combinations\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SYSTEMATIC COMBINATION EVALUATION\")\nprint(\"=\" * 70)\n\nif embeddings_db and ground_truth_questions:\n    all_combinations_results = evaluate_all_combinations(\n        ground_truth_questions,\n        embeddings_db,\n        EMBEDDING_MODEL_ALIAS,\n        db_connection\n    )\n    \n    # Create comparison DataFrame\n    print(\"\\n\" + \"=\" * 70)\n    print(\"COMBINATION COMPARISON RESULTS\")\n    print(\"=\" * 70)\n    \n    comparison_data = []\n    for combo_name, metrics in all_combinations_results.items():\n        comparison_data.append({\n            'combination': combo_name,\n            'precision@5': metrics.get('precision@5', 0),\n            'recall@5': metrics.get('recall@5', 0),\n            'mrr': metrics.get('mrr', 0),\n            'ndcg@5': metrics.get('ndcg@5', 0)\n        })\n    \n    comparison_df = pd.DataFrame(comparison_data)\n    \n    # Display results\n    print(\"\\n\" + comparison_df.to_string(index=False))\n    \n    # Identify best combination\n    best_ndcg_idx = comparison_df['ndcg@5'].idxmax()\n    best_combo = comparison_df.iloc[best_ndcg_idx]\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"BEST CONFIGURATION IDENTIFIED\")\n    print(\"=\" * 70)\n    \n    print(f\"\\nBest Combination: {best_combo['combination']}\")\n    print(f\"  Precision@5: {best_combo['precision@5']:.4f}\")\n    print(f\"  Recall@5:    {best_combo['recall@5']:.4f}\")\n    print(f\"  MRR:         {best_combo['mrr']:.4f}\")\n    print(f\"  NDCG@5:      {best_combo['ndcg@5']:.4f}\")\n    \n    # Show top 3 combinations\n    print(f\"\\nTop 3 Best Combinations (by NDCG@5):\")\n    top3 = comparison_df.nlargest(3, 'ndcg@5')\n    \n    for i, row in top3.iterrows():\n        print(f\"\\n  {i+1}. {row['combination']}\")\n        print(f\"     P@5: {row['precision@5']:.4f}, R@5: {row['recall@5']:.4f}, NDCG@5: {row['ndcg@5']:.4f}\")\n    \n    # Measure cumulative improvements\n    print(\"\\n\" + \"=\" * 70)\n    print(\"CUMULATIVE IMPROVEMENTS\")\n    print(\"=\" * 70)\n    \n    baseline_metrics = all_combinations_results['baseline']\n    \n    improvements = []\n    for combo_name in ['expansion_only', 'hybrid_only', 'reranking_only', 'all_combined']:\n        if combo_name in all_combinations_results:\n            variant = all_combinations_results[combo_name]\n            \n            improvements.append({\n                'technique': combo_name,\n                'precision_improvement_%': ((variant.get('precision@5', 0) - baseline_metrics.get('precision@5', 1)) / max(baseline_metrics.get('precision@5', 1), 0.001) * 100),\n                'recall_improvement_%': ((variant.get('recall@5', 0) - baseline_metrics.get('recall@5', 1)) / max(baseline_metrics.get('recall@5', 1), 0.001) * 100),\n                'ndcg_improvement_%': ((variant.get('ndcg@5', 0) - baseline_metrics.get('ndcg@5', 1)) / max(baseline_metrics.get('ndcg@5', 1), 0.001) * 100)\n            })\n    \n    improvements_df = pd.DataFrame(improvements)\n    \n    print(\"\\n\" + improvements_df.to_string(index=False))\n    \n    # Visualization\n    print(\"\\n\" + \"=\" * 70)\n    print(\"CREATING COMPARISON VISUALIZATIONS\")\n    print(\"=\" * 70)\n    \n    import matplotlib.pyplot as plt\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    fig.suptitle('Technique Combination Comparison', fontsize=14, fontweight='bold')\n    \n    metrics_to_plot = ['precision@5', 'recall@5', 'mrr', 'ndcg@5']\n    \n    for idx, metric in enumerate(metrics_to_plot):\n        ax = axes[idx // 2, idx % 2]\n        \n        colors = ['#FF6B6B' if 'baseline' in x else '#4ECDC4' if 'all_combined' in x else '#95E1D3'\n                 for x in comparison_df['combination']]\n        \n        ax.barh(comparison_df['combination'], comparison_df[metric], color=colors, alpha=0.7)\n        ax.set_xlabel(metric, fontweight='bold')\n        ax.set_title(f'{metric} by Combination')\n        ax.grid(axis='x', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nVisualization displayed above\")\n    \nelse:\n    print(\"⚠️  Cannot evaluate combinations: embeddings or test questions not available\")\n    all_combinations_results = {}"
  },
  {
   "cell_type": "markdown",
   "id": "deb22792",
   "metadata": {},
   "source": [
    "## Track Final Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b2965",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 5: EXPERIMENT TRACKING\n# ============================================================================\n\ndef compute_config_hash(config_dict: Dict) -> str:\n    \"\"\"Create deterministic SHA256 hash of configuration.\"\"\"\n    config_str = json.dumps(config_dict, sort_keys=True)\n    hash_obj = hashlib.sha256(config_str.encode())\n    return hash_obj.hexdigest()[:12]\n\n\ndef start_experiment(db_connection, experiment_name: str,\n                     notebook_path: str = None,\n                     embedding_model_alias: str = None,\n                     config: Dict = None,\n                     techniques: List[str] = None,\n                     notes: str = None) -> int:\n    \"\"\"Start a new experiment and return its ID for tracking.\"\"\"\n    if config is None:\n        config = {}\n    if techniques is None:\n        techniques = []\n\n    config_hash = compute_config_hash(config)\n\n    with db_connection.cursor() as cur:\n        cur.execute('''\n            INSERT INTO experiments (\n                experiment_name, notebook_path, embedding_model_alias,\n                config_hash, config_json, techniques_applied, notes, status\n            )\n            VALUES (%s, %s, %s, %s, %s, %s, %s, 'running')\n            RETURNING id\n        ''', (\n            experiment_name,\n            notebook_path,\n            embedding_model_alias,\n            config_hash,\n            json.dumps(config),\n            techniques,\n            notes\n        ))\n        exp_id = cur.fetchone()[0]\n    db_connection.commit()\n    print(f\"✓ Started experiment #{exp_id}: {experiment_name}\")\n    return exp_id\n\n\ndef save_metrics(db_connection, experiment_id: int, metrics_dict: Dict,\n                 export_to_file: bool = True,\n                 export_dir: str = 'data/experiment_results') -> Tuple[bool, str]:\n    \"\"\"Save experiment metrics to database and optionally to JSON file.\"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            for metric_name, metric_data in metrics_dict.items():\n                if isinstance(metric_data, dict):\n                    metric_value = metric_data.get('value', 0.0)\n                    metric_details = metric_data.get('details', {})\n                else:\n                    metric_value = metric_data\n                    metric_details = {}\n\n                cur.execute('''\n                    INSERT INTO evaluation_results (\n                        experiment_id, metric_name, metric_value, metric_details_json\n                    )\n                    VALUES (%s, %s, %s, %s)\n                ''', (\n                    experiment_id,\n                    metric_name,\n                    float(metric_value),\n                    json.dumps(metric_details) if metric_details else '{}'\n                ))\n        db_connection.commit()\n\n        file_path = None\n        if export_to_file:\n            os.makedirs(export_dir, exist_ok=True)\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            file_path = os.path.join(export_dir, f'experiment_{experiment_id}_{timestamp}.json')\n            with open(file_path, 'w') as f:\n                json.dump({\n                    'experiment_id': experiment_id,\n                    'timestamp': timestamp,\n                    'metrics': metrics_dict\n                }, f, indent=2)\n\n        msg = f\"✓ Saved {len(metrics_dict)} metrics for experiment #{experiment_id}\"\n        if file_path:\n            msg += f\" to {file_path}\"\n        print(msg)\n        return True, msg\n    except Exception as e:\n        msg = f\"✗ Failed to save metrics: {e}\"\n        print(msg)\n        db_connection.rollback()\n        return False, msg\n\n\ndef complete_experiment(db_connection, experiment_id: int,\n                       status: str = 'completed',\n                       notes: str = None) -> bool:\n    \"\"\"Mark an experiment as complete.\"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            if notes:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, notes = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, notes, experiment_id))\n            else:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, experiment_id))\n        db_connection.commit()\n        print(f\"✓ Experiment #{experiment_id} marked as {status}\")\n        return True\n    except Exception as e:\n        print(f\"✗ Failed to complete experiment: {e}\")\n        db_connection.rollback()\n        return False\n\n\n# ============================================================================\n# TRACK FINAL EXPERIMENT\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRACKING FINAL EXPERIMENT\")\nprint(\"=\" * 70)\n\nif eval_metrics or all_combinations_results:\n    \n    # Prepare configuration\n    config_dict = {\n        'embedding_model_alias': EMBEDDING_MODEL_ALIAS,\n        'enable_query_expansion': ENABLE_QUERY_EXPANSION,\n        'enable_hybrid_search': ENABLE_HYBRID_SEARCH,\n        'enable_reranking': ENABLE_RERANKING,\n        'enable_citation_tracking': ENABLE_CITATION_TRACKING,\n        'num_expansions': NUM_EXPANSIONS,\n        'top_k_initial': TOP_K_INITIAL,\n        'top_k_final': TOP_K_FINAL,\n        'rrf_k': RRF_K,\n        'num_test_queries': eval_metrics.get('num_queries', 0),\n    }\n\n    config_hash = compute_config_hash(config_dict)\n\n    print(f\"\\nExperiment Configuration:\")\n    print(f\"  Name: {EXPERIMENT_NAME}\")\n    print(f\"  Enabled Techniques: {', '.join(ENABLED_TECHNIQUES) if ENABLED_TECHNIQUES else 'baseline'}\")\n    print(f\"  Embedding Model: {EMBEDDING_MODEL_ALIAS}\")\n    print(f\"  Config Hash: {config_hash}\")\n    print(f\"  Test Queries: {eval_metrics.get('num_queries', 0)}\\n\")\n\n    # Start experiment tracking\n    experiment_id = start_experiment(\n        db_connection,\n        experiment_name=EXPERIMENT_NAME,\n        notebook_path='advanced-techniques/10-combined-advanced-rag.ipynb',\n        embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n        config=config_dict,\n        techniques=ENABLED_TECHNIQUES,\n        notes=f'Combined advanced RAG evaluation with feature flags: {\", \".join(ENABLED_TECHNIQUES) if ENABLED_TECHNIQUES else \"baseline\"}'\n    )\n\n    # Prepare metrics for storage\n    metrics_to_store = {}\n\n    # Store current configuration metrics\n    if eval_metrics:\n        for metric_name, metric_value in eval_metrics.items():\n            if metric_name != 'num_queries':\n                metrics_to_store[f'config_{metric_name}'] = float(metric_value)\n\n    # Store all combination results\n    if all_combinations_results:\n        for combo_name, combo_metrics in all_combinations_results.items():\n            for metric_name, metric_value in combo_metrics.items():\n                if metric_name != 'num_queries':\n                    metrics_to_store[f'combo_{combo_name}_{metric_name}'] = float(metric_value)\n\n    # Configuration and metadata\n    metrics_to_store['num_queries_evaluated'] = len(ground_truth_questions)\n    metrics_to_store['config_hash'] = config_hash\n\n    # Save metrics\n    print(\"\\nSaving metrics to database...\\n\")\n    success, message = save_metrics(db_connection, experiment_id, metrics_to_store, export_to_file=True)\n\n    # Complete experiment\n    if success:\n        notes = f\"Successfully evaluated combined advanced RAG on {len(ground_truth_questions)} queries. \"\n        \n        if eval_metrics:\n            notes += f\"Precision@5: {eval_metrics.get('precision@5', 0):.4f}, \"\n            notes += f\"NDCG@5: {eval_metrics.get('ndcg@5', 0):.4f}\"\n\n        complete_experiment(db_connection, experiment_id, status='completed', notes=notes)\n\n        # Display results summary\n        print(\"\\n\" + \"=\" * 70)\n        print(\"FINAL EXPERIMENT RESULTS\")\n        print(\"=\" * 70)\n\n        print(f\"\\nExperiment ID: {experiment_id}\")\n        print(f\"Experiment Name: {EXPERIMENT_NAME}\")\n        print(f\"Status: Completed\")\n        print(f\"Config Hash: {config_hash}\")\n\n        print(f\"\\nTechniques Applied:\")\n        for technique in ENABLED_TECHNIQUES:\n            print(f\"  - {technique}\")\n\n        if eval_metrics:\n            print(f\"\\nMetrics (Current Configuration):\")\n            for metric in ['precision@5', 'recall@5', 'mrr', 'ndcg@5']:\n                value = eval_metrics.get(metric, 0)\n                print(f\"  {metric:<20} {value:.4f}\")\n\n        if all_combinations_results:\n            print(f\"\\nBest Combination Evaluated: \", end=\"\")\n            best_combo = max(all_combinations_results.items(),\n                           key=lambda x: x[1].get('ndcg@5', 0))\n            print(f\"{best_combo[0]}\")\n            print(f\"  NDCG@5: {best_combo[1].get('ndcg@5', 0):.4f}\")\n\n        print(f\"\\nResults exported to:\")\n        print(f\"  Database: evaluation_results table (experiment_id={experiment_id})\")\n        print(f\"  JSON: data/experiment_results/experiment_{experiment_id}_*.json\")\n\n        print(\"\\n\" + \"=\" * 70)\n        print(\"RECOMMENDATIONS\")\n        print(\"=\" * 70)\n        \n        print(\"\\n1. Review the combination comparison results above\")\n        print(\"2. Select the best performing configuration for your use case\")\n        print(\"3. Consider the trade-off between:\")\n        print(\"   - Retrieval Quality (NDCG, Precision)\")\n        print(\"   - Computational Cost (query expansion, reranking)\")\n        print(\"   - Latency (important for real-time applications)\")\n        print(\"4. Deploy the recommended configuration\")\n        print(\"5. Monitor performance on production queries\")\n\n        print(\"\\nNext Steps:\")\n        print(\"  - Review evaluation-lab/03-compare-experiments.ipynb to compare with previous techniques\")\n        print(\"  - Use evaluation-lab/04-plot-improvements.ipynb for detailed visualization\")\n        print(\"  - Run EVALUATION_GUIDE.md for production deployment checklist\")\n\n    else:\n        print(\"\\n✗ Failed to track experiment\")\n        complete_experiment(db_connection, experiment_id, status='failed', notes='Failed to save metrics')\n\nelse:\n    print(\"⚠️  Cannot track experiment: evaluation results not available\")\n\n# Close database connection\nprint(\"\\n\\nClosing database connections...\")\nif embeddings_db:\n    embeddings_db.close()\ndb_connection.close()\nprint(\"✓ All connections closed\")"
  },
  {
   "cell_type": "markdown",
   "id": "1ca042b3",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on results, you can:\n",
    "\n",
    "1. **Deploy your best configuration** - Use the combination with highest quality that meets your latency budget\n",
    "2. **Further optimize** - Fine-tune parameters of winning techniques\n",
    "3. **Ablation study** - Understand which techniques matter most for your domain\n",
    "4. **Production evaluation** - Test on real user queries and monitor performance\n",
    "\n",
    "See [EVALUATION_GUIDE.md](../EVALUATION_GUIDE.md) for deployment checklist."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}