{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77230aef",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. Complete `foundation/02-rag-postgresql-persistent.ipynb` to generate and store embeddings\n",
    "2. Ensure PostgreSQL is running with your embeddings registered\n",
    "3. Install dependencies: `pip install ollama psycopg2-binary`\n",
    "\n",
    "**Note:** This notebook is now integrated into `evaluation-lab/05-supplemental-embedding-analysis.ipynb` for a more complete evaluation workflow. Consider using that version instead for the full context of embedding analysis within the evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31073e36",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e263405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL connection\n",
    "POSTGRES_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'rag_db',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "}\n",
    "\n",
    "# Models\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n",
    "\n",
    "# Embedding model alias (must match the main notebook)\n",
    "EMBEDDING_MODEL_ALIAS = 'bge_base_en_v1.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcd981",
   "metadata": {},
   "source": [
    "### Load Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d07a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the PostgreSQLVectorDB class from the main notebook\n",
    "# Or import it if you've made it a shared module\n",
    "\n",
    "class PostgreSQLVectorDB:\n",
    "    \"\"\"Helper class to manage embeddings in PostgreSQL with pgvector.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, table_name):\n",
    "        self.config = config\n",
    "        self.table_name = table_name\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "        self.setup_table()\n",
    "    \n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(\n",
    "                host=self.config['host'],\n",
    "                port=self.config['port'],\n",
    "                database=self.config['database'],\n",
    "                user=self.config['user'],\n",
    "                password=self.config['password']\n",
    "            )\n",
    "            print(f'✓ Connected to PostgreSQL at {self.config[\"host\"]}:{self.config[\"port\"]}')\n",
    "        except psycopg2.OperationalError as e:\n",
    "            print(f'✗ Failed to connect to PostgreSQL: {e}')\n",
    "            raise\n",
    "    \n",
    "    def setup_table(self):\n",
    "        with self.conn.cursor() as cur:\n",
    "            # Create extension\n",
    "            cur.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "            \n",
    "            # Create table if it doesn't exist\n",
    "            cur.execute(f'''\n",
    "                CREATE TABLE IF NOT EXISTS {self.table_name} (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    chunk_text TEXT NOT NULL,\n",
    "                    embedding vector(768),\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create index\n",
    "            index_name = f'{self.table_name}_embedding_idx'\n",
    "            cur.execute(f'''\n",
    "                CREATE INDEX IF NOT EXISTS {index_name}\n",
    "                ON {self.table_name} USING hnsw (embedding vector_cosine_ops)\n",
    "            ''')\n",
    "            \n",
    "            self.conn.commit()\n",
    "            print(f'✓ Table \"{self.table_name}\" ready')\n",
    "    \n",
    "    def get_chunk_count(self):\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n",
    "            return cur.fetchone()[0]\n",
    "    \n",
    "    def similarity_search(self, query_embedding, top_n=3):\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                SELECT chunk_text, \n",
    "                       1 - (embedding <=> %s::vector) as similarity\n",
    "                FROM {self.table_name}\n",
    "                ORDER BY embedding <=> %s::vector\n",
    "                LIMIT %s\n",
    "            ''', (query_embedding, query_embedding, top_n))\n",
    "            \n",
    "            results = cur.fetchall()\n",
    "            return [(chunk, score) for chunk, score in results]\n",
    "    \n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb288f",
   "metadata": {},
   "source": [
    "### Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the stored embeddings\n",
    "table_name = f'embeddings_{EMBEDDING_MODEL_ALIAS.replace(\".\", \"_\")}'\n",
    "db = PostgreSQLVectorDB(POSTGRES_CONFIG, table_name)\n",
    "\n",
    "count = db.get_chunk_count()\n",
    "print(f'\\n✓ Loaded {count} embeddings from database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4b011",
   "metadata": {},
   "source": [
    "## Experiment Ideas\n",
    "\n",
    "### 1. Analyze Query Performance\n",
    "\n",
    "Test how well different types of queries retrieve relevant chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_retrieval(query, top_n=5):\n",
    "    \"\"\"Test retrieval quality for a query.\"\"\"\n",
    "    query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    results = db.similarity_search(query_embedding, top_n=top_n)\n",
    "    \n",
    "    print(f'\\nQuery: \"{query}\"')\n",
    "    print(f'\\nTop {top_n} results:')\n",
    "    for i, (chunk, score) in enumerate(results, 1):\n",
    "        preview = chunk[:100].replace('\\n', ' ') + '...'\n",
    "        print(f'{i}. (similarity: {score:.4f}) {preview}')\n",
    "\n",
    "# Test different query types\n",
    "analyze_retrieval('What is photosynthesis?', top_n=3)\n",
    "analyze_retrieval('Napoleon', top_n=3)\n",
    "analyze_retrieval('programming', top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482461a2",
   "metadata": {},
   "source": [
    "### 2. Compare Embedding Models\n",
    "\n",
    "If you have embeddings from different models stored in different tables, compare them:\n",
    "\n",
    "```python\n",
    "# After generating embeddings with a different model (e.g., 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "# you'll have a separate table\n",
    "\n",
    "db1 = PostgreSQLVectorDB(POSTGRES_CONFIG, 'embeddings_bge_base_en_v1_5')\n",
    "db2 = PostgreSQLVectorDB(POSTGRES_CONFIG, 'embeddings_all_minilm_l6_v2')\n",
    "\n",
    "# Compare retrieval results\n",
    "query = 'What is AI?'\n",
    "query_emb1 = ollama.embed(model='model1', input=query)['embeddings'][0]\n",
    "query_emb2 = ollama.embed(model='model2', input=query)['embeddings'][0]\n",
    "\n",
    "results1 = db1.similarity_search(query_emb1, top_n=3)\n",
    "results2 = db2.similarity_search(query_emb2, top_n=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b4efb",
   "metadata": {},
   "source": [
    "### 3. Statistical Analysis\n",
    "\n",
    "Analyze embedding statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d92ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def analyze_embeddings():\n",
    "    \"\"\"Compute statistics about embeddings.\"\"\"\n",
    "    with db.conn.cursor() as cur:\n",
    "        # Get embedding dimension\n",
    "        cur.execute(f'SELECT embedding FROM {db.table_name} LIMIT 1')\n",
    "        first_embedding = cur.fetchone()[0]\n",
    "        dimension = len(first_embedding)\n",
    "        \n",
    "        print(f'Embedding dimension: {dimension}')\n",
    "        \n",
    "        # Calculate average pairwise similarity (optional - can be slow for large datasets)\n",
    "        # This shows how similar chunks are to each other on average\n",
    "        # For demonstration, we'll just report the count and dimension\n",
    "        \n",
    "        print(f'Total chunks: {db.get_chunk_count()}')\n",
    "\n",
    "analyze_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2e65a",
   "metadata": {},
   "source": [
    "### 4. Debug Retrieval Quality\n",
    "\n",
    "Identify queries that retrieve poor results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_retrieval_quality(queries):\n",
    "    \"\"\"Test a batch of queries and identify low-quality retrievals.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "        retrieved = db.similarity_search(query_embedding, top_n=1)\n",
    "        \n",
    "        if retrieved:\n",
    "            chunk, score = retrieved[0]\n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'top_score': score,\n",
    "                'preview': chunk[:50] + '...'\n",
    "            })\n",
    "    \n",
    "    # Sort by quality\n",
    "    results.sort(key=lambda x: x['top_score'])\n",
    "    \n",
    "    print('\\nRetrieval Quality (sorted by score):')\n",
    "    for r in results:\n",
    "        print(f\"{r['top_score']:.4f} | {r['query']:<30} | {r['preview']}\")\n",
    "\n",
    "# Test with various queries\n",
    "test_queries = [\n",
    "    'What is water?',\n",
    "    'Tell me about plants',\n",
    "    'How do computers work?',\n",
    "    'What is mathematics?',\n",
    "]\n",
    "\n",
    "test_retrieval_quality(test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae8965",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Close database connection when done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
