{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb3fb09",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. ✅ foundation/00-setup-postgres-schema.ipynb\n",
    "2. ✅ foundation/02-rag-postgresql-persistent.ipynb (generates initial embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab78a6f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29284f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic generation approach\n",
    "GENERATION_APPROACH = \"blended\"  # 'llm' or 'template' or 'blended'\n",
    "LLM_MODEL = \"gpt-3.5-turbo\"      # For LLM generation\n",
    "QUESTIONS_PER_CHUNK = 3          # How many questions per document chunk\n",
    "\n",
    "# Curation\n",
    "BATCH_SIZE = 10  # How many to show at once\n",
    "INTERACTIVE_MODE = True  # Set to False for automated labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f6dd3",
   "metadata": {},
   "source": [
    "## Phase 1: Synthetic Generation\n",
    "\n",
    "### Option A: LLM-Based Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: LLM-based question generation\n",
    "# For each chunk from dataset:\n",
    "#   1. Call LLM: \"Generate 3 questions that would be answered by this chunk:\"\n",
    "#   2. Parse response, extract questions\n",
    "#   3. Append (question, chunk_id, source='llm') to candidate list\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e58986",
   "metadata": {},
   "source": [
    "### Option B: Template-Based Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Template-based question generation\n",
    "# For each chunk:\n",
    "#   1. Extract named entities (persons, places, organizations)\n",
    "#   2. Extract facts (who did what, when, where)\n",
    "#   3. Generate questions using templates:\n",
    "#      - \"Who is {entity}?\"\n",
    "#      - \"What did {entity} do?\"\n",
    "#      - \"When did {event} happen?\"\n",
    "#   4. Append (question, chunk_id, source='template') to candidate list\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14754c4",
   "metadata": {},
   "source": [
    "### Deduplicate Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadf235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deduplicate\n",
    "# 1. For each pair of questions, compute similarity\n",
    "# 2. If similarity > 0.8, keep only one\n",
    "# 3. Randomize order for curation\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d63db8",
   "metadata": {},
   "source": [
    "## Phase 2: Interactive Human Curation\n",
    "\n",
    "Curator reviews each generated question and provides feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaaf0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Interactive curation loop\n",
    "# For each question in candidate list:\n",
    "#   1. Show: question, source chunks (full text), suggested chunk IDs\n",
    "#   2. Prompt curator:\n",
    "#      - [G]ood - accept as-is\n",
    "#      - [B]ad - reject\n",
    "#      - [A]mbiguous - needs clarification\n",
    "#      - [R]eject - don't want this question\n",
    "#      - [N]otes - add notes\n",
    "#   3. If [G]ood:\n",
    "#      - Insert to evaluation_groundtruth with quality_rating='good'\n",
    "#   4. If [A]mbiguous:\n",
    "#      - Let curator edit question and relevant_chunk_ids\n",
    "#      - Insert with quality_rating='ambiguous'\n",
    "#   5. If [B]ad or [R]eject:\n",
    "#      - Insert with quality_rating='rejected'\n",
    "#   6. If [N]otes:\n",
    "#      - Capture human_notes field\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f497e50",
   "metadata": {},
   "source": [
    "## Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36557636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Store curated ground-truth\n",
    "# INSERT curated questions into evaluation_groundtruth table:\n",
    "# - question (string)\n",
    "# - source_type ('llm_generated' or 'template_based' or 'human_written')\n",
    "# - relevant_chunk_ids (integer array)\n",
    "# - quality_rating ('good', 'ambiguous', 'rejected')\n",
    "# - human_notes (optional curator comments)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018426d8",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show summary statistics\n",
    "# - Total questions curated\n",
    "# - Distribution by quality_rating\n",
    "# - Distribution by source_type\n",
    "# - Average chunk_ids per question\n",
    "# - Time spent on curation\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
