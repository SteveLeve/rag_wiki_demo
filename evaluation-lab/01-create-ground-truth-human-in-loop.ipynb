{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb3fb09",
   "metadata": {},
   "source": "## Prerequisites\n\n1. ✅ foundation/00-setup-postgres-schema.ipynb (creates evaluation_groundtruth table)\n2. ✅ foundation/02-rag-postgresql-persistent.ipynb (generates and stores embeddings)\n\n## What This Notebook Does\n\nThis notebook creates a **ground-truth test set** for evaluating RAG systems:\n\n1. **Phase 1: Synthetic Generation** - Generate candidate questions from chunks using:\n   - LLM-based generation (using Ollama, local and free)\n   - Template-based generation (fallback, pattern matching)\n   - Blended approach (use both)\n\n2. **Phase 2: Deduplication** - Remove near-duplicate questions using semantic similarity\n\n3. **Phase 3: Interactive Curation** - Human review and rating of each question with options:\n   - [G]ood - Accept as-is\n   - [B]ad - Reject\n   - [E]dit - Modify the question\n   - [N]otes - Add clarification notes\n   - [S]kip - Skip this question\n\n4. **Phase 4: Storage** - Store curated questions to `evaluation_groundtruth` table\n\n## Quick Start\n\n1. Set `INTERACTIVE_MODE = True` (line below Configuration) to enable human curation\n2. Set `SAMPLE_SIZE = 20` to start with a small test (adjust up after testing)\n3. Run all cells from top to bottom\n4. Answer curation prompts as they appear\n5. View summary statistics at the end\n\n## Output\n\nQuestions stored in PostgreSQL `evaluation_groundtruth` table with:\n- question (text)\n- source_type ('llm_generated', 'template_based', '_edited')\n- relevant_chunk_ids (which chunks answer this)\n- quality_rating ('good', 'bad', 'ambiguous')\n- human_notes (optional curator comments)"
  },
  {
   "cell_type": "markdown",
   "id": "2ab78a6f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "t0uk1e9pm9",
   "source": "import ollama\nimport psycopg2\nimport json\nimport pandas as pd\nimport numpy as np\nimport re\nimport random\nimport time\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Optional",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29284f2a",
   "metadata": {},
   "outputs": [],
   "source": "# PostgreSQL configuration\nPOSTGRES_CONFIG = {\n    'host': 'localhost',\n    'port': 5432,\n    'database': 'rag_db',\n    'user': 'postgres',\n    'password': 'postgres',\n}\n\n# LLM Configuration\nLLM_MODEL = 'llama3.2:1b'  # Ollama model for generation (local, free)\nGENERATION_APPROACH = 'blended'  # 'llm', 'template', or 'blended'\nQUESTIONS_PER_CHUNK = 3\nBATCH_SIZE = 10  # Questions per curation batch\n\n# Deduplication\nSIMILARITY_THRESHOLD = 0.85\n\n# Sampling\nSAMPLE_SIZE = 20  # Number of chunks to generate questions from (start small)\n\n# Interactive Mode\nINTERACTIVE_MODE = True  # Set False to use automatic labeling\n\n# Which embedding model to use for deduplication\nEMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\nEMBEDDING_MODEL_ALIAS = 'bge_base_en_v1.5'"
  },
  {
   "cell_type": "markdown",
   "id": "e74f6dd3",
   "metadata": {},
   "source": [
    "## Phase 1: Synthetic Generation\n",
    "\n",
    "### Option A: LLM-Based Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7b489",
   "metadata": {},
   "outputs": [],
   "source": "def generate_questions_with_llm(chunk_text: str, chunk_id: int, num_questions: int = 3, \n                               llm_model: str = 'llama3.2:1b') -> List[Tuple[str, int, str]]:\n    \"\"\"\n    Generate questions from a chunk using Ollama LLM.\n    \n    Args:\n        chunk_text: Text chunk to generate questions from\n        chunk_id: ID of the chunk (for tracking)\n        num_questions: How many questions to generate\n        llm_model: Ollama model name\n        \n    Returns:\n        List of (question, chunk_id, source_type) tuples\n    \"\"\"\n    prompt = f\"\"\"Based on the following text, generate exactly {num_questions} clear, specific questions that can be answered using ONLY the information in this text.\n\nFormat each question on a new line starting with \"Q:\". Do not include the \"Q:\" prefix in the actual question - only use it as a marker.\n\nText:\n{chunk_text[:1000]}\n\nGenerate exactly {num_questions} questions:\"\"\"\n    \n    try:\n        response = ollama.chat(\n            model=llm_model,\n            messages=[{\n                'role': 'user',\n                'content': prompt\n            }]\n        )\n        \n        # Parse response to extract questions\n        content = response['message']['content']\n        questions = []\n        \n        for line in content.split('\\n'):\n            line = line.strip()\n            \n            # Pattern 1: \"Q: What is...\"\n            if line.startswith('Q:'):\n                question = line[2:].strip()\n                if question and len(question) > 10:  # Skip too-short questions\n                    questions.append((question, chunk_id, 'llm_generated'))\n            \n            # Pattern 2: \"1. What is...\" or \"1) What is...\"\n            elif line and (line[0].isdigit()):\n                # Split on . or )\n                parts = re.split(r'[\\.\\)]\\s+', line, 1)\n                if len(parts) > 1:\n                    question = parts[1].strip()\n                    if question and len(question) > 10 and ('?' in question or any(q in question for q in ['What', 'How', 'When', 'Where', 'Who', 'Why'])):\n                        questions.append((question, chunk_id, 'llm_generated'))\n        \n        return questions[:num_questions]\n    \n    except Exception as e:\n        print(f\"  ✗ LLM generation failed for chunk {chunk_id}: {e}\")\n        return []\n\n# Test LLM generation\nprint(\"Testing LLM-based question generation...\")\ntest_chunk = \"\"\"\nAlbert Einstein was a German-born theoretical physicist who developed the theory of relativity. \nHe won the Nobel Prize in Physics in 1921. Einstein's mass-energy equivalence formula, E=mc², \nis one of the most famous equations in science. He spent his later years at Princeton University.\n\"\"\"\n\ntest_qs = generate_questions_with_llm(test_chunk, chunk_id=1, num_questions=3)\nprint(f\"Generated {len(test_qs)} questions:\")\nfor q, cid, src in test_qs:\n    print(f\"  - {q} (chunk_id={cid}, source={src})\")\nprint()"
  },
  {
   "cell_type": "markdown",
   "id": "58e58986",
   "metadata": {},
   "source": [
    "### Option B: Template-Based Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b4bc1",
   "metadata": {},
   "outputs": [],
   "source": "def generate_questions_with_templates(chunk_text: str, chunk_id: int) -> List[Tuple[str, int, str]]:\n    \"\"\"\n    Generate questions using template patterns as fallback.\n    \n    Args:\n        chunk_text: Text to generate questions from\n        chunk_id: ID of the chunk\n        \n    Returns:\n        List of (question, chunk_id, source_type) tuples\n    \"\"\"\n    questions = []\n    \n    # Pattern 1: Extract capitalized entities (potential names/places)\n    entities = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', chunk_text)\n    entities = list(dict.fromkeys(entities))[:5]  # Unique, max 5, preserve order\n    \n    # Pattern 2: Extract numbers and years\n    years = re.findall(r'\\b(19|20)\\d{2}\\b', chunk_text)\n    years = list(set(years))[:3]\n    \n    numbers_with_units = re.findall(r'\\b(\\d+)\\s*(?:percent|%|million|billion|thousand)\\b', chunk_text, re.IGNORECASE)\n    \n    # Generate questions from entities\n    for entity in entities:\n        if entity in chunk_text and entity not in ['Article', 'Text', 'The', 'A']:\n            questions.append((f\"What is {entity}?\", chunk_id, 'template_based'))\n            questions.append((f\"Tell me about {entity}.\", chunk_id, 'template_based'))\n    \n    # Generate questions from years\n    for year in years:\n        questions.append((f\"What happened in {year}?\", chunk_id, 'template_based'))\n    \n    # Generate questions from numbers\n    for num in numbers_with_units[:2]:\n        questions.append((f\"What does {num} refer to in this text?\", chunk_id, 'template_based'))\n    \n    # Generic questions\n    if len(chunk_text) > 100:\n        questions.append((\"What is the main topic discussed?\", chunk_id, 'template_based'))\n        questions.append((\"What key information is provided?\", chunk_id, 'template_based'))\n    \n    # Remove duplicates and return\n    seen = set()\n    unique_questions = []\n    for q, cid, src in questions:\n        q_lower = q.lower()\n        if q_lower not in seen:\n            seen.add(q_lower)\n            unique_questions.append((q, cid, src))\n    \n    return unique_questions[:10]  # Max 10 template questions\n\n# Test template generation\nprint(\"Testing template-based question generation...\")\ntest_qs = generate_questions_with_templates(test_chunk, chunk_id=1)\nprint(f\"Generated {len(test_qs)} questions:\")\nfor q, cid, src in test_qs:\n    print(f\"  - {q} (chunk_id={cid}, source={src})\")\nprint()"
  },
  {
   "cell_type": "markdown",
   "id": "f14754c4",
   "metadata": {},
   "source": [
    "### Deduplicate Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadf235",
   "metadata": {},
   "outputs": [],
   "source": "def deduplicate_questions(candidate_questions: List[Tuple[str, int, str]], \n                         threshold: float = 0.85) -> List[Tuple[str, int, str]]:\n    \"\"\"\n    Remove near-duplicate questions using semantic similarity with embeddings.\n    \n    Args:\n        candidate_questions: List of (question, chunk_id, source_type) tuples\n        threshold: Similarity threshold (0.85 = 85% similar means duplicate)\n        \n    Returns:\n        Deduplicated list of questions\n    \"\"\"\n    if len(candidate_questions) <= 1:\n        return candidate_questions\n    \n    print(f\"Deduplicating {len(candidate_questions)} questions (threshold: {threshold})...\")\n    \n    # Extract just the question text\n    question_texts = [q[0] for q in candidate_questions]\n    \n    # Generate embeddings for all questions\n    embeddings = []\n    for i, q_text in enumerate(question_texts):\n        try:\n            emb_response = ollama.embed(model=EMBEDDING_MODEL, input=q_text)\n            emb = emb_response['embeddings'][0]\n            embeddings.append(emb)\n        except Exception as e:\n            print(f\"  Warning: Failed to embed question {i}: {e}\")\n            # Use zero embedding as fallback\n            embeddings.append([0.0] * 768)\n    \n    # Compute cosine similarity between pairs\n    def cosine_similarity(a, b):\n        \"\"\"Compute cosine similarity between two vectors.\"\"\"\n        a_arr = np.array(a)\n        b_arr = np.array(b)\n        dot_product = np.dot(a_arr, b_arr)\n        norm_a = np.linalg.norm(a_arr)\n        norm_b = np.linalg.norm(b_arr)\n        if norm_a == 0 or norm_b == 0:\n            return 0.0\n        return dot_product / (norm_a * norm_b)\n    \n    # Keep indices of non-duplicate questions\n    keep_indices = []\n    for i in range(len(embeddings)):\n        is_duplicate = False\n        \n        # Check similarity with all previously kept questions\n        for j in keep_indices:\n            sim = cosine_similarity(embeddings[i], embeddings[j])\n            if sim > threshold:\n                is_duplicate = True\n                break\n        \n        if not is_duplicate:\n            keep_indices.append(i)\n    \n    # Return non-duplicate questions\n    deduplicated = [candidate_questions[i] for i in keep_indices]\n    print(f\"  Deduplicated: {len(candidate_questions)} → {len(deduplicated)} questions\")\n    \n    return deduplicated\n\n# Test deduplication\nprint(\"Testing deduplication...\")\ntest_questions = [\n    (\"What is Albert Einstein?\", 1, \"llm_generated\"),\n    (\"Tell me about Einstein.\", 1, \"template_based\"),\n    (\"Who was Einstein?\", 1, \"template_based\"),\n    (\"What is physics?\", 1, \"template_based\"),\n]\ndedup = deduplicate_questions(test_questions, threshold=0.85)\nprint(f\"Deduplication result: {len(test_questions)} → {len(dedup)} questions\")\nfor q, cid, src in dedup:\n    print(f\"  - {q}\")\nprint()"
  },
  {
   "cell_type": "markdown",
   "id": "f7d63db8",
   "metadata": {},
   "source": [
    "## Phase 2: Interactive Human Curation\n",
    "\n",
    "Curator reviews each generated question and provides feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaaf0c4",
   "metadata": {},
   "outputs": [],
   "source": "def curate_questions_interactively(candidate_questions: List[Tuple[str, int, str]], \n                                   db_connection,\n                                   batch_size: int = 10) -> List[Dict]:\n    \"\"\"\n    Interactive curation with human-in-the-loop feedback.\n    \n    Args:\n        candidate_questions: List of (question, chunk_id, source_type) tuples\n        db_connection: PostgreSQL connection for immediate storage\n        batch_size: Questions per batch\n        \n    Returns:\n        List of curated question dicts\n    \"\"\"\n    curated = []\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"INTERACTIVE CURATION - {len(candidate_questions)} questions to review\")\n    print(f\"{'='*70}\")\n    print(\"\\nOptions: [G]ood | [B]ad | [E]dit | [N]otes | [S]kip\")\n    print(\"=\"*70 + \"\\n\")\n    \n    for batch_num, i in enumerate(range(0, len(candidate_questions), batch_size)):\n        batch = candidate_questions[i:i+batch_size]\n        \n        print(f\"\\n--- Batch {batch_num + 1} ({len(batch)} questions) ---\\n\")\n        \n        for idx, (question, chunk_id, source_type) in enumerate(batch):\n            print(f\"\\nQuestion {idx+1}/{len(batch)}:\")\n            print(f\"  Text: {question}\")\n            print(f\"  Source: {source_type}\")\n            print(f\"  Chunk ID: {chunk_id}\")\n            \n            # Get user feedback\n            while True:\n                choice = input(\"  Rate: [G]ood / [B]ad / [E]dit / [N]otes / [S]kip: \").strip().lower()\n                \n                if choice == 'g':\n                    curated.append({\n                        'question': question,\n                        'chunk_ids': [chunk_id],\n                        'source_type': source_type,\n                        'quality_rating': 'good',\n                        'human_notes': None\n                    })\n                    print(\"  ✓ Marked as GOOD\\n\")\n                    break\n                \n                elif choice == 'b':\n                    curated.append({\n                        'question': question,\n                        'chunk_ids': [chunk_id],\n                        'source_type': source_type,\n                        'quality_rating': 'bad',\n                        'human_notes': 'Rejected during curation'\n                    })\n                    print(\"  ✗ Marked as BAD\\n\")\n                    break\n                \n                elif choice == 'e':\n                    edited_q = input(\"  Enter edited question: \").strip()\n                    if edited_q:\n                        curated.append({\n                            'question': edited_q,\n                            'chunk_ids': [chunk_id],\n                            'source_type': f\"{source_type}_edited\",\n                            'quality_rating': 'good',\n                            'human_notes': f\"Edited from: {question}\"\n                        })\n                        print(\"  ✓ Saved EDITED version\\n\")\n                    break\n                \n                elif choice == 'n':\n                    notes = input(\"  Enter notes: \").strip()\n                    curated.append({\n                        'question': question,\n                        'chunk_ids': [chunk_id],\n                        'source_type': source_type,\n                        'quality_rating': 'ambiguous',\n                        'human_notes': notes if notes else 'Flagged for ambiguity'\n                    })\n                    print(\"  ⚠ Marked as AMBIGUOUS with notes\\n\")\n                    break\n                \n                elif choice == 's':\n                    print(\"  ⊘ Skipped\\n\")\n                    break\n                \n                else:\n                    print(\"  Invalid choice. Try again.\")\n    \n    return curated\n\nprint(\"Interactive curation function defined.\")\nprint(\"Set INTERACTIVE_MODE = True to enable curation when running the full pipeline.\")"
  },
  {
   "cell_type": "markdown",
   "id": "4f497e50",
   "metadata": {},
   "source": [
    "## Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36557636",
   "metadata": {},
   "outputs": [],
   "source": "def store_curated_questions(curated_questions: List[Dict], db_connection):\n    \"\"\"\n    Store curated questions to evaluation_groundtruth table.\n    \n    Args:\n        curated_questions: List of question dicts with all metadata\n        db_connection: PostgreSQL connection\n        \n    Returns:\n        Number of questions stored\n    \"\"\"\n    if not curated_questions:\n        print(\"No curated questions to store.\")\n        return 0\n    \n    try:\n        stored_count = 0\n        with db_connection.cursor() as cur:\n            for q in curated_questions:\n                try:\n                    cur.execute('''\n                        INSERT INTO evaluation_groundtruth \n                        (question, source_type, relevant_chunk_ids, quality_rating, human_notes)\n                        VALUES (%s, %s, %s, %s, %s)\n                    ''', (\n                        q['question'],\n                        q['source_type'],\n                        q['chunk_ids'],  # PostgreSQL will store as integer array\n                        q['quality_rating'],\n                        q['human_notes']\n                    ))\n                    stored_count += 1\n                except Exception as e:\n                    print(f\"  Warning: Failed to store question '{q['question'][:50]}...': {e}\")\n        \n        db_connection.commit()\n        print(f\"\\n✓ Stored {stored_count}/{len(curated_questions)} questions to evaluation_groundtruth table\")\n        return stored_count\n    \n    except Exception as e:\n        db_connection.rollback()\n        print(f\"\\n✗ Error storing questions: {e}\")\n        return 0\n\nprint(\"Storage function defined.\")"
  },
  {
   "cell_type": "markdown",
   "id": "018426d8",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50128c",
   "metadata": {},
   "outputs": [],
   "source": "def show_summary_statistics(curated_questions: List[Dict]):\n    \"\"\"\n    Display comprehensive summary of curation results.\n    \n    Args:\n        curated_questions: List of curated question dicts\n    \"\"\"\n    if not curated_questions:\n        print(\"No curated questions to summarize.\")\n        return\n    \n    # Create DataFrame for analysis\n    df = pd.DataFrame(curated_questions)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"CURATION SUMMARY STATISTICS\")\n    print(\"=\"*70)\n    \n    print(f\"\\nTotal Questions Curated: {len(curated_questions)}\")\n    \n    # Quality rating distribution\n    print(\"\\nBy Quality Rating:\")\n    rating_counts = df['quality_rating'].value_counts()\n    for rating, count in rating_counts.items():\n        pct = (count / len(curated_questions)) * 100\n        print(f\"  {rating:12s}: {count:3d} ({pct:5.1f}%)\")\n    \n    # Source type distribution\n    print(\"\\nBy Source Type:\")\n    source_counts = df['source_type'].value_counts()\n    for source, count in source_counts.items():\n        pct = (count / len(curated_questions)) * 100\n        print(f\"  {source:20s}: {count:3d} ({pct:5.1f}%)\")\n    \n    # Chunk ID statistics\n    chunk_counts = df['chunk_ids'].apply(len)\n    print(f\"\\nChunk ID Statistics:\")\n    print(f\"  Average chunk IDs per question: {chunk_counts.mean():.2f}\")\n    print(f\"  Min chunk IDs: {chunk_counts.min()}\")\n    print(f\"  Max chunk IDs: {chunk_counts.max()}\")\n    \n    # Notes statistics\n    has_notes = df[df['human_notes'].notna()].shape[0]\n    print(f\"\\nHuman Notes:\")\n    print(f\"  Questions with notes: {has_notes} ({(has_notes/len(curated_questions))*100:.1f}%)\")\n    \n    # Good questions summary\n    good_count = (df['quality_rating'] == 'good').sum()\n    print(f\"\\nQuality Summary:\")\n    print(f\"  Good questions (ready for evaluation): {good_count}\")\n    print(f\"  Ambiguous questions (need clarification): {(df['quality_rating'] == 'ambiguous').sum()}\")\n    print(f\"  Bad questions (rejected): {(df['quality_rating'] == 'bad').sum()}\")\n    \n    print(\"\\n\" + \"=\"*70)\n\n# Test summary function\nprint(\"Summary function defined.\")"
  },
  {
   "cell_type": "code",
   "id": "xuu4o4qwhkd",
   "source": "# Test summary function\nprint(\"Summary function defined.\")\n\n# =============================================================================\n# MAIN PIPELINE: Generate, Deduplicate, Curate, and Store Ground Truth\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"GROUND TRUTH CREATION PIPELINE\")\nprint(\"=\"*70)\n\n# Step 1: Connect to database\nprint(\"\\n[1/6] Connecting to PostgreSQL...\")\ntry:\n    conn = psycopg2.connect(\n        host=POSTGRES_CONFIG['host'],\n        port=POSTGRES_CONFIG['port'],\n        database=POSTGRES_CONFIG['database'],\n        user=POSTGRES_CONFIG['user'],\n        password=POSTGRES_CONFIG['password']\n    )\n    print(\"✓ Connected to PostgreSQL\")\nexcept Exception as e:\n    print(f\"✗ Failed to connect: {e}\")\n    print(\"Make sure PostgreSQL is running. Exiting.\")\n    raise\n\n# Step 2: Load sample chunks from database\nprint(\"\\n[2/6] Loading sample chunks from embeddings database...\")\ntry:\n    table_name = f'embeddings_{EMBEDDING_MODEL_ALIAS.replace(\".\", \"_\")}'\n    with conn.cursor() as cur:\n        # Get total count\n        cur.execute(f'SELECT COUNT(*) FROM {table_name}')\n        total_count = cur.fetchone()[0]\n        \n        if total_count == 0:\n            print(f\"✗ No embeddings found in {table_name}\")\n            print(\"Please run foundation/02-rag-postgresql-persistent.ipynb first\")\n            raise ValueError(\"No embeddings in database\")\n        \n        # Get random sample\n        actual_sample_size = min(SAMPLE_SIZE, total_count)\n        cur.execute(f'''\n            SELECT id, chunk_text FROM {table_name}\n            ORDER BY RANDOM()\n            LIMIT %s\n        ''', (actual_sample_size,))\n        \n        chunks = cur.fetchall()\n        print(f\"✓ Loaded {len(chunks)} sample chunks from {total_count:,} total embeddings\")\nexcept Exception as e:\n    print(f\"✗ Failed to load chunks: {e}\")\n    raise\n\n# Step 3: Generate candidate questions\nprint(\"\\n[3/6] Generating candidate questions...\")\nstart_time = time.time()\ncandidate_questions = []\n\nfor chunk_id, chunk_text in chunks:\n    if GENERATION_APPROACH in ['llm', 'blended']:\n        # LLM-based generation\n        llm_questions = generate_questions_with_llm(\n            chunk_text, \n            chunk_id=chunk_id, \n            num_questions=QUESTIONS_PER_CHUNK,\n            llm_model=LLM_MODEL\n        )\n        candidate_questions.extend(llm_questions)\n    \n    if GENERATION_APPROACH in ['template', 'blended']:\n        # Template-based generation (fallback or blended)\n        template_questions = generate_questions_with_templates(chunk_text, chunk_id=chunk_id)\n        candidate_questions.extend(template_questions)\n\ngeneration_time = time.time() - start_time\nprint(f\"✓ Generated {len(candidate_questions)} candidate questions in {generation_time:.1f}s\")\nprint(f\"  - Approach: {GENERATION_APPROACH}\")\nprint(f\"  - Sample size: {len(chunks)} chunks\")\n\n# Step 4: Deduplicate questions\nprint(\"\\n[4/6] Deduplicating questions...\")\nstart_time = time.time()\ndeduplicated_questions = deduplicate_questions(candidate_questions, threshold=SIMILARITY_THRESHOLD)\ndedup_time = time.time() - start_time\nprint(f\"✓ Deduplication complete in {dedup_time:.1f}s\")\nprint(f\"  - Before: {len(candidate_questions)} questions\")\nprint(f\"  - After: {len(deduplicated_questions)} questions\")\nprint(f\"  - Removed: {len(candidate_questions) - len(deduplicated_questions)} duplicates\")\n\n# Randomize order for curation\nrandom.shuffle(deduplicated_questions)\n\n# Step 5: Interactive curation (if enabled)\nprint(\"\\n[5/6] Starting interactive curation...\")\nif INTERACTIVE_MODE:\n    curated_questions = curate_questions_interactively(\n        deduplicated_questions, \n        conn, \n        batch_size=BATCH_SIZE\n    )\nelse:\n    # Automated labeling: mark all as good\n    print(\"Running in automated mode (not interactive)\")\n    curated_questions = []\n    for question, chunk_id, source_type in deduplicated_questions:\n        curated_questions.append({\n            'question': question,\n            'chunk_ids': [chunk_id],\n            'source_type': source_type,\n            'quality_rating': 'good',\n            'human_notes': 'Auto-accepted (non-interactive mode)'\n        })\n    print(f\"✓ Auto-curated {len(curated_questions)} questions\")\n\n# Step 6: Store to database and show summary\nprint(\"\\n[6/6] Storing results and generating summary...\")\nstored = store_curated_questions(curated_questions, conn)\nshow_summary_statistics(curated_questions)\n\n# Final summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"PIPELINE COMPLETE\")\nprint(\"=\"*70)\nprint(f\"\\nResults:\")\nprint(f\"  Generated: {len(candidate_questions)} questions\")\nprint(f\"  Deduplicated: {len(deduplicated_questions)} questions\")\nprint(f\"  Curated: {len(curated_questions)} questions\")\nprint(f\"  Stored: {stored} questions to evaluation_groundtruth table\")\nprint(f\"\\nYou can now use these questions in evaluation-lab/02+ notebooks\")\nprint(f\"to measure RAG retrieval and generation quality.\")\nprint(\"=\"*70)\n\n# Close connection\nconn.close()\nprint(\"\\n✓ Database connection closed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}