{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bed2f7d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. ✅ foundation/00-setup-postgres-schema.ipynb\n",
    "2. ✅ evaluation-lab/01-create-ground-truth-human-in-loop.ipynb (curated test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee6a51",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3189a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_ALIAS = \"all-minilm-l6-v2\"\n",
    "TOP_K_VALUES = [1, 3, 5, 10]  # Compute metrics at these K values\n",
    "\n",
    "# Generation metrics (optional)\n",
    "COMPUTE_BLEU_ROUGE = False  # Set to True if you have reference answers\n",
    "USE_LLM_AS_JUDGE = False     # Set to True for answer quality scoring\n",
    "JUDGE_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "EXPERIMENT_NAME = \"metrics-framework-baseline\"\n",
    "TECHNIQUES_APPLIED = [\"vector_retrieval\"]  # Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107dd4c",
   "metadata": {},
   "source": [
    "## Load Ground-Truth Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22604e0f",
   "metadata": {},
   "outputs": [],
   "source": "import psycopg2\nimport psycopg2.extras\nimport json\nimport math\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Set, Dict, Tuple, Optional\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Database connection\ndb_connection = psycopg2.connect(\n    host='localhost',\n    port=5432,\n    database='rag_wiki_demo',\n    user='postgres',\n    password='postgres'\n)\n\n# Load test queries from evaluation_groundtruth\nground_truth_questions = []\n\nwith db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n    cur.execute('''\n        SELECT \n            id,\n            question,\n            relevant_chunk_ids,\n            quality_rating,\n            source_type\n        FROM evaluation_groundtruth\n        WHERE quality_rating = 'good'\n        ORDER BY id\n    ''')\n    \n    for row in cur.fetchall():\n        ground_truth_questions.append({\n            'id': row['id'],\n            'question': row['question'],\n            'relevant_chunk_ids': row['relevant_chunk_ids'],\n            'quality_rating': row['quality_rating'],\n            'source_type': row['source_type']\n        })\n\nprint(f\"Loaded {len(ground_truth_questions)} ground truth questions\")\nif ground_truth_questions:\n    print(f\"Sample: {ground_truth_questions[0]['question'][:80]}...\")\n    print(f\"Relevant chunks: {ground_truth_questions[0]['relevant_chunk_ids'][:3]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "a498f66d",
   "metadata": {},
   "source": [
    "## Compute Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a56ec",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 1: RETRIEVAL METRICS FUNCTIONS\n# ============================================================================\n\ndef precision_at_k(retrieved_chunk_ids: List[int], \n                   relevant_chunk_ids: List[int], \n                   k: int = 5) -> float:\n    \"\"\"\n    Precision@K: What percentage of top-K results are relevant?\n    \n    Formula: |{relevant in top-K}| / K\n    \n    Interpretation:\n    - 1.0 = Perfect: all K results are relevant\n    - 0.5 = Half of results are relevant\n    - 0.0 = None of the results are relevant\n    \n    Good for: Understanding result quality from user perspective\n    (\"Of what I saw, how much was useful?\")\n    \n    Args:\n        retrieved_chunk_ids: List of chunk IDs in ranked order\n        relevant_chunk_ids: List/set of ground-truth relevant chunk IDs\n        k: Number of top results to consider\n        \n    Returns:\n        float: Precision score between 0.0 and 1.0\n    \n    Example:\n        >>> retrieved = [1, 2, 3, 4, 5]\n        >>> relevant = [1, 3, 5]\n        >>> precision_at_k(retrieved, relevant, k=5)\n        0.6  # 3 out of 5 are relevant\n    \"\"\"\n    if k == 0:\n        return 0.0\n    \n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n    \n    num_relevant_in_k = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n    \n    return num_relevant_in_k / k\n\n\ndef recall_at_k(retrieved_chunk_ids: List[int], \n                relevant_chunk_ids: List[int], \n                k: int = 5) -> float:\n    \"\"\"\n    Recall@K: What percentage of all relevant chunks were found in top-K?\n    \n    Formula: |{relevant in top-K}| / |all relevant|\n    \n    Interpretation:\n    - 1.0 = Perfect: found all relevant chunks\n    - 0.5 = Found half of relevant chunks\n    - 0.0 = Found none of the relevant chunks\n    \n    Good for: Understanding coverage\n    (\"Did I find everything that's relevant?\")\n    \n    Args:\n        retrieved_chunk_ids: List of chunk IDs in ranked order\n        relevant_chunk_ids: List/set of ground-truth relevant chunk IDs\n        k: Number of top results to consider\n        \n    Returns:\n        float: Recall score between 0.0 and 1.0\n    \n    Example:\n        >>> retrieved = [1, 2, 3, 4, 5]\n        >>> relevant = [1, 3, 5, 7, 9]  # 5 total relevant\n        >>> recall_at_k(retrieved, relevant, k=5)\n        0.6  # Found 3 out of 5 relevant chunks\n    \"\"\"\n    if len(relevant_chunk_ids) == 0:\n        return 0.0\n    \n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n    \n    num_relevant_found = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n    \n    return num_relevant_found / len(relevant_set)\n\n\ndef mean_reciprocal_rank(retrieved_chunk_ids: List[int], \n                         relevant_chunk_ids: List[int]) -> float:\n    \"\"\"\n    MRR: How quickly do we find the first relevant result?\n    \n    Formula: 1 / (rank of first relevant result)\n    \n    Interpretation:\n    - 1.0 = First result is relevant\n    - 0.5 = Second result is first relevant\n    - 0.333 = Third result is first relevant\n    - 0.0 = No relevant results found\n    \n    Good for: Understanding user satisfaction\n    (\"How many results did user need to look through?\")\n    \n    Args:\n        retrieved_chunk_ids: List of chunk IDs in ranked order\n        relevant_chunk_ids: List/set of ground-truth relevant chunk IDs\n        \n    Returns:\n        float: MRR score between 0.0 and 1.0\n    \n    Example:\n        >>> retrieved = [1, 2, 3, 4, 5]\n        >>> relevant = [3, 7, 9]\n        >>> mean_reciprocal_rank(retrieved, relevant)\n        0.333  # First relevant at position 3\n    \"\"\"\n    relevant_set = set(relevant_chunk_ids)\n    \n    for rank, chunk_id in enumerate(retrieved_chunk_ids, start=1):\n        if chunk_id in relevant_set:\n            return 1.0 / rank\n    \n    return 0.0\n\n\ndef ndcg_at_k(retrieved_chunk_ids: List[int], \n              relevant_chunk_ids: List[int], \n              k: int = 5) -> float:\n    \"\"\"\n    NDCG@K: Normalized Discounted Cumulative Gain\n    How well-ranked are the results? (rewards relevant results at top)\n    \n    Formula: DCG@K / IDCG@K\n    where DCG = sum((2^rel - 1) / log2(rank + 1))\n    \n    Interpretation:\n    - 1.0 = Perfect ranking (all relevant at top)\n    - 0.8 = Good ranking (most relevant near top)\n    - 0.5 = Mediocre ranking\n    - 0.0 = No relevant results\n    \n    Good for: Understanding ranking quality\n    (\"Are the most relevant results at the top?\")\n    \n    Args:\n        retrieved_chunk_ids: List of chunk IDs in ranked order\n        relevant_chunk_ids: List/set of ground-truth relevant chunk IDs\n        k: Number of top results to consider\n        \n    Returns:\n        float: NDCG score between 0.0 and 1.0\n    \n    Example:\n        >>> retrieved = [1, 2, 3, 4, 5]\n        >>> relevant = [1, 3, 5]\n        >>> ndcg_at_k(retrieved, relevant, k=5)\n        0.934  # Good ranking, relevant items at positions 1, 3, 5\n    \"\"\"\n    \n    def dcg_score(relevance_scores: List[float]) -> float:\n        \"\"\"Compute DCG from relevance scores.\"\"\"\n        return sum(\n            (2**rel - 1) / math.log2(rank + 2)\n            for rank, rel in enumerate(relevance_scores)\n        )\n    \n    if k == 0 or len(relevant_chunk_ids) == 0:\n        return 0.0\n    \n    # Get top-K retrieved\n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n    \n    # Binary relevance: 1 if relevant, 0 if not\n    relevance = [1 if chunk_id in relevant_set else 0 for chunk_id in retrieved_k]\n    \n    # Compute DCG for retrieved ranking\n    dcg = dcg_score(relevance)\n    \n    # Compute ideal DCG (perfect ranking)\n    ideal_relevance = sorted(relevance, reverse=True)\n    idcg = dcg_score(ideal_relevance)\n    \n    if idcg == 0:\n        return 0.0\n    \n    return dcg / idcg\n\n\n# ============================================================================\n# PART 2: FRAMEWORK ORCHESTRATOR\n# ============================================================================\n\ndef evaluate_rag_results(ground_truth_questions: List[Dict], \n                        rag_results: List[Dict], \n                        k_values: List[int] = [1, 3, 5, 10]) -> Dict:\n    \"\"\"\n    Compute all metrics for RAG results against ground truth.\n    \n    Args:\n        ground_truth_questions: List of dicts with 'question', 'relevant_chunk_ids'\n        rag_results: List of dicts with 'question', 'retrieved_chunk_ids'\n        k_values: List of K values to compute metrics for\n        \n    Returns:\n        dict: {\n            'precision@1': float (mean),\n            'precision@3': float (mean),\n            ...\n            'recall@1': float (mean),\n            ...\n            'ndcg@1': float (mean),\n            ...\n            'mrr': float (mean),\n            'per_query': [list of per-query metrics dicts]\n        }\n    \"\"\"\n    # Map questions to relevant chunks\n    gt_map = {q['question']: q['relevant_chunk_ids'] for q in ground_truth_questions}\n    \n    # Initialize metric storage\n    metrics = {f'precision@{k}': [] for k in k_values}\n    metrics.update({f'recall@{k}': [] for k in k_values})\n    metrics.update({f'ndcg@{k}': [] for k in k_values})\n    metrics['mrr'] = []\n    \n    per_query_metrics = []\n    \n    # Compute metrics for each query\n    for result in rag_results:\n        question = result['question']\n        retrieved = result['retrieved_chunk_ids']\n        \n        if question not in gt_map:\n            continue  # Skip if no ground truth\n        \n        relevant = gt_map[question]\n        \n        query_metrics = {'question': question}\n        \n        # Precision and Recall at each K\n        for k in k_values:\n            p = precision_at_k(retrieved, relevant, k)\n            r = recall_at_k(retrieved, relevant, k)\n            n = ndcg_at_k(retrieved, relevant, k)\n            \n            metrics[f'precision@{k}'].append(p)\n            metrics[f'recall@{k}'].append(r)\n            metrics[f'ndcg@{k}'].append(n)\n            \n            query_metrics[f'precision@{k}'] = p\n            query_metrics[f'recall@{k}'] = r\n            query_metrics[f'ndcg@{k}'] = n\n        \n        # MRR\n        mrr = mean_reciprocal_rank(retrieved, relevant)\n        metrics['mrr'].append(mrr)\n        query_metrics['mrr'] = mrr\n        \n        per_query_metrics.append(query_metrics)\n    \n    # Aggregate: compute means\n    aggregated = {}\n    for key, values in metrics.items():\n        if len(values) > 0:\n            aggregated[key] = np.mean(values)\n        else:\n            aggregated[key] = 0.0\n    \n    aggregated['per_query'] = per_query_metrics\n    aggregated['num_queries'] = len(per_query_metrics)\n    \n    return aggregated\n\n\n# ============================================================================\n# STEP 1: Retrieve results for all test questions\n# ============================================================================\n\ndef retrieve_results_for_questions(db_connection, questions: List[Dict], \n                                   embedding_model_alias: str = \"all-minilm-l6-v2\",\n                                   top_k: int = 10) -> List[Dict]:\n    \"\"\"\n    Retrieve top-K results for each test question using vector similarity.\n    \n    Args:\n        db_connection: PostgreSQL connection\n        questions: List of question dicts\n        embedding_model_alias: Which embedding model to use\n        top_k: Number of results to retrieve\n        \n    Returns:\n        List of dicts with 'question' and 'retrieved_chunk_ids'\n    \"\"\"\n    from sentence_transformers import SentenceTransformer\n    \n    # Load embedding model\n    model = SentenceTransformer(embedding_model_alias)\n    \n    rag_results = []\n    \n    # Get question embeddings\n    question_texts = [q['question'] for q in questions]\n    question_embeddings = model.encode(question_texts)\n    \n    with db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n        for i, question_dict in enumerate(questions):\n            question = question_dict['question']\n            question_embedding = question_embeddings[i]\n            \n            # Search for similar chunks using vector similarity\n            cur.execute('''\n                SELECT \n                    id,\n                    content,\n                    embedding\n                FROM chunks\n                WHERE embedding_model = %s\n                ORDER BY embedding <-> %s\n                LIMIT %s\n            ''', (embedding_model_alias, question_embedding, top_k))\n            \n            retrieved_chunks = cur.fetchall()\n            retrieved_ids = [chunk['id'] for chunk in retrieved_chunks]\n            \n            rag_results.append({\n                'question': question,\n                'retrieved_chunk_ids': retrieved_ids\n            })\n    \n    return rag_results\n\n\n# Retrieve results for all test questions\nprint(f\"Retrieving top-{max(TOP_K_VALUES)} results for {len(ground_truth_questions)} questions...\")\nrag_results = retrieve_results_for_questions(\n    db_connection, \n    ground_truth_questions,\n    embedding_model_alias=EMBEDDING_MODEL_ALIAS,\n    top_k=max(TOP_K_VALUES)\n)\nprint(f\"Retrieved {len(rag_results)} result sets\")\n\n\n# ============================================================================\n# STEP 2: Compute all retrieval metrics\n# ============================================================================\n\nprint(\"\\nComputing retrieval metrics...\")\nall_metrics = evaluate_rag_results(\n    ground_truth_questions,\n    rag_results,\n    k_values=TOP_K_VALUES\n)\n\n# Print summary\nprint(f\"\\nMetrics for {all_metrics['num_queries']} queries:\")\nprint(f\"{'Metric':<20} {'Value':<10}\")\nprint(\"-\" * 30)\n\nfor k in TOP_K_VALUES:\n    print(f\"Precision@{k:<13} {all_metrics[f'precision@{k}']:.4f}\")\n\nprint()\nfor k in TOP_K_VALUES:\n    print(f\"Recall@{k:<16} {all_metrics[f'recall@{k}']:.4f}\")\n\nprint()\nfor k in TOP_K_VALUES:\n    print(f\"NDCG@{k:<19} {all_metrics[f'ndcg@{k}']:.4f}\")\n\nprint()\nprint(f\"MRR (Mean Reciprocal Rank):    {all_metrics['mrr']:.4f}\")\n\n# Compute standard deviations for additional insight\nprint(\"\\nStandard Deviations:\")\nprint(f\"{'Metric':<20} {'Std Dev':<10}\")\nprint(\"-\" * 30)\n\nfor k in TOP_K_VALUES:\n    p_values = [q[f'precision@{k}'] for q in all_metrics['per_query']]\n    print(f\"Precision@{k:<13} {np.std(p_values):.4f}\")\n\nfor k in TOP_K_VALUES:\n    r_values = [q[f'recall@{k}'] for q in all_metrics['per_query']]\n    print(f\"Recall@{k:<16} {np.std(r_values):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "1ff4646e",
   "metadata": {},
   "source": [
    "## Compute Generation Metrics (Optional)\n",
    "\n",
    "Only if you have reference answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df9dfd",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 3: GENERATION METRICS (Optional)\n# ============================================================================\n\ndef compute_bleu_score(reference: str, candidate: str) -> float:\n    \"\"\"\n    BLEU (Bilingual Evaluation Understudy): n-gram precision with brevity penalty.\n    \n    Measures: \"How similar is the generated answer to the reference answer?\"\n    \n    Good for: When you have human-written reference answers\n    \n    Args:\n        reference: Ground-truth answer text\n        candidate: Generated answer text\n        \n    Returns:\n        float: BLEU score between 0.0 and 1.0\n    \"\"\"\n    try:\n        from nltk.translate.bleu_score import sentence_bleu\n        from nltk.tokenize import word_tokenize\n        \n        reference_tokens = word_tokenize(reference.lower())\n        candidate_tokens = word_tokenize(candidate.lower())\n        \n        # Use BLEU-4 (up to 4-gram matches)\n        score = sentence_bleu(\n            [reference_tokens],\n            candidate_tokens,\n            weights=(0.25, 0.25, 0.25, 0.25)\n        )\n        return score\n    except Exception as e:\n        print(f\"Error computing BLEU: {e}\")\n        return 0.0\n\n\ndef compute_rouge_score(reference: str, candidate: str) -> float:\n    \"\"\"\n    ROUGE (Recall-Oriented Understudy for Gisting Evaluation): n-gram recall.\n    \n    Measures: \"How much of the reference answer appears in the generated answer?\"\n    \n    Good for: When you want to measure coverage of key information\n    \n    Args:\n        reference: Ground-truth answer text\n        candidate: Generated answer text\n        \n    Returns:\n        float: ROUGE-L score between 0.0 and 1.0\n    \"\"\"\n    try:\n        from rouge_score import rouge_scorer\n        \n        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n        scores = scorer.score(reference, candidate)\n        \n        # Return ROUGE-L F1 score\n        return scores['rougeL'].fmeasure\n    except Exception as e:\n        print(f\"Error computing ROUGE: {e}\")\n        return 0.0\n\n\ndef compute_llm_judge_score(question: str, answer: str, \n                            judge_model: str = \"gpt-3.5-turbo\") -> Tuple[int, str]:\n    \"\"\"\n    LLM-as-Judge: Use an LLM to evaluate answer quality.\n    \n    Measures: \"Is this a good answer to the question?\"\n    \n    Good for: When you don't have reference answers but want quality scoring\n    \n    Args:\n        question: The original question\n        answer: The generated answer\n        judge_model: Which LLM to use for judging\n        \n    Returns:\n        Tuple of (score 1-5, reasoning)\n    \"\"\"\n    try:\n        import openai\n        \n        prompt = f\"\"\"Rate the quality of this answer to the question on a scale of 1-5:\n\nQuestion: {question}\n\nAnswer: {answer}\n\nProvide your rating (1-5) and brief reasoning. Format as:\nSCORE: <number>\nREASONING: <your explanation>\"\"\"\n        \n        response = openai.ChatCompletion.create(\n            model=judge_model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert evaluator of question-answering systems. Rate answers on how well they answer the question, accuracy, and completeness.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.0\n        )\n        \n        response_text = response.choices[0].message.content\n        \n        # Parse response\n        lines = response_text.split('\\n')\n        score = 3  # Default\n        reasoning = \"\"\n        \n        for line in lines:\n            if line.startswith('SCORE:'):\n                try:\n                    score = int(line.split(':')[1].strip())\n                except:\n                    pass\n            elif line.startswith('REASONING:'):\n                reasoning = line.split(':', 1)[1].strip()\n        \n        return score, reasoning\n    except Exception as e:\n        print(f\"Error with LLM judge: {e}\")\n        return 3, \"Error evaluating answer\"\n\n\n# ============================================================================\n# STEP 3: Optionally compute generation metrics\n# ============================================================================\n\ngeneration_metrics = {}\n\nif COMPUTE_BLEU_ROUGE or USE_LLM_AS_JUDGE:\n    print(\"\\n\" + \"=\" * 70)\n    print(\"COMPUTING GENERATION METRICS\")\n    print(\"=\" * 70)\n    \n    # For this example, we'll create mock generated answers\n    # In practice, you would generate answers from your RAG system\n    print(\"\\nNote: Generation metrics require either:\")\n    print(\"  1. Reference answers in evaluation_groundtruth table\")\n    print(\"  2. Generated answers from your RAG system\")\n    print(\"\\nSet COMPUTE_BLEU_ROUGE=True and COMPUTE_LLM_JUDGE=True to enable.\")\n    \n    if COMPUTE_BLEU_ROUGE:\n        print(\"\\n[BLEU/ROUGE] These metrics require reference answers.\")\n        print(\"Add 'reference_answer' column to evaluation_groundtruth if available.\")\n    \n    if USE_LLM_AS_JUDGE:\n        print(\"\\n[LLM-as-Judge] This requires OpenAI API key in environment.\")\n        print(\"Set OPENAI_API_KEY environment variable to enable.\")\n    \n    # Example: If you have reference answers and generated answers:\n    # bleu_scores = []\n    # rouge_scores = []\n    # \n    # for i, question in enumerate(ground_truth_questions):\n    #     reference = question.get('reference_answer', '')\n    #     # In practice, generate answer from your RAG system:\n    #     # generated = rag_system.generate_answer(question['question'], retrieved_docs)\n    #     \n    #     if reference and generated:\n    #         bleu = compute_bleu_score(reference, generated)\n    #         rouge = compute_rouge_score(reference, generated)\n    #         bleu_scores.append(bleu)\n    #         rouge_scores.append(rouge)\n    # \n    # if bleu_scores:\n    #     generation_metrics['bleu_score'] = np.mean(bleu_scores)\n    #     generation_metrics['rouge_score'] = np.mean(rouge_scores)\n    \n    print(\"\\nGeneration metrics computation skipped (set flags to enable)\")\n\nelse:\n    print(\"\\nGeneration metrics disabled. Focusing on retrieval evaluation.\")"
  },
  {
   "cell_type": "markdown",
   "id": "2575b7bf",
   "metadata": {},
   "source": [
    "## Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214ea89",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 4: VISUALIZATION\n# ============================================================================\n\ndef visualize_metrics(metrics: Dict, k_values: List[int] = [1, 3, 5, 10]):\n    \"\"\"\n    Create comprehensive visualizations for metrics.\n    \n    Args:\n        metrics: Dict returned by evaluate_rag_results()\n        k_values: K values to plot\n    \"\"\"\n    fig = plt.figure(figsize=(16, 12))\n    \n    # ========================================================================\n    # Plot 1: Precision and Recall at different K values\n    # ========================================================================\n    ax1 = plt.subplot(2, 3, 1)\n    \n    precision_values = [metrics[f'precision@{k}'] for k in k_values]\n    recall_values = [metrics[f'recall@{k}'] for k in k_values]\n    \n    ax1.plot(k_values, precision_values, marker='o', linewidth=2, \n             markersize=8, label='Precision@K', color='#2E86AB')\n    ax1.plot(k_values, recall_values, marker='s', linewidth=2, \n             markersize=8, label='Recall@K', color='#A23B72')\n    \n    ax1.set_xlabel('K (Number of Results)', fontsize=11, fontweight='bold')\n    ax1.set_ylabel('Score', fontsize=11, fontweight='bold')\n    ax1.set_title('Precision and Recall at Different K', fontsize=12, fontweight='bold')\n    ax1.legend(fontsize=10)\n    ax1.grid(True, alpha=0.3)\n    ax1.set_ylim(0, 1.0)\n    ax1.set_xticks(k_values)\n    \n    # ========================================================================\n    # Plot 2: NDCG at different K values\n    # ========================================================================\n    ax2 = plt.subplot(2, 3, 2)\n    \n    ndcg_values = [metrics[f'ndcg@{k}'] for k in k_values]\n    \n    bars = ax2.bar([str(k) for k in k_values], ndcg_values, alpha=0.8, \n                    color=['#06A77D', '#1FA24A', '#F18F01', '#C73E1D'],\n                    edgecolor='black', linewidth=1.5)\n    \n    # Add value labels on bars\n    for bar, value in zip(bars, ndcg_values):\n        height = bar.get_height()\n        ax2.text(bar.get_x() + bar.get_width()/2., height,\n                f'{value:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    ax2.set_xlabel('K (Number of Results)', fontsize=11, fontweight='bold')\n    ax2.set_ylabel('NDCG@K', fontsize=11, fontweight='bold')\n    ax2.set_title('Normalized Discounted Cumulative Gain', fontsize=12, fontweight='bold')\n    ax2.set_ylim(0, 1.0)\n    ax2.grid(True, alpha=0.3, axis='y')\n    \n    # ========================================================================\n    # Plot 3: MRR Score\n    # ========================================================================\n    ax3 = plt.subplot(2, 3, 3)\n    \n    mrr_score = metrics['mrr']\n    colors = ['#06A77D' if mrr_score >= 0.7 else '#F18F01' if mrr_score >= 0.4 else '#C73E1D']\n    \n    bar = ax3.bar(['MRR'], [mrr_score], alpha=0.8, color=colors[0], \n                   edgecolor='black', linewidth=2, width=0.5)\n    \n    ax3.text(0, mrr_score + 0.03, f'{mrr_score:.4f}', \n            ha='center', va='bottom', fontsize=12, fontweight='bold')\n    \n    ax3.set_ylabel('Mean Reciprocal Rank', fontsize=11, fontweight='bold')\n    ax3.set_title('First Relevant Result Position', fontsize=12, fontweight='bold')\n    ax3.set_ylim(0, 1.0)\n    ax3.grid(True, alpha=0.3, axis='y')\n    ax3.set_xticklabels(['MRR'], fontsize=11)\n    \n    # ========================================================================\n    # Plot 4: Distribution of Precision@5 Across Queries\n    # ========================================================================\n    ax4 = plt.subplot(2, 3, 4)\n    \n    if 'per_query' in metrics and len(metrics['per_query']) > 0:\n        precision_5_scores = [q['precision@5'] for q in metrics['per_query']]\n        \n        ax4.hist(precision_5_scores, bins=10, alpha=0.7, edgecolor='black', \n                color='#2E86AB', linewidth=1.5)\n        \n        mean_p5 = np.mean(precision_5_scores)\n        median_p5 = np.median(precision_5_scores)\n        \n        ax4.axvline(mean_p5, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_p5:.3f}')\n        ax4.axvline(median_p5, color='green', linestyle='--', linewidth=2, label=f'Median: {median_p5:.3f}')\n        \n        ax4.set_xlabel('Precision@5', fontsize=11, fontweight='bold')\n        ax4.set_ylabel('Number of Queries', fontsize=11, fontweight='bold')\n        ax4.set_title('Distribution of Precision@5 Across Queries', fontsize=12, fontweight='bold')\n        ax4.legend(fontsize=10)\n        ax4.grid(True, alpha=0.3, axis='y')\n    \n    # ========================================================================\n    # Plot 5: Distribution of Recall@5 Across Queries\n    # ========================================================================\n    ax5 = plt.subplot(2, 3, 5)\n    \n    if 'per_query' in metrics and len(metrics['per_query']) > 0:\n        recall_5_scores = [q['recall@5'] for q in metrics['per_query']]\n        \n        ax5.hist(recall_5_scores, bins=10, alpha=0.7, edgecolor='black', \n                color='#A23B72', linewidth=1.5)\n        \n        mean_r5 = np.mean(recall_5_scores)\n        median_r5 = np.median(recall_5_scores)\n        \n        ax5.axvline(mean_r5, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_r5:.3f}')\n        ax5.axvline(median_r5, color='green', linestyle='--', linewidth=2, label=f'Median: {median_r5:.3f}')\n        \n        ax5.set_xlabel('Recall@5', fontsize=11, fontweight='bold')\n        ax5.set_ylabel('Number of Queries', fontsize=11, fontweight='bold')\n        ax5.set_title('Distribution of Recall@5 Across Queries', fontsize=12, fontweight='bold')\n        ax5.legend(fontsize=10)\n        ax5.grid(True, alpha=0.3, axis='y')\n    \n    # ========================================================================\n    # Plot 6: Per-Query Performance Heatmap (Top queries)\n    # ========================================================================\n    ax6 = plt.subplot(2, 3, 6)\n    \n    if 'per_query' in metrics and len(metrics['per_query']) > 0:\n        # Create DataFrame for heatmap\n        df = pd.DataFrame(metrics['per_query'])\n        \n        # Select top 10 hardest queries (lowest precision@5)\n        if 'precision@5' in df.columns:\n            df_sorted = df.nsmallest(10, 'precision@5')\n            \n            # Prepare data for heatmap\n            metric_cols = [f'precision@{k}' for k in [3, 5, 10]] + ['mrr']\n            heatmap_data = df_sorted[metric_cols].values\n            \n            # Create heatmap\n            im = ax6.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n            \n            # Set ticks and labels\n            ax6.set_xticks(range(len(metric_cols)))\n            ax6.set_xticklabels(metric_cols, rotation=45, ha='right')\n            ax6.set_yticks(range(len(df_sorted)))\n            ax6.set_yticklabels([f\"Q{i}\" for i in range(1, len(df_sorted) + 1)])\n            \n            ax6.set_title('Hardest 10 Queries: Metric Performance', fontsize=12, fontweight='bold')\n            \n            # Add colorbar\n            cbar = plt.colorbar(im, ax=ax6)\n            cbar.set_label('Score', fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print summary statistics\n    print(\"\\n\" + \"=\" * 70)\n    print(\"VISUALIZATION SUMMARY\")\n    print(\"=\" * 70)\n    \n    if 'per_query' in metrics and len(metrics['per_query']) > 0:\n        df = pd.DataFrame(metrics['per_query'])\n        \n        print(\"\\nPer-Query Statistics:\")\n        print(f\"{'Metric':<20} {'Mean':<10} {'Median':<10} {'Std Dev':<10} {'Min':<10} {'Max':<10}\")\n        print(\"-\" * 60)\n        \n        for k in [3, 5, 10]:\n            col = f'precision@{k}'\n            if col in df.columns:\n                mean = df[col].mean()\n                median = df[col].median()\n                std = df[col].std()\n                min_val = df[col].min()\n                max_val = df[col].max()\n                print(f\"Precision@{k:<14} {mean:.4f}     {median:.4f}     {std:.4f}     {min_val:.4f}     {max_val:.4f}\")\n        \n        print()\n        \n        for k in [3, 5, 10]:\n            col = f'recall@{k}'\n            if col in df.columns:\n                mean = df[col].mean()\n                median = df[col].median()\n                std = df[col].std()\n                min_val = df[col].min()\n                max_val = df[col].max()\n                print(f\"Recall@{k:<17} {mean:.4f}     {median:.4f}     {std:.4f}     {min_val:.4f}     {max_val:.4f}\")\n\n\n# ============================================================================\n# STEP 4: Create visualizations\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CREATING VISUALIZATIONS\")\nprint(\"=\" * 70)\n\nvisualize_metrics(all_metrics, k_values=TOP_K_VALUES)"
  },
  {
   "cell_type": "markdown",
   "id": "cc307e10",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe986bb",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PART 5: EXPORT AND STORAGE\n# ============================================================================\n\ndef store_metrics_to_database(db_connection: psycopg2.extensions.connection,\n                             experiment_id: int, \n                             metrics: Dict) -> Tuple[bool, str]:\n    \"\"\"\n    Store computed metrics to evaluation_results table.\n    \n    Args:\n        db_connection: PostgreSQL connection\n        experiment_id: ID from start_experiment()\n        metrics: Dict of metrics from evaluate_rag_results()\n        \n    Returns:\n        Tuple of (success: bool, message: str)\n    \"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            for metric_name, metric_value in metrics.items():\n                if metric_name == 'per_query':\n                    # Store per-query details as JSON\n                    metric_details = {'per_query_results': metric_value}\n                    cur.execute('''\n                        INSERT INTO evaluation_results \n                        (experiment_id, metric_name, metric_value, metric_details_json)\n                        VALUES (%s, %s, %s, %s)\n                    ''', (\n                        experiment_id,\n                        'per_query_details',\n                        0.0,\n                        json.dumps(metric_details)\n                    ))\n                elif metric_name == 'num_queries':\n                    # Store as metadata\n                    cur.execute('''\n                        INSERT INTO evaluation_results \n                        (experiment_id, metric_name, metric_value, metric_details_json)\n                        VALUES (%s, %s, %s, %s)\n                    ''', (\n                        experiment_id,\n                        'num_queries_evaluated',\n                        float(metric_value),\n                        '{}'\n                    ))\n                else:\n                    # Store aggregate metrics\n                    cur.execute('''\n                        INSERT INTO evaluation_results \n                        (experiment_id, metric_name, metric_value, metric_details_json)\n                        VALUES (%s, %s, %s, %s)\n                    ''', (\n                        experiment_id,\n                        metric_name,\n                        float(metric_value),\n                        '{}'\n                    ))\n        \n        db_connection.commit()\n        msg = f\"Successfully stored {len(metrics)} metrics to database for experiment #{experiment_id}\"\n        print(f\"[OK] {msg}\")\n        return True, msg\n    \n    except Exception as e:\n        db_connection.rollback()\n        msg = f\"Failed to store metrics to database: {str(e)}\"\n        print(f\"[ERROR] {msg}\")\n        return False, msg\n\n\ndef save_metrics_to_json(metrics: Dict, \n                        experiment_name: str,\n                        export_dir: str = 'data/experiment_results') -> Tuple[bool, str]:\n    \"\"\"\n    Export metrics to JSON file for easy sharing and archival.\n    \n    Args:\n        metrics: Dict of metrics from evaluate_rag_results()\n        experiment_name: Name of the experiment\n        export_dir: Directory to save the JSON file\n        \n    Returns:\n        Tuple of (success: bool, file_path: str)\n    \"\"\"\n    import os\n    \n    try:\n        os.makedirs(export_dir, exist_ok=True)\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        filename = f\"{experiment_name.lower().replace(' ', '_')}_{timestamp}.json\"\n        file_path = os.path.join(export_dir, filename)\n        \n        # Prepare export data\n        export_data = {\n            'experiment_name': experiment_name,\n            'timestamp': timestamp,\n            'computed_at': datetime.now().isoformat(),\n            'metrics': {\n                'aggregated': {},\n                'per_query': metrics.get('per_query', [])\n            }\n        }\n        \n        # Add aggregated metrics\n        for key, value in metrics.items():\n            if key != 'per_query' and key != 'num_queries':\n                export_data['metrics']['aggregated'][key] = float(value)\n        \n        export_data['metrics']['aggregated']['num_queries'] = metrics.get('num_queries', 0)\n        \n        # Write to file\n        with open(file_path, 'w') as f:\n            json.dump(export_data, f, indent=2)\n        \n        msg = f\"Exported metrics to {file_path}\"\n        print(f\"[OK] {msg}\")\n        return True, file_path\n    \n    except Exception as e:\n        msg = f\"Failed to export metrics to JSON: {str(e)}\"\n        print(f\"[ERROR] {msg}\")\n        return False, \"\"\n\n\ndef generate_metrics_summary_report(metrics: Dict, \n                                   experiment_name: str,\n                                   techniques_applied: List[str]) -> str:\n    \"\"\"\n    Generate a human-readable summary report of the evaluation.\n    \n    Args:\n        metrics: Dict of metrics from evaluate_rag_results()\n        experiment_name: Name of the experiment\n        techniques_applied: List of techniques used\n        \n    Returns:\n        String containing the formatted report\n    \"\"\"\n    report = []\n    report.append(\"=\" * 80)\n    report.append(\"EVALUATION METRICS SUMMARY REPORT\")\n    report.append(\"=\" * 80)\n    report.append(\"\")\n    \n    report.append(f\"Experiment Name: {experiment_name}\")\n    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    report.append(f\"Techniques Applied: {', '.join(techniques_applied)}\")\n    report.append(f\"Number of Queries Evaluated: {metrics.get('num_queries', 0)}\")\n    report.append(\"\")\n    \n    report.append(\"-\" * 80)\n    report.append(\"AGGREGATE METRICS\")\n    report.append(\"-\" * 80)\n    report.append(\"\")\n    \n    # Precision metrics\n    report.append(\"PRECISION (What % of results shown are relevant?)\")\n    report.append(f\"  Precision@1  = {metrics.get('precision@1', 0):.4f}\")\n    report.append(f\"  Precision@3  = {metrics.get('precision@3', 0):.4f}\")\n    report.append(f\"  Precision@5  = {metrics.get('precision@5', 0):.4f}\")\n    report.append(f\"  Precision@10 = {metrics.get('precision@10', 0):.4f}\")\n    report.append(\"\")\n    \n    # Recall metrics\n    report.append(\"RECALL (What % of relevant results were found?)\")\n    report.append(f\"  Recall@1     = {metrics.get('recall@1', 0):.4f}\")\n    report.append(f\"  Recall@3     = {metrics.get('recall@3', 0):.4f}\")\n    report.append(f\"  Recall@5     = {metrics.get('recall@5', 0):.4f}\")\n    report.append(f\"  Recall@10    = {metrics.get('recall@10', 0):.4f}\")\n    report.append(\"\")\n    \n    # NDCG metrics\n    report.append(\"NDCG (How well-ranked are results?)\")\n    report.append(f\"  NDCG@1       = {metrics.get('ndcg@1', 0):.4f}\")\n    report.append(f\"  NDCG@3       = {metrics.get('ndcg@3', 0):.4f}\")\n    report.append(f\"  NDCG@5       = {metrics.get('ndcg@5', 0):.4f}\")\n    report.append(f\"  NDCG@10      = {metrics.get('ndcg@10', 0):.4f}\")\n    report.append(\"\")\n    \n    # MRR metric\n    report.append(\"MRR (How quickly do we find first relevant result?)\")\n    report.append(f\"  MRR          = {metrics.get('mrr', 0):.4f}\")\n    report.append(\"\")\n    \n    report.append(\"-\" * 80)\n    report.append(\"INTERPRETATION GUIDE\")\n    report.append(\"-\" * 80)\n    report.append(\"\")\n    report.append(\"Precision@K (higher is better, aim for 0.6+)\")\n    report.append(\"  Answers: Of the top K results shown, what fraction were relevant?\")\n    report.append(\"  User perspective: How much noise in my search results?\")\n    report.append(\"\")\n    report.append(\"Recall@K (higher is better, aim for 0.5+)\")\n    report.append(\"  Answers: Did I find all the relevant information?\")\n    report.append(\"  System perspective: How much coverage do I have?\")\n    report.append(\"\")\n    report.append(\"NDCG@K (higher is better, aim for 0.8+)\")\n    report.append(\"  Answers: Are the most relevant results ranked at the top?\")\n    report.append(\"  Ranking quality: How well-ordered is my result list?\")\n    report.append(\"\")\n    report.append(\"MRR (higher is better, aim for 0.8+)\")\n    report.append(\"  Answers: On average, which position is the first relevant result?\")\n    report.append(\"  User satisfaction: How many items until user finds value?\")\n    report.append(\"\")\n    \n    report.append(\"=\" * 80)\n    \n    return \"\\n\".join(report)\n\n\n# ============================================================================\n# STEP 5: Start experiment and store results\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"STORING RESULTS\")\nprint(\"=\" * 70)\n\n# For tracking in database, we need to use the patterns from foundation/00\n# This creates an experiment record to track the evaluation run\n\nconfig = {\n    'top_k_values': TOP_K_VALUES,\n    'embedding_model': EMBEDDING_MODEL_ALIAS,\n    'num_test_queries': len(ground_truth_questions),\n    'compute_bleu_rouge': COMPUTE_BLEU_ROUGE,\n    'use_llm_judge': USE_LLM_AS_JUDGE,\n}\n\nprint(\"\\nExperiment Configuration:\")\nfor key, value in config.items():\n    print(f\"  {key}: {value}\")\n\n# Start experiment (using patterns from foundation/00)\nprint(f\"\\nStarting experiment: {EXPERIMENT_NAME}\")\n\nwith db_connection.cursor() as cur:\n    import hashlib\n    \n    # Compute config hash for tracking\n    config_str = json.dumps(config, sort_keys=True)\n    config_hash = hashlib.sha256(config_str.encode()).hexdigest()[:12]\n    \n    # Create experiment record\n    cur.execute('''\n        INSERT INTO experiments (\n            experiment_name, \n            notebook_path, \n            embedding_model_alias,\n            config_hash,\n            config_json,\n            techniques_applied,\n            notes,\n            status\n        )\n        VALUES (%s, %s, %s, %s, %s, %s, %s, 'completed')\n        RETURNING id\n    ''', (\n        EXPERIMENT_NAME,\n        'evaluation-lab/02-evaluation-metrics-framework.ipynb',\n        EMBEDDING_MODEL_ALIAS,\n        config_hash,\n        json.dumps(config),\n        TECHNIQUES_APPLIED,\n        f\"Baseline metrics evaluation on {len(ground_truth_questions)} test queries\"\n    ))\n    \n    experiment_id = cur.fetchone()[0]\n\ndb_connection.commit()\n\nprint(f\"Created experiment #{experiment_id}\")\n\n# Store metrics to database\nprint(\"\\nStoring metrics to evaluation_results table...\")\nsuccess_db, msg_db = store_metrics_to_database(db_connection, experiment_id, all_metrics)\n\n# Export to JSON file\nprint(\"\\nExporting metrics to JSON...\")\nsuccess_json, json_path = save_metrics_to_json(all_metrics, EXPERIMENT_NAME)\n\n# Generate and print summary report\nprint(\"\\n\" + \"=\" * 70)\nsummary_report = generate_metrics_summary_report(all_metrics, EXPERIMENT_NAME, TECHNIQUES_APPLIED)\nprint(summary_report)\nprint(\"=\" * 70)\n\n# Save report to text file\nimport os\nos.makedirs('data/experiment_results', exist_ok=True)\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nreport_path = f\"data/experiment_results/{EXPERIMENT_NAME.lower().replace(' ', '_')}_{timestamp}.txt\"\nwith open(report_path, 'w') as f:\n    f.write(summary_report)\nprint(f\"\\nReport saved to: {report_path}\")\n\n# Final summary\nprint(\"\\n\" + \"=\" * 70)\nprint(\"EVALUATION COMPLETE\")\nprint(\"=\" * 70)\nprint(f\"\\nResults stored successfully:\")\nprint(f\"  Database: experiment #{experiment_id}\")\nif success_json:\n    print(f\"  JSON file: {json_path}\")\nprint(f\"  Report: {report_path}\")\n\nprint(f\"\\nKey metrics:\")\nprint(f\"  Precision@5: {all_metrics.get('precision@5', 0):.4f}\")\nprint(f\"  Recall@5: {all_metrics.get('recall@5', 0):.4f}\")\nprint(f\"  NDCG@5: {all_metrics.get('ndcg@5', 0):.4f}\")\nprint(f\"  MRR: {all_metrics.get('mrr', 0):.4f}\")\nprint(f\"  Queries evaluated: {all_metrics.get('num_queries', 0)}\")\n\nprint(\"\\nNext steps:\")\nprint(\"  1. Review the visualizations above\")\nprint(\"  2. Check the metrics report\")\nprint(\"  3. Use metrics as baseline for advanced techniques\")\nprint(\"  4. Run advanced-techniques notebooks to improve scores\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}