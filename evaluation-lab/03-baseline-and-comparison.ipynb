{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c9f475",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. ✅ foundation/00-setup-postgres-schema.ipynb\n",
    "2. ✅ evaluation-lab/01-create-ground-truth-human-in-loop.ipynb\n",
    "3. ✅ evaluation-lab/02-evaluation-metrics-framework.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417a5aa",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6182db",
   "metadata": {},
   "outputs": [],
   "source": "# Configurations to compare\nCONFIGURATIONS = [\n    {\n        \"name\": \"baseline-vector-only\",\n        \"description\": \"Vector-only retrieval (simple baseline)\",\n        \"embedding_model\": \"all-minilm-l6-v2\",\n        \"top_k\": 5,\n        \"techniques\": [\"vector_retrieval\"]\n    },\n    {\n        \"name\": \"config-variant-1\",\n        \"description\": \"Vector retrieval with larger top_k (10 vs 5)\",\n        \"embedding_model\": \"all-minilm-l6-v2\",\n        \"top_k\": 10,\n        \"techniques\": [\"vector_retrieval\"]\n    },\n    {\n        \"name\": \"config-variant-2\",\n        \"description\": \"Vector-only but increased retrieval set\",\n        \"embedding_model\": \"all-minilm-l6-v2\",\n        \"top_k\": 15,\n        \"techniques\": [\"vector_retrieval\"]\n    },\n]\n\nSIGNIFICANCE_THRESHOLD = 0.05  # Statistical significance p-value threshold"
  },
  {
   "cell_type": "markdown",
   "id": "fb9733ae",
   "metadata": {},
   "source": "## Run Baseline\n\nSimple vector-only retrieval as reference point."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c1891",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# UTILITY FUNCTIONS: Copy from foundation/00-registry-and-tracking-utilities\n# ============================================================================\n\ndef compute_config_hash(config_dict: Dict) -> str:\n    \"\"\"Create deterministic SHA256 hash of a configuration.\"\"\"\n    config_str = json.dumps(config_dict, sort_keys=True)\n    hash_obj = __import__('hashlib').sha256(config_str.encode())\n    return hash_obj.hexdigest()[:12]\n\n\ndef start_experiment(db_connection, experiment_name: str, \n                     notebook_path: str = None,\n                     embedding_model_alias: str = None,\n                     config: Dict = None,\n                     techniques: List[str] = None,\n                     notes: str = None) -> int:\n    \"\"\"Start a new experiment and return its ID for tracking.\"\"\"\n    if config is None:\n        config = {}\n    if techniques is None:\n        techniques = []\n    \n    config_hash = compute_config_hash(config)\n    \n    with db_connection.cursor() as cur:\n        cur.execute('''\n            INSERT INTO experiments (\n                experiment_name, notebook_path, embedding_model_alias,\n                config_hash, config_json, techniques_applied, notes, status\n            )\n            VALUES (%s, %s, %s, %s, %s, %s, %s, 'running')\n            RETURNING id\n        ''', (\n            experiment_name,\n            notebook_path,\n            embedding_model_alias,\n            config_hash,\n            json.dumps(config),\n            techniques,\n            notes\n        ))\n        exp_id = cur.fetchone()[0]\n    db_connection.commit()\n    return exp_id\n\n\ndef complete_experiment(db_connection, experiment_id: int, \n                       status: str = 'completed',\n                       notes: str = None) -> bool:\n    \"\"\"Mark an experiment as complete.\"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            if notes:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, notes = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, notes, experiment_id))\n            else:\n                cur.execute('''\n                    UPDATE experiments\n                    SET status = %s, completed_at = CURRENT_TIMESTAMP\n                    WHERE id = %s\n                ''', (status, experiment_id))\n        db_connection.commit()\n        return True\n    except Exception as e:\n        db_connection.rollback()\n        print(f\"✗ Failed to complete experiment: {e}\")\n        return False\n\n\ndef save_metrics(db_connection, experiment_id: int, metrics_dict: Dict) -> Tuple[bool, str]:\n    \"\"\"Save experiment metrics to database.\"\"\"\n    try:\n        with db_connection.cursor() as cur:\n            for metric_name, metric_value in metrics_dict.items():\n                if isinstance(metric_value, dict):\n                    metric_val = metric_value.get('value', 0.0)\n                    metric_details = metric_value.get('details', {})\n                else:\n                    metric_val = metric_value\n                    metric_details = {}\n                \n                cur.execute('''\n                    INSERT INTO evaluation_results (\n                        experiment_id, metric_name, metric_value, metric_details_json\n                    )\n                    VALUES (%s, %s, %s, %s)\n                ''', (\n                    experiment_id,\n                    metric_name,\n                    float(metric_val),\n                    json.dumps(metric_details) if metric_details else '{}'\n                ))\n        db_connection.commit()\n        msg = f\"✓ Saved {len(metrics_dict)} metrics for experiment #{experiment_id}\"\n        return True, msg\n    except Exception as e:\n        db_connection.rollback()\n        msg = f\"✗ Failed to save metrics: {e}\"\n        return False, msg\n\n\n# ============================================================================\n# RETRIEVAL METRICS (from evaluation-lab/02)\n# ============================================================================\n\ndef precision_at_k(retrieved_chunk_ids: List[int], \n                   relevant_chunk_ids: List[int], \n                   k: int = 5) -> float:\n    \"\"\"Precision@K: What percentage of top-K results are relevant?\"\"\"\n    if k == 0:\n        return 0.0\n    \n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n    \n    num_relevant_in_k = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n    \n    return num_relevant_in_k / k\n\n\ndef recall_at_k(retrieved_chunk_ids: List[int], \n                relevant_chunk_ids: List[int], \n                k: int = 5) -> float:\n    \"\"\"Recall@K: What percentage of all relevant chunks were found in top-K?\"\"\"\n    if len(relevant_chunk_ids) == 0:\n        return 0.0\n    \n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n    \n    num_relevant_found = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_set)\n    \n    return num_relevant_found / len(relevant_set)\n\n\ndef mean_reciprocal_rank(retrieved_chunk_ids: List[int], \n                         relevant_chunk_ids: List[int]) -> float:\n    \"\"\"MRR: How quickly do we find the first relevant result?\"\"\"\n    relevant_set = set(relevant_chunk_ids)\n    \n    for rank, chunk_id in enumerate(retrieved_chunk_ids, start=1):\n        if chunk_id in relevant_set:\n            return 1.0 / rank\n    \n    return 0.0\n\n\ndef ndcg_at_k(retrieved_chunk_ids: List[int], \n              relevant_chunk_ids: List[int], \n              k: int = 5) -> float:\n    \"\"\"NDCG@K: Normalized Discounted Cumulative Gain\"\"\"\n    \n    def dcg_score(relevance_scores: List[float]) -> float:\n        \"\"\"Compute DCG from relevance scores.\"\"\"\n        return sum(\n            (2**rel - 1) / math.log2(rank + 2)\n            for rank, rel in enumerate(relevance_scores)\n        )\n    \n    if k == 0 or len(relevant_chunk_ids) == 0:\n        return 0.0\n    \n    retrieved_k = retrieved_chunk_ids[:k]\n    relevant_set = set(relevant_chunk_ids)\n    \n    relevance = [1 if chunk_id in relevant_set else 0 for chunk_id in retrieved_k]\n    \n    dcg = dcg_score(relevance)\n    ideal_relevance = sorted(relevance, reverse=True)\n    idcg = dcg_score(ideal_relevance)\n    \n    if idcg == 0:\n        return 0.0\n    \n    return dcg / idcg\n\n\ndef evaluate_rag_results(ground_truth_questions: List[Dict], \n                        rag_results: List[Dict], \n                        k_values: List[int] = [1, 3, 5, 10]) -> Dict:\n    \"\"\"Compute all metrics for RAG results against ground truth.\"\"\"\n    gt_map = {q['question']: q['relevant_chunk_ids'] for q in ground_truth_questions}\n    \n    metrics = {f'precision@{k}': [] for k in k_values}\n    metrics.update({f'recall@{k}': [] for k in k_values})\n    metrics.update({f'ndcg@{k}': [] for k in k_values})\n    metrics['mrr'] = []\n    \n    per_query_metrics = []\n    \n    for result in rag_results:\n        question = result['question']\n        retrieved = result['retrieved_chunk_ids']\n        \n        if question not in gt_map:\n            continue\n        \n        relevant = gt_map[question]\n        \n        query_metrics = {'question': question}\n        \n        for k in k_values:\n            p = precision_at_k(retrieved, relevant, k)\n            r = recall_at_k(retrieved, relevant, k)\n            n = ndcg_at_k(retrieved, relevant, k)\n            \n            metrics[f'precision@{k}'].append(p)\n            metrics[f'recall@{k}'].append(r)\n            metrics[f'ndcg@{k}'].append(n)\n            \n            query_metrics[f'precision@{k}'] = p\n            query_metrics[f'recall@{k}'] = r\n            query_metrics[f'ndcg@{k}'] = n\n        \n        mrr = mean_reciprocal_rank(retrieved, relevant)\n        metrics['mrr'].append(mrr)\n        query_metrics['mrr'] = mrr\n        \n        per_query_metrics.append(query_metrics)\n    \n    aggregated = {}\n    for key, values in metrics.items():\n        if len(values) > 0:\n            aggregated[key] = np.mean(values)\n        else:\n            aggregated[key] = 0.0\n    \n    aggregated['per_query'] = per_query_metrics\n    aggregated['num_queries'] = len(per_query_metrics)\n    \n    return aggregated\n\n\nprint(\"✓ Utility functions loaded\")"
  },
  {
   "cell_type": "markdown",
   "id": "66f88f8b",
   "metadata": {},
   "source": "## Run Configuration Variations\n\nRun all configured variants with full metrics computation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0638503",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PHASE 1: LOAD GROUND TRUTH TEST SET\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 1: LOADING GROUND TRUTH TEST SET\")\nprint(\"=\"*70)\n\nground_truth_questions = []\n\nwith db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n    cur.execute('''\n        SELECT \n            id,\n            question,\n            relevant_chunk_ids,\n            quality_rating,\n            source_type\n        FROM evaluation_groundtruth\n        WHERE quality_rating = 'good'\n        ORDER BY id\n    ''')\n    \n    for row in cur.fetchall():\n        ground_truth_questions.append({\n            'id': row['id'],\n            'question': row['question'],\n            'relevant_chunk_ids': row['relevant_chunk_ids'],\n            'quality_rating': row['quality_rating'],\n            'source_type': row['source_type']\n        })\n\nprint(f\"\\nLoaded {len(ground_truth_questions)} ground truth questions\")\nif ground_truth_questions:\n    print(f\"Sample: {ground_truth_questions[0]['question'][:80]}...\")\n    print(f\"Relevant chunks: {ground_truth_questions[0]['relevant_chunk_ids'][:3]}\")\nelse:\n    print(\"WARNING: No ground truth questions found. Run evaluation-lab/01 first.\")\n\n\n# ============================================================================\n# PHASE 2: RETRIEVE RESULTS FOR EACH CONFIGURATION\n# ============================================================================\n\ndef retrieve_results_for_questions(db_connection, questions: List[Dict], \n                                   embedding_model_alias: str = \"all-minilm-l6-v2\",\n                                   top_k: int = 10) -> List[Dict]:\n    \"\"\"Retrieve top-K results for each test question using vector similarity.\"\"\"\n    from sentence_transformers import SentenceTransformer\n    \n    model = SentenceTransformer(embedding_model_alias)\n    \n    rag_results = []\n    question_texts = [q['question'] for q in questions]\n    question_embeddings = model.encode(question_texts)\n    \n    with db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n        for i, question_dict in enumerate(questions):\n            question = question_dict['question']\n            question_embedding = question_embeddings[i]\n            \n            cur.execute('''\n                SELECT \n                    id,\n                    content,\n                    embedding\n                FROM chunks\n                WHERE embedding_model = %s\n                ORDER BY embedding <-> %s\n                LIMIT %s\n            ''', (embedding_model_alias, question_embedding, top_k))\n            \n            retrieved_chunks = cur.fetchall()\n            retrieved_ids = [chunk['id'] for chunk in retrieved_chunks]\n            \n            rag_results.append({\n                'question': question,\n                'retrieved_chunk_ids': retrieved_ids\n            })\n    \n    return rag_results\n\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 2: RUNNING CONFIGURATIONS\")\nprint(\"=\"*70)\n\n# Run each configuration\nall_config_results = {}\n\nfor config_def in CONFIGURATIONS:\n    config_name = config_def['name']\n    \n    print(f\"\\n[{config_name}] {config_def['description']}\")\n    print(\"-\" * 70)\n    \n    # Start experiment\n    exp_id = start_experiment(\n        db_connection,\n        experiment_name=config_name,\n        notebook_path='evaluation-lab/03-baseline-and-comparison.ipynb',\n        embedding_model_alias=config_def['embedding_model'],\n        config=config_def,\n        techniques=config_def['techniques'],\n        notes=config_def['description']\n    )\n    \n    print(f\"  Experiment ID: #{exp_id}\")\n    \n    try:\n        # Retrieve results\n        top_k_retrieve = config_def['top_k']\n        print(f\"  Retrieving top {top_k_retrieve} results...\")\n        \n        rag_results = retrieve_results_for_questions(\n            db_connection,\n            ground_truth_questions,\n            embedding_model_alias=config_def['embedding_model'],\n            top_k=top_k_retrieve\n        )\n        \n        # Compute metrics\n        print(f\"  Computing metrics...\")\n        metrics = evaluate_rag_results(\n            ground_truth_questions,\n            rag_results,\n            k_values=[1, 3, 5, 10]\n        )\n        \n        # Store metrics\n        success, msg = save_metrics(db_connection, exp_id, metrics)\n        if success:\n            print(f\"  {msg}\")\n        else:\n            print(f\"  ERROR: {msg}\")\n        \n        # Complete experiment\n        complete_experiment(db_connection, exp_id, status='completed')\n        \n        # Store results for comparison\n        all_config_results[config_name] = {\n            'experiment_id': exp_id,\n            'config': config_def,\n            'metrics': metrics\n        }\n        \n        # Print summary\n        print(f\"  Precision@5: {metrics.get('precision@5', 0):.4f}\")\n        print(f\"  Recall@5:    {metrics.get('recall@5', 0):.4f}\")\n        print(f\"  NDCG@5:      {metrics.get('ndcg@5', 0):.4f}\")\n        print(f\"  MRR:         {metrics.get('mrr', 0):.4f}\")\n        print(f\"  Queries:     {metrics.get('num_queries', 0)}\")\n        \n    except Exception as e:\n        print(f\"  ERROR: Failed to run configuration: {e}\")\n        complete_experiment(db_connection, exp_id, status='failed', notes=str(e))\n\nprint(\"\\n✓ All configurations completed\")"
  },
  {
   "cell_type": "markdown",
   "id": "329b89a5",
   "metadata": {},
   "source": "## Compare Results\n\nSide-by-side comparison of all configurations with improvement percentages."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2dfcf",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PHASE 3: COMPARE RESULTS SIDE-BY-SIDE\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 3: COMPARING CONFIGURATIONS\")\nprint(\"=\"*70)\n\nbaseline_name = CONFIGURATIONS[0]['name']\nbaseline_metrics = all_config_results[baseline_name]['metrics']\n\nprint(f\"\\nBaseline: {baseline_name}\")\nprint(f\"Variants: {', '.join([c['name'] for c in CONFIGURATIONS[1:]])}\")\n\n# Create comparison table\ncomparisons = []\n\nfor config_name, data in all_config_results.items():\n    config_metrics = data['metrics']\n    \n    # Add baseline row\n    if config_name == baseline_name:\n        for metric in ['precision@5', 'recall@5', 'mrr', 'ndcg@5']:\n            comparisons.append({\n                'configuration': config_name,\n                'metric': metric,\n                'value': config_metrics.get(metric, 0.0),\n                'improvement_%': 0.0,\n                'is_baseline': True\n            })\n    else:\n        # Compare against baseline\n        for metric in ['precision@5', 'recall@5', 'mrr', 'ndcg@5']:\n            baseline_val = baseline_metrics.get(metric, 0.0)\n            variant_val = config_metrics.get(metric, 0.0)\n            \n            if baseline_val > 0:\n                improvement = ((variant_val - baseline_val) / baseline_val) * 100\n            else:\n                improvement = 0.0\n            \n            comparisons.append({\n                'configuration': config_name,\n                'metric': metric,\n                'value': variant_val,\n                'baseline': baseline_val,\n                'improvement_%': improvement,\n                'is_baseline': False\n            })\n\ncomparison_df = pd.DataFrame(comparisons)\n\n# Display comparison table\nprint(\"\\nMetrics Comparison Table:\")\nprint(\"-\" * 100)\n\nfor metric in ['precision@5', 'recall@5', 'mrr', 'ndcg@5']:\n    print(f\"\\n{metric.upper()}\")\n    print(\"-\" * 100)\n    print(f\"{'Configuration':<30} {'Value':<15} {'Baseline':<15} {'Improvement':<15}\")\n    print(\"-\" * 100)\n    \n    metric_data = comparison_df[comparison_df['metric'] == metric]\n    \n    for _, row in metric_data.iterrows():\n        if row['is_baseline']:\n            print(f\"{row['configuration']:<30} {row['value']:<15.4f}\")\n        else:\n            baseline = row.get('baseline', 0.0)\n            improvement = row['improvement_%']\n            marker = '↑' if improvement > 0 else '↓' if improvement < 0 else '='\n            print(f\"{row['configuration']:<30} {row['value']:<15.4f} {baseline:<15.4f} {marker} {improvement:+.2f}%\")\n\nprint(\"\\n✓ Comparison complete\")"
  },
  {
   "cell_type": "markdown",
   "id": "081540b7",
   "metadata": {},
   "source": "## Statistical Significance\n\nTest whether improvements are statistically significant using paired t-tests."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0795ff",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PHASE 4: STATISTICAL SIGNIFICANCE TESTING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 4: STATISTICAL SIGNIFICANCE TESTING\")\nprint(\"=\"*70)\n\ndef paired_t_test(baseline_results, variant_results, metric_name='precision@5'):\n    \"\"\"\n    Perform paired t-test to check if improvement is statistically significant.\n    \n    Args:\n        baseline_results: List of per-query metrics\n        variant_results: List of per-query metrics\n        metric_name: Which metric to test\n        \n    Returns:\n        dict with t_statistic, p_value, significant (p < 0.05), effect_size\n    \"\"\"\n    baseline_values = [r[metric_name] for r in baseline_results]\n    variant_values = [r[metric_name] for r in variant_results]\n    \n    if len(baseline_values) < 2:\n        return {\n            't_statistic': 0.0,\n            'p_value': 1.0,\n            'significant': False,\n            'effect_size': 0.0,\n            'n_queries': len(baseline_values)\n        }\n    \n    t_stat, p_value = stats.ttest_rel(variant_values, baseline_values)\n    \n    effect_size = (np.mean(variant_values) - np.mean(baseline_values)) / np.std(baseline_values) if np.std(baseline_values) > 0 else 0\n    \n    return {\n        't_statistic': t_stat,\n        'p_value': p_value,\n        'significant': p_value < SIGNIFICANCE_THRESHOLD,\n        'effect_size': effect_size,\n        'n_queries': len(baseline_values),\n        'baseline_mean': np.mean(baseline_values),\n        'variant_mean': np.mean(variant_values)\n    }\n\n\n# Run statistical tests for each variant\nprint(f\"\\nSignificance threshold: p < {SIGNIFICANCE_THRESHOLD}\")\nprint(\"\\nStatistical Significance Tests (Paired t-test):\")\nprint(\"-\" * 110)\n\nsignificance_results = []\n\nbaseline_per_query = baseline_metrics['per_query']\n\nfor config_name, data in all_config_results.items():\n    if config_name == baseline_name:\n        continue\n    \n    config_metrics = data['metrics']\n    variant_per_query = config_metrics['per_query']\n    \n    print(f\"\\n{config_name}:\")\n    print(f\"  {'Metric':<15} {'t-stat':<12} {'p-value':<12} {'Significant':<12} {'Effect Size':<12} {'N Queries':<10}\")\n    print(\"  \" + \"-\" * 100)\n    \n    for metric in ['precision@5', 'recall@5', 'mrr', 'ndcg@5']:\n        result = paired_t_test(baseline_per_query, variant_per_query, metric)\n        \n        sig_marker = \"YES *\" if result['significant'] else \"NO\"\n        print(f\"  {metric:<15} {result['t_statistic']:<12.4f} {result['p_value']:<12.4f} {sig_marker:<12} {result['effect_size']:<12.4f} {result['n_queries']:<10}\")\n        \n        significance_results.append({\n            'configuration': config_name,\n            'metric': metric,\n            't_statistic': result['t_statistic'],\n            'p_value': result['p_value'],\n            'significant': result['significant'],\n            'effect_size': result['effect_size'],\n            'baseline_mean': result['baseline_mean'],\n            'variant_mean': result['variant_mean'],\n            'n_queries': result['n_queries']\n        })\n\nsignificance_df = pd.DataFrame(significance_results)\n\n# Summary of significant improvements\nprint(\"\\n\" + \"=\"*110)\nprint(\"SUMMARY: Statistically Significant Improvements (p < 0.05)\")\nprint(\"=\"*110)\n\nsignificant_improvements = significance_df[significance_df['significant']]\n\nif len(significant_improvements) > 0:\n    for _, row in significant_improvements.iterrows():\n        improvement = ((row['variant_mean'] - row['baseline_mean']) / row['baseline_mean'] * 100) if row['baseline_mean'] > 0 else 0\n        print(f\"\\n{row['configuration']} - {row['metric']}:\")\n        print(f\"  Baseline: {row['baseline_mean']:.4f}\")\n        print(f\"  Variant:  {row['variant_mean']:.4f}\")\n        print(f\"  Improvement: {improvement:+.2f}%\")\n        print(f\"  t-statistic: {row['t_statistic']:.4f}\")\n        print(f\"  p-value: {row['p_value']:.4f}\")\n        print(f\"  Effect size (Cohen's d): {row['effect_size']:.4f}\")\nelse:\n    print(\"\\nNo statistically significant improvements found (p < 0.05)\")\n    print(\"Note: This doesn't mean configurations are identical - consider increasing sample size or lowering threshold.\")\n\nprint(\"\\n✓ Statistical significance testing complete\")"
  },
  {
   "cell_type": "markdown",
   "id": "fcf26f05",
   "metadata": {},
   "source": "## Visualize Comparison\n\nCreate visualizations showing metric improvements and distributions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc143887",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PHASE 5: VISUALIZE COMPARISON\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 5: VISUALIZATION\")\nprint(\"=\"*70)\n\ndef visualize_configuration_comparison(comparison_df, baseline_name):\n    \"\"\"Create bar charts showing improvements across configurations.\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    fig.suptitle('Configuration Comparison: Improvement vs Baseline', fontsize=16, fontweight='bold')\n    \n    metrics = ['precision@5', 'recall@5', 'mrr', 'ndcg@5']\n    \n    for idx, metric in enumerate(metrics):\n        ax = axes[idx // 2, idx % 2]\n        \n        subset = comparison_df[comparison_df['metric'] == metric]\n        baseline_val = subset[subset['configuration'] == baseline_name]['value'].values[0] if baseline_name in subset['configuration'].values else 0\n        \n        variant_data = subset[subset['configuration'] != baseline_name]\n        \n        if len(variant_data) > 0:\n            configs = variant_data['configuration'].values\n            improvements = variant_data['improvement_%'].values\n            values = variant_data['value'].values\n            \n            colors = ['green' if imp > 0 else 'red' if imp < 0 else 'gray' for imp in improvements]\n            \n            bars = ax.bar(range(len(configs)), improvements, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n            \n            # Add value labels on bars\n            for bar, val, imp in zip(bars, values, improvements):\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height,\n                       f'{imp:+.1f}%\\n({val:.3f})', ha='center', va='bottom' if height > 0 else 'top', fontsize=9)\n            \n            ax.set_xticks(range(len(configs)))\n            ax.set_xticklabels(configs, rotation=45, ha='right')\n            ax.set_ylabel('Improvement %', fontweight='bold')\n            ax.set_title(f'{metric.upper()} (Baseline: {baseline_val:.4f})', fontweight='bold')\n            ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n            ax.grid(True, alpha=0.3, axis='y')\n        \n    plt.tight_layout()\n    plt.show()\n    print(\"✓ Comparison chart generated\")\n\n\nvisualize_configuration_comparison(comparison_df, baseline_name)\n\n\n# Distribution plots\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nfig.suptitle('Per-Query Metric Distributions', fontsize=14, fontweight='bold')\n\n# Precision@5 distribution\nax = axes[0]\nfor config_name, data in all_config_results.items():\n    per_query = data['metrics']['per_query']\n    precision_values = [q['precision@5'] for q in per_query]\n    ax.hist(precision_values, alpha=0.5, label=config_name, bins=10, edgecolor='black')\n\nax.set_xlabel('Precision@5', fontweight='bold')\nax.set_ylabel('Frequency', fontweight='bold')\nax.set_title('Distribution of Precision@5 Across Queries', fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Recall@5 distribution\nax = axes[1]\nfor config_name, data in all_config_results.items():\n    per_query = data['metrics']['per_query']\n    recall_values = [q['recall@5'] for q in per_query]\n    ax.hist(recall_values, alpha=0.5, label=config_name, bins=10, edgecolor='black')\n\nax.set_xlabel('Recall@5', fontweight='bold')\nax.set_ylabel('Frequency', fontweight='bold')\nax.set_title('Distribution of Recall@5 Across Queries', fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\nprint(\"✓ Distribution charts generated\")\n\n\n# Metric comparison heatmap\nfig, ax = plt.subplots(figsize=(10, 6))\n\nmetrics_list = ['precision@1', 'precision@3', 'precision@5', 'precision@10',\n                'recall@1', 'recall@3', 'recall@5', 'recall@10',\n                'ndcg@1', 'ndcg@3', 'ndcg@5', 'ndcg@10', 'mrr']\n\nheatmap_data = []\nconfig_names = []\n\nfor config_name, data in all_config_results.items():\n    config_names.append(config_name)\n    row = []\n    for metric in metrics_list:\n        value = data['metrics'].get(metric, 0.0)\n        row.append(value)\n    heatmap_data.append(row)\n\nheatmap_array = np.array(heatmap_data)\n\nim = ax.imshow(heatmap_array, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n\nax.set_xticks(range(len(metrics_list)))\nax.set_xticklabels(metrics_list, rotation=45, ha='right', fontsize=9)\nax.set_yticks(range(len(config_names)))\nax.set_yticklabels(config_names)\n\nax.set_title('Metrics Heatmap Across Configurations', fontweight='bold', pad=20)\n\n# Add values to heatmap\nfor i in range(len(config_names)):\n    for j in range(len(metrics_list)):\n        text = ax.text(j, i, f'{heatmap_array[i, j]:.2f}',\n                      ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n\ncbar = plt.colorbar(im, ax=ax)\ncbar.set_label('Score', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\nprint(\"✓ Heatmap generated\")"
  },
  {
   "cell_type": "markdown",
   "id": "4f21d6d3",
   "metadata": {},
   "source": "## Recommendation\n\nGenerate production recommendation based on statistical significance and trade-off analysis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3b0e1",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PHASE 6: RECOMMENDATION\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 6: RECOMMENDATION\")\nprint(\"=\"*70)\n\ndef generate_recommendation(comparison_df, significance_df, all_config_results, baseline_name):\n    \"\"\"\n    Analyze results and provide actionable recommendation.\n    \n    Returns:\n        str: Recommendation report\n    \"\"\"\n    report = []\n    report.append(\"\\n\" + \"=\"*80)\n    report.append(\"CONFIGURATION RECOMMENDATION REPORT\")\n    report.append(\"=\"*80)\n    report.append(f\"\\nBaseline: {baseline_name}\")\n    report.append(f\"Variants: {', '.join([c for c in all_config_results.keys() if c != baseline_name])}\")\n    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    report.append(f\"Significance Threshold: p < {SIGNIFICANCE_THRESHOLD}\")\n    \n    report.append(\"\\n\" + \"-\"*80)\n    report.append(\"BEST CONFIGURATIONS BY METRIC\")\n    report.append(\"-\"*80)\n    \n    # Find best configuration per metric\n    best_configs = {}\n    for metric in ['precision@5', 'recall@5', 'mrr', 'ndcg@5']:\n        subset = comparison_df[comparison_df['metric'] == metric]\n        \n        # Find best absolute value\n        best_idx = subset['value'].idxmax()\n        best_row = subset.loc[best_idx]\n        \n        # Check if significant\n        sig_data = significance_df[\n            (significance_df['configuration'] == best_row['configuration']) & \n            (significance_df['metric'] == metric)\n        ]\n        \n        is_significant = False\n        if len(sig_data) > 0:\n            is_significant = sig_data['significant'].values[0]\n        elif best_row['configuration'] == baseline_name:\n            is_significant = True  # Baseline is inherently significant\n        \n        best_configs[metric] = {\n            'config': best_row['configuration'],\n            'value': best_row['value'],\n            'improvement': best_row['improvement_%'],\n            'significant': is_significant\n        }\n        \n        sig_marker = \"*\" if is_significant else \"\"\n        report.append(f\"\\n{metric}:\")\n        report.append(f\"  Best: {best_row['configuration']} (value: {best_row['value']:.4f}){sig_marker}\")\n        if best_row['configuration'] != baseline_name:\n            report.append(f\"  Improvement vs baseline: {best_row['improvement_%']:+.2f}%\")\n    \n    # Overall recommendation\n    report.append(\"\\n\" + \"-\"*80)\n    report.append(\"PRODUCTION RECOMMENDATION\")\n    report.append(\"-\"*80)\n    \n    # Count wins per configuration\n    config_wins = {}\n    config_sig_wins = {}\n    \n    for metric, data in best_configs.items():\n        config = data['config']\n        config_wins[config] = config_wins.get(config, 0) + 1\n        \n        if data['significant']:\n            config_sig_wins[config] = config_sig_wins.get(config, 0) + 1\n    \n    if len(config_wins) == 0:\n        report.append(\"\\nERROR: No configurations to recommend\")\n        return \"\\n\".join(report)\n    \n    winner = max(config_wins, key=config_wins.get)\n    winner_wins = config_wins[winner]\n    winner_sig_wins = config_sig_wins.get(winner, 0)\n    \n    report.append(f\"\\nRecommended Configuration: {winner}\")\n    report.append(f\"  Wins best in {winner_wins} out of 4 metrics\")\n    report.append(f\"  Statistically significant wins: {winner_sig_wins} out of {winner_wins}\")\n    \n    if winner != baseline_name:\n        # Compute average improvement\n        improvements = []\n        for metric in ['precision@5', 'recall@5', 'mrr', 'ndcg@5']:\n            subset = comparison_df[\n                (comparison_df['configuration'] == winner) & \n                (comparison_df['metric'] == metric)\n            ]\n            if len(subset) > 0:\n                improvements.append(subset['improvement_%'].values[0])\n        \n        avg_improvement = np.mean(improvements) if improvements else 0\n        report.append(f\"  Average improvement across all metrics: {avg_improvement:+.2f}%\")\n    \n    # Trade-offs analysis\n    report.append(\"\\n\" + \"-\"*80)\n    report.append(\"TRADE-OFF ANALYSIS\")\n    report.append(\"-\"*80)\n    \n    for config_name in all_config_results.keys():\n        if config_name == baseline_name:\n            continue\n        \n        config_data = all_config_results[config_name]\n        \n        report.append(f\"\\n{config_name}:\")\n        \n        # Compute average performance\n        metrics_to_check = ['precision@5', 'recall@5', 'mrr', 'ndcg@5']\n        avg_value = np.mean([config_data['metrics'].get(m, 0) for m in metrics_to_check])\n        baseline_avg = np.mean([all_config_results[baseline_name]['metrics'].get(m, 0) for m in metrics_to_check])\n        \n        avg_improvement = ((avg_value - baseline_avg) / baseline_avg * 100) if baseline_avg > 0 else 0\n        \n        report.append(f\"  Average metric value: {avg_value:.4f} ({avg_improvement:+.2f}% vs baseline)\")\n        report.append(f\"  Configuration details: {config_data['config']['techniques']}\")\n    \n    # Caveats and assumptions\n    report.append(\"\\n\" + \"-\"*80)\n    report.append(\"CAVEATS & ASSUMPTIONS\")\n    report.append(\"-\"*80)\n    \n    n_queries = all_config_results[baseline_name]['metrics'].get('num_queries', 0)\n    report.append(f\"\\n1. Sample Size: {n_queries} test queries\")\n    if n_queries < 20:\n        report.append(\"   WARNING: Small sample size. Consider collecting more ground truth for reliable results.\")\n    elif n_queries < 50:\n        report.append(\"   Note: Sample size is moderate. Results may vary with larger datasets.\")\n    \n    report.append(f\"\\n2. Metric Coverage: Evaluation based on\")\n    report.append(\"   - Precision@5, Recall@5 (retrieval quality)\")\n    report.append(\"   - NDCG@5 (ranking quality)\")\n    report.append(\"   - MRR (user satisfaction)\")\n    \n    report.append(f\"\\n3. Statistical Test: Paired t-test at p < {SIGNIFICANCE_THRESHOLD}\")\n    report.append(\"   - Appropriate for comparing same set of queries\")\n    report.append(\"   - Assumes approximately normal distribution\")\n    \n    report.append(f\"\\n4. Configuration Context:\")\n    report.append(f\"   - Embedding model: all-minilm-l6-v2\")\n    report.append(f\"   - Ground truth created: evaluation-lab/01\")\n    report.append(f\"   - Note: Results are specific to this test set and may differ on other domains\")\n    \n    # Next steps\n    report.append(\"\\n\" + \"-\"*80)\n    report.append(\"NEXT STEPS\")\n    report.append(\"-\"*80)\n    \n    if len(significance_df[significance_df['significant']]) > 0:\n        report.append(f\"\\n1. VALIDATE: Run recommended configuration on holdout test set\")\n        report.append(f\"2. DEPLOY: If results hold, deploy {winner} to production\")\n        report.append(f\"3. MONITOR: Track metrics in production to detect drift\")\n    else:\n        report.append(f\"\\n1. COLLECT MORE DATA: Current sample size may be too small\")\n        report.append(f\"2. ADJUST THRESHOLD: Consider lowering significance threshold if practical difference exists\")\n        report.append(f\"3. ANALYZE FAILURE CASES: Look at worst-performing queries for insights\")\n    \n    report.append(f\"\\n4. EXPLORE ADVANCED TECHNIQUES:\")\n    report.append(f\"   - Reranking with cross-encoders\")\n    report.append(f\"   - Query expansion with LLM\")\n    report.append(f\"   - Hybrid search combining multiple modalities\")\n    report.append(f\"   See advanced-techniques/ notebooks for implementations\")\n    \n    report.append(\"\\n\" + \"=\"*80 + \"\\n\")\n    \n    return \"\\n\".join(report)\n\n\nrecommendation_report = generate_recommendation(comparison_df, significance_df, all_config_results, baseline_name)\nprint(recommendation_report)\n\n# Save recommendation to file\nimport os\nos.makedirs('data/experiment_results', exist_ok=True)\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nreport_path = f\"data/experiment_results/baseline_comparison_{timestamp}.txt\"\nwith open(report_path, 'w') as f:\n    f.write(recommendation_report)\nprint(f\"✓ Report saved to: {report_path}\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}