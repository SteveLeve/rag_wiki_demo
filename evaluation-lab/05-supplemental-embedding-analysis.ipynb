{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e839fb4d",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import psycopg2\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean, stdev\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL connection\n",
    "POSTGRES_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'rag_db',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "}\n",
    "\n",
    "# Models\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "EMBEDDING_MODEL_ALIAS = 'bge_base_en_v1.5'\n",
    "\n",
    "# Embedding table name\n",
    "TABLE_NAME = f'embeddings_{EMBEDDING_MODEL_ALIAS.replace(\".\", \"_\")}'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  PostgreSQL: {POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}/{POSTGRES_CONFIG['database']}\")\n",
    "print(f\"  Embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"  Table: {TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5abb09",
   "metadata": {},
   "source": [
    "## Helper Classes for Embedding Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1075086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostgreSQLVectorDB:\n",
    "    \"\"\"Helper class to manage embeddings in PostgreSQL with pgvector.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, table_name):\n",
    "        self.config = config\n",
    "        self.table_name = table_name\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "    \n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(\n",
    "                host=self.config['host'],\n",
    "                port=self.config['port'],\n",
    "                database=self.config['database'],\n",
    "                user=self.config['user'],\n",
    "                password=self.config['password']\n",
    "            )\n",
    "            print(f'✓ Connected to PostgreSQL at {self.config[\"host\"]}:{self.config[\"port\"]}')\n",
    "        except psycopg2.OperationalError as e:\n",
    "            print(f'✗ Failed to connect to PostgreSQL: {e}')\n",
    "            raise\n",
    "    \n",
    "    def get_chunk_count(self):\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n",
    "            return cur.fetchone()[0]\n",
    "    \n",
    "    def similarity_search(self, query_embedding, top_n=3):\n",
    "        \"\"\"Find most similar chunks using pgvector.\"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                SELECT chunk_text, \n",
    "                       1 - (embedding <=> %s::vector) as similarity\n",
    "                FROM {self.table_name}\n",
    "                ORDER BY embedding <=> %s::vector\n",
    "                LIMIT %s\n",
    "            ''', (query_embedding, query_embedding, top_n))\n",
    "            \n",
    "            results = cur.fetchall()\n",
    "            return [(chunk, score) for chunk, score in results]\n",
    "    \n",
    "    def get_all_similarity_scores(self, query_embedding):\n",
    "        \"\"\"Get similarity scores for all chunks (for statistical analysis).\"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                SELECT 1 - (embedding <=> %s::vector) as similarity\n",
    "                FROM {self.table_name}\n",
    "                ORDER BY similarity DESC\n",
    "            ''', (query_embedding,))\n",
    "            \n",
    "            return [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "# Connect to database\n",
    "db = PostgreSQLVectorDB(POSTGRES_CONFIG, TABLE_NAME)\n",
    "count = db.get_chunk_count()\n",
    "print(f'\\n✓ Loaded {count} embeddings from database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d390e3b",
   "metadata": {},
   "source": [
    "## 1. Analyze Retrieval Quality by Query Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebab15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_retrieval_quality(queries_by_type, top_n=5):\n",
    "    \"\"\"Test different query types and analyze retrieval quality.\n",
    "    \n",
    "    Args:\n",
    "        queries_by_type: Dict mapping query type to list of queries\n",
    "        top_n: Number of results to retrieve per query\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with query type, query text, and top similarity scores\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for query_type, queries in queries_by_type.items():\n",
    "        print(f\"\\nAnalyzing {query_type} queries...\")\n",
    "        \n",
    "        type_results = {\n",
    "            'query_type': query_type,\n",
    "            'query_count': len(queries),\n",
    "            'avg_top1_similarity': [],\n",
    "            'avg_top5_similarity': [],\n",
    "            'avg_all_similarities': [],\n",
    "        }\n",
    "        \n",
    "        for query in queries:\n",
    "            query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "            \n",
    "            # Get top N results\n",
    "            top_results = db.similarity_search(query_embedding, top_n=top_n)\n",
    "            \n",
    "            # Get all similarity scores for statistical analysis\n",
    "            all_scores = db.get_all_similarity_scores(query_embedding)\n",
    "            \n",
    "            # Record metrics\n",
    "            if top_results:\n",
    "                top1_score = top_results[0][1]\n",
    "                top5_scores = [score for _, score in top_results[:5]]\n",
    "                \n",
    "                type_results['avg_top1_similarity'].append(top1_score)\n",
    "                type_results['avg_top5_similarity'].append(mean(top5_scores) if top5_scores else 0)\n",
    "                type_results['avg_all_similarities'].append(mean(all_scores))\n",
    "        \n",
    "        # Compute averages\n",
    "        results.append({\n",
    "            'Query Type': query_type,\n",
    "            'Test Count': len(queries),\n",
    "            'Avg Top-1 Similarity': mean(type_results['avg_top1_similarity']) if type_results['avg_top1_similarity'] else 0,\n",
    "            'Avg Top-5 Similarity': mean(type_results['avg_top5_similarity']) if type_results['avg_top5_similarity'] else 0,\n",
    "            'Avg All Similarities': mean(type_results['avg_all_similarities']) if type_results['avg_all_similarities'] else 0,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example: Test different query types\n",
    "test_queries = {\n",
    "    'Science': [\n",
    "        'What is photosynthesis?',\n",
    "        'How does DNA work?',\n",
    "        'What is quantum physics?',\n",
    "    ],\n",
    "    'Geography': [\n",
    "        'What is the capital of France?',\n",
    "        'Where is Mount Everest?',\n",
    "        'What country is Tokyo in?',\n",
    "    ],\n",
    "    'History': [\n",
    "        'When did World War 2 end?',\n",
    "        'Who was Napoleon?',\n",
    "        'What was the Renaissance?',\n",
    "    ],\n",
    "}\n",
    "\n",
    "quality_df = analyze_retrieval_quality(test_queries, top_n=5)\n",
    "print(\"\\n=== Retrieval Quality by Query Type ===\")\n",
    "print(quality_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252dad2",
   "metadata": {},
   "source": [
    "## 2. Debug Poor Retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_retrieval(query, top_n=10):\n",
    "    \"\"\"Detailed analysis of what gets retrieved for a query.\n",
    "    \n",
    "    Shows:\n",
    "    - Top N chunks with similarity scores\n",
    "    - Quality assessment (is top result actually relevant?)\n",
    "    - Distribution of similarity scores\n",
    "    \"\"\"\n",
    "    query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    results = db.similarity_search(query_embedding, top_n=top_n)\n",
    "    \n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    print(\"Top retrievals:\")\n",
    "    \n",
    "    for i, (chunk, score) in enumerate(results, 1):\n",
    "        # Extract article title\n",
    "        title = chunk.split('\\n')[0].replace('Article: ', '')\n",
    "        preview = chunk[:150].replace('\\n', ' ') + '...'\n",
    "        \n",
    "        print(f\"\\n  [{i}] Similarity: {score:.4f}\")\n",
    "        print(f\"      Article: {title}\")\n",
    "        print(f\"      Preview: {preview}\")\n",
    "    \n",
    "    # Statistics\n",
    "    scores = [score for _, score in results]\n",
    "    print(f\"\\n=== Similarity Statistics ===\")\n",
    "    print(f\"Max: {max(scores):.4f}\")\n",
    "    print(f\"Min: {min(scores):.4f}\")\n",
    "    print(f\"Mean: {mean(scores):.4f}\")\n",
    "    if len(scores) > 1:\n",
    "        print(f\"Stdev: {stdev(scores):.4f}\")\n",
    "\n",
    "# Example: Debug a specific query\n",
    "debug_retrieval('What is machine learning?', top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583e39b",
   "metadata": {},
   "source": [
    "## 3. Compare Embedding Models (if multiple registered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embedding_models(query, models_and_tables):\n",
    "    \"\"\"Compare retrieval results across different embedding models.\n",
    "    \n",
    "    Args:\n",
    "        query: Test query\n",
    "        models_and_tables: Dict mapping model name to table name\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame comparing results from each model\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name, table_name in models_and_tables.items():\n",
    "        try:\n",
    "            model_db = PostgreSQLVectorDB(POSTGRES_CONFIG, table_name)\n",
    "            \n",
    "            # For model comparison, use appropriate embedding endpoint\n",
    "            # Note: This is simplified - in reality you'd need the model's embedding API\n",
    "            query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "            \n",
    "            top_result = model_db.similarity_search(query_embedding, top_n=1)\n",
    "            \n",
    "            if top_result:\n",
    "                chunk, score = top_result[0]\n",
    "                title = chunk.split('\\n')[0].replace('Article: ', '')\n",
    "                \n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Top Similarity': score,\n",
    "                    'Top Article': title,\n",
    "                })\n",
    "            \n",
    "            model_db.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not compare with {model_name}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Model comparison is available if you have multiple embedding models registered.\")\n",
    "print(\"\\nTo generate embeddings with different models:\")\n",
    "print(\"  1. Run foundation/02-rag-postgresql-persistent.ipynb with different EMBEDDING_MODEL\")\n",
    "print(\"  2. Each model creates its own embeddings_* table\")\n",
    "print(\"  3. Use this function to compare retrieval quality across models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a866e3",
   "metadata": {},
   "source": [
    "## 4. Retrieval Performance Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_retrieval_speed(queries, num_runs=1):\n",
    "    \"\"\"Measure retrieval latency and identify bottlenecks.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of test queries\n",
    "        num_runs: Number of times to run each query (for averaging)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with latency metrics\n",
    "    \"\"\"\n",
    "    embedding_times = []\n",
    "    retrieval_times = []\n",
    "    \n",
    "    print(f\"Profiling {len(queries)} queries (averaging {num_runs} runs)...\\n\")\n",
    "    \n",
    "    for query in queries:\n",
    "        # Time embedding generation\n",
    "        start = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "        embedding_time = (time.time() - start) / num_runs * 1000  # ms\n",
    "        embedding_times.append(embedding_time)\n",
    "        \n",
    "        # Time similarity search\n",
    "        start = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            results = db.similarity_search(query_embedding, top_n=5)\n",
    "        retrieval_time = (time.time() - start) / num_runs * 1000  # ms\n",
    "        retrieval_times.append(retrieval_time)\n",
    "    \n",
    "    print(f\"=== Performance Profile ===\")\n",
    "    print(f\"\\nEmbedding Generation (BGE model):\")\n",
    "    print(f\"  Mean: {mean(embedding_times):.2f} ms\")\n",
    "    print(f\"  Min: {min(embedding_times):.2f} ms\")\n",
    "    print(f\"  Max: {max(embedding_times):.2f} ms\")\n",
    "    \n",
    "    print(f\"\\nRetrieval (PostgreSQL pgvector):\")\n",
    "    print(f\"  Mean: {mean(retrieval_times):.2f} ms\")\n",
    "    print(f\"  Min: {min(retrieval_times):.2f} ms\")\n",
    "    print(f\"  Max: {max(retrieval_times):.2f} ms\")\n",
    "    \n",
    "    print(f\"\\nTotal Per-Query Time: {mean(embedding_times) + mean(retrieval_times):.2f} ms\")\n",
    "    \n",
    "    return {\n",
    "        'embedding_mean_ms': mean(embedding_times),\n",
    "        'retrieval_mean_ms': mean(retrieval_times),\n",
    "        'total_mean_ms': mean(embedding_times) + mean(retrieval_times),\n",
    "    }\n",
    "\n",
    "# Profile with test queries\n",
    "profile_queries = [\n",
    "    'What is photosynthesis?',\n",
    "    'Who was Albert Einstein?',\n",
    "    'What is Python?',\n",
    "]\n",
    "\n",
    "profile_metrics = profile_retrieval_speed(profile_queries, num_runs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b6afc",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd62f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection when done\n",
    "db.close()\n",
    "print(\"✓ Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
