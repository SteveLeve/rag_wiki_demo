{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3de8aa6",
   "metadata": {},
   "source": [
    "# üöÄ Quick Start: RAG in 5 Minutes\n",
    "\n",
    "**Already have Ollama installed with the models?** Jump straight to the \"Load Dataset\" section and run cells sequentially.\n",
    "\n",
    "**First time?** Complete setup below (3 terminal commands), then run all cells.\n",
    "\n",
    "```bash\n",
    "# 1. Pull embedding model (creates vector representations)\n",
    "ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\n",
    "\n",
    "# 2. Pull language model (generates answers)\n",
    "ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n",
    "\n",
    "# 3. Install Python packages\n",
    "pip install ollama datasets jupyter\n",
    "```\n",
    "\n",
    "Then open this notebook and run all cells from top to bottom. No database setup needed!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4644e21",
   "metadata": {},
   "source": [
    "# Foundation 01: Basic RAG (In-Memory)\n",
    "\n",
    "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) system using Simple Wikipedia articles with **in-memory storage** and optional JSON file caching.\n",
    "\n",
    "This is the **simple version** with minimal dependencies - perfect for learning RAG fundamentals without database setup.\n",
    "\n",
    "**Ready for persistent storage?** See `foundation/02-rag-postgresql-persistent.ipynb` for the PostgreSQL version with durable embeddings and registry integration.\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "Before running this notebook, you need to:\n",
    "\n",
    "1. Install Ollama from [ollama.com](https://ollama.com/)\n",
    "2. Download the required models by running these commands in your terminal:\n",
    "\n",
    "```bash\n",
    "ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\n",
    "ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n",
    "```\n",
    "\n",
    "3. Install the required Python packages:\n",
    "\n",
    "```bash\n",
    "pip install ollama datasets jupyter\n",
    "```\n",
    "\n",
    "## Learning Progression\n",
    "\n",
    "- ‚úÖ **You are here:** foundation/01 - In-memory RAG basics\n",
    "- ‚è≠Ô∏è  **Next:** foundation/02 - PostgreSQL persistent storage with registry\n",
    "- üéØ **Path:** See `LEARNING_ROADMAP.md` for complete learning paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3198d7f",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e96af9",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the target dataset size. The script will download articles until it reaches approximately this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target dataset size in MB (adjust as needed: 10, 20, 30, 40, 50)\n",
    "TARGET_SIZE_MB = 10\n",
    "\n",
    "# Maximum chunk size in characters (for splitting long articles)\n",
    "MAX_CHUNK_SIZE = 1000\n",
    "\n",
    "# Whether to save the dataset locally for reuse\n",
    "SAVE_LOCALLY = True\n",
    "LOCAL_DATASET_PATH = f'wikipedia_dataset_{TARGET_SIZE_MB}mb.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_loading",
   "metadata": {},
   "source": [
    "## Load and Filter the Wikipedia Dataset\n",
    "\n",
    "We'll use Simple Wikipedia, which has cleaner, more concise articles. The dataset will be filtered to approximately your target size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_size_mb(text):\n",
    "    \"\"\"Estimate the size of text in megabytes.\"\"\"\n",
    "    return sys.getsizeof(text) / (1024 * 1024)\n",
    "\n",
    "def chunk_text(text, max_size=1000):\n",
    "    \"\"\"Split text into chunks of approximately max_size characters.\n",
    "    \n",
    "    Why chunking?\n",
    "    - Long documents don't fit in embedding models efficiently\n",
    "    - Smaller chunks retrieve more precise context\n",
    "    - Overlapping chunks preserve semantic continuity\n",
    "    \n",
    "    Tries to break at paragraph boundaries when possible.\n",
    "    \"\"\"\n",
    "    # EARLY EXIT: If text is already short enough, return as-is\n",
    "    # This avoids unnecessary processing and preserves the original text format\n",
    "    if len(text) <= max_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    # Split by double newlines to respect document structure (paragraphs are semantic units)\n",
    "    # This is key to the algorithm: paragraphs are natural boundaries in human-written text\n",
    "    # Example: \"Para1\\n\\nPara2\\n\\nPara3\" ‚Üí [\"Para1\", \"Para2\", \"Para3\"]\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    current_chunk = ''\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # ALGORITHM STEP 1: Check if adding this paragraph would exceed the limit\n",
    "        # We check BEFORE adding to ensure no chunk exceeds max_size\n",
    "        # Pattern: accumulate paragraphs until next one would overflow\n",
    "        if len(current_chunk) + len(paragraph) > max_size:\n",
    "            # BOUNDARY DETECTION: Current chunk is \"full\", save it and start fresh\n",
    "            if current_chunk:  # Only save if not empty (avoid empty chunks at boundaries)\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = ''\n",
    "            \n",
    "            # OVERFLOW HANDLING: If a single paragraph exceeds max_size, we must split further\n",
    "            # Fall back to sentence-level splitting (finer granularity than paragraphs)\n",
    "            if len(paragraph) > max_size:\n",
    "                # Split by sentence boundary (period followed by space)\n",
    "                # This is less ideal than paragraph boundaries but necessary for overflow handling\n",
    "                sentences = paragraph.split('. ')\n",
    "                for sentence in sentences:\n",
    "                    # RECURSIVE OVERFLOW: Even sentences might be too large (rare but possible)\n",
    "                    # Handle by accumulating sentences until hitting the limit\n",
    "                    if len(current_chunk) + len(sentence) > max_size:\n",
    "                        # Start a new sentence-level chunk\n",
    "                        if current_chunk:\n",
    "                            chunks.append(current_chunk.strip())\n",
    "                        # New chunk starts with this sentence (add period back)\n",
    "                        current_chunk = sentence + '. '\n",
    "                    else:\n",
    "                        # Accumulate this sentence with previous ones\n",
    "                        current_chunk += sentence + '. '\n",
    "            else:\n",
    "                # CASE: Single paragraph fits in a chunk by itself\n",
    "                # Assign it as the start of a new chunk (might accumulate more paragraphs)\n",
    "                current_chunk = paragraph\n",
    "        else:\n",
    "            # ACCUMULATION: Paragraph fits within remaining space, add to current chunk\n",
    "            # Preserve paragraph boundary by adding newlines (except for first paragraph)\n",
    "            current_chunk += '\\n\\n' + paragraph if current_chunk else paragraph\n",
    "    \n",
    "    # FINALIZATION: Don't forget the last chunk accumulated\n",
    "    # Edge case: last paragraph was already added to current_chunk but not committed\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_load_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "",
    "print(f'Loading Wikipedia dataset (target size: {TARGET_SIZE_MB}MB)...')",
    "print('Please wait, this may take a minute...\\n')",
    "",
    "# Check if we have a cached version locally",
    "if SAVE_LOCALLY and os.path.exists(LOCAL_DATASET_PATH):",
    "    print(f'‚úì Found cached dataset: {LOCAL_DATASET_PATH}')",
    "    with open(LOCAL_DATASET_PATH, 'r', encoding='utf-8') as f:",
    "        saved_data = json.load(f)",
    "    dataset = saved_data['chunks']",
    "    print(f'‚úì Loaded {len(dataset)} chunks from cache')",
    "else:",
    "    # Load from HuggingFace datasets",
    "    print('Downloading Simple Wikipedia from HuggingFace...')",
    "    wikipedia = load_dataset(\"wikimedia/wikipedia\", \"20231101.simple\", trust_remote_code=True)",
    "    articles = wikipedia['train']",
    "    ",
    "    # Filter and chunk articles to reach target size",
    "    dataset = []",
    "    total_size = 0",
    "    target_bytes = TARGET_SIZE_MB * 1024 * 1024",
    "    ",
    "    print(f'Processing articles (target: {TARGET_SIZE_MB}MB)...\\n')",
    "    ",
    "    for i, article in enumerate(articles):",
    "        # Stop when we reach target size",
    "        if total_size >= target_bytes:",
    "            break",
    "        ",
    "        # Format: \"Article: {title}\\n\\n{text}\"",
    "        article_text = f\"Article: {article['title']}\\n\\n{article['text']}\"",
    "        ",
    "        # Chunk the article text",
    "        chunks = chunk_text(article_text, max_size=MAX_CHUNK_SIZE)",
    "        dataset.extend(chunks)",
    "        ",
    "        # Track size",
    "        article_size = sys.getsizeof(article_text)",
    "        total_size += article_size",
    "        ",
    "        # Progress update every 20 articles",
    "        if (i + 1) % 20 == 0:",
    "            progress_pct = (total_size / target_bytes) * 100",
    "            print(f'  Processed {i+1} articles, {len(dataset)} chunks ({progress_pct:.1f}% of target size)')",
    "    ",
    "    print(f'\\n‚úì Dataset ready: {len(dataset)} chunks from {i+1} articles')",
    "    print(f'  Total size: {total_size / (1024*1024):.2f} MB\\n')",
    "    ",
    "    # Save locally if requested",
    "    if SAVE_LOCALLY:",
    "        print(f'Saving dataset to {LOCAL_DATASET_PATH}...')",
    "        with open(LOCAL_DATASET_PATH, 'w', encoding='utf-8') as f:",
    "            json.dump({'chunks': dataset}, f, ensure_ascii=False)",
    "        print(f'‚úì Saved dataset locally for future reuse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample_data",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's look at a few examples from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample chunks from the dataset:\\n')\n",
    "for i, chunk in enumerate(dataset[:3]):\n",
    "    print(f'--- Chunk {i+1} ---')\n",
    "    print(chunk[:300] + '...' if len(chunk) > 300 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a326fc",
   "metadata": {},
   "source": [
    "## Configure Models\n",
    "\n",
    "We'll use two models:\n",
    "- **Embedding Model**: Converts text into vector representations\n",
    "- **Language Model**: Generates responses based on retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a7821",
   "metadata": {},
   "source": [
    "## Implement the Vector Database\n",
    "\n",
    "### Indexing Phase\n",
    "\n",
    "In the indexing phase, we:\n",
    "1. Break the dataset into chunks (already done during loading)\n",
    "2. Calculate embedding vectors for each chunk\n",
    "3. Store chunks with their embeddings in our vector database\n",
    "\n",
    "Each element in `VECTOR_DB` will be a tuple: `(chunk, embedding)`\n",
    "\n",
    "The embedding is a list of floats, for example: `[0.1, 0.04, -0.34, 0.21, ...]`\n",
    "\n",
    "**Note**: This may take a few minutes depending on your dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# embedding = [0.12, -0.45, 0.78, ...] (768 dimensions for our model)\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "    \"\"\"Add a chunk and its embedding to the vector database.\n",
    "    \n",
    "    This is the critical step that makes RAG work:\n",
    "    \n",
    "    TEXT CHUNK              EMBEDDING MODEL            VECTOR (768 numbers)\n",
    "    \"Paris is the      ‚Üí    (BGE Model)        ‚Üí    [0.12, -0.45, 0.78, ...]\n",
    "     capital of France\"     \n",
    "    \n",
    "    The embedding captures semantic meaning. Similar chunks get similar vectors!\n",
    "    \"\"\"\n",
    "    # Generate embedding vector from text (768-dimensional for BGE model)\n",
    "    embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "    # Store both the original text and its vector representation\n",
    "    VECTOR_DB.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df5ca8",
   "metadata": {},
   "source": [
    "Now let's populate our vector database with all chunks from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Building vector database with {len(dataset)} chunks...')\n",
    "print('This may take a few minutes...\\n')\n",
    "\n",
    "for i, chunk in enumerate(dataset):\n",
    "    add_chunk_to_database(chunk)\n",
    "    \n",
    "    # Progress update every 50 chunks\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f'Embedded {i+1}/{len(dataset)} chunks ({(i+1)/len(dataset)*100:.1f}%)')\n",
    "\n",
    "print(f'\\n‚úì Vector database ready with {len(VECTOR_DB)} embeddings!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864c65a",
   "metadata": {},
   "source": [
    "## Implement the Retrieval Function\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "To find the most relevant chunks, we need to compare vector similarity. We'll use cosine similarity, which measures how \"close\" two vectors are in the vector space. Higher cosine similarity means more similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    MATHEMATICAL FOUNDATION:\n",
    "    Formula: similarity = (A ¬∑ B) / (||A|| √ó ||B||)\n",
    "    \n",
    "    Result ranges from -1 to 1:\n",
    "      1.0  = identical direction (perfect match)\n",
    "      0.5  = 60¬∞ angle (moderately similar)\n",
    "      0.0  = 90¬∞ angle (perpendicular, unrelated)\n",
    "    \n",
    "    Why cosine for text embeddings?\n",
    "    - Measures DIRECTION not magnitude\n",
    "    - Short text vs long text with same meaning ‚Üí same similarity\n",
    "    - Ignores document length bias (unlike Euclidean distance)\n",
    "    - Works in high dimensions (768D embeddings)\n",
    "    \n",
    "    See CONCEPTS.md Section 5 for detailed derivation\n",
    "    \"\"\"\n",
    "    # STEP 1: Compute dot product (sum of element-wise products)\n",
    "    # This measures how aligned the vectors are: A ¬∑ B = Œ£(a·µ¢ √ó b·µ¢)\n",
    "    # Example: [1,2,3] ¬∑ [2,3,4] = 1√ó2 + 2√ó3 + 3√ó4 = 2+6+12 = 20\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    \n",
    "    # STEP 2: Compute magnitude (L2 norm) of vector A\n",
    "    # Magnitude = ‚àö(a‚ÇÅ¬≤ + a‚ÇÇ¬≤ + ... + a‚Çô¬≤)\n",
    "    # Represents the \"length\" of the vector\n",
    "    # Example: [1,2,3] ‚Üí ‚àö(1+4+9) = ‚àö14 ‚âà 3.742\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    \n",
    "    # STEP 3: Compute magnitude of vector B\n",
    "    # Same formula as above, applied to the second vector\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    \n",
    "    # STEP 4: Normalize by magnitudes to get cosine similarity\n",
    "    # The division removes magnitude influence, keeping only direction\n",
    "    # Cosine of angle between vectors = dot_product / (magnitude_a √ó magnitude_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11352532",
   "metadata": {},
   "source": [
    "### Retrieval Function\n",
    "\n",
    "The retrieval function:\n",
    "1. Converts the query into an embedding vector\n",
    "2. Compares it against all vectors in the database\n",
    "3. Returns the top N most relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99953b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    \"\"\"Retrieve the top N most relevant chunks for a given query.\n",
    "    \n",
    "    This is the RETRIEVAL phase of RAG:\n",
    "    \n",
    "    1. Convert query to embedding  ‚Üí  [query_vector]\n",
    "    2. Compare to all stored embeddings using cosine similarity\n",
    "    3. Return top N most similar chunks\n",
    "    \n",
    "    Why it works: Similar meanings produce similar vectors!\n",
    "    If you ask \"What is the capital of France?\"\n",
    "    It will find chunks about Paris because they're semantically similar.\n",
    "    \n",
    "    COMPLEXITY ANALYSIS:\n",
    "    - Query embedding: O(token_count) in embedding model\n",
    "    - Similarity computation: O(n √ó d) where n=chunk count, d=embedding dimension\n",
    "    - Sorting: O(n log n) to find top-K\n",
    "    - Total: O(n log n) for large n, dominated by sorting\n",
    "    \n",
    "    For large databases (>100k chunks), consider using a vector index\n",
    "    (HNSW, Faiss) to reduce retrieval from O(n) to O(log n).\n",
    "    \"\"\"\n",
    "    # PHASE 2A: EMBED THE QUERY\n",
    "    # Convert user's natural language question into a vector in the same space as indexed chunks\n",
    "    # Critical: MUST use the same embedding model as indexing phase!\n",
    "    # Mismatch (e.g., indexing with BGE, retrieving with OpenAI) causes complete failure\n",
    "    query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    \n",
    "    # PHASE 2B: COMPUTE SIMILARITIES\n",
    "    # For each stored chunk, measure how similar it is to the query\n",
    "    # This is the semantic search: finding meaning-based matches, not keyword matches\n",
    "    similarities = []\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "        # COSINE SIMILARITY: 1 = identical direction, 0 = unrelated, -1 = opposite\n",
    "        # For text, values are typically 0.3-0.98 (rarely negative in practice)\n",
    "        similarity = cosine_similarity(query_embedding, embedding)\n",
    "        # Store both chunk content and its similarity score for ranking\n",
    "        similarities.append((chunk, similarity))\n",
    "    \n",
    "    # PHASE 2C: TOP-K SELECTION\n",
    "    # Sort all chunks by similarity (descending = highest first)\n",
    "    # Time complexity: O(n log n) where n = chunk count\n",
    "    # OPTIMIZATION: For n > 100k, use a heap-based selection: O(n log k)\n",
    "    # Heap approach keeps only k smallest (reverse heap for largest k)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # PHASE 2D: RETURN TOP N\n",
    "    # Return the top_n most relevant chunks\n",
    "    # Edge case: if fewer chunks than top_n exist, return all (no error)\n",
    "    # Each chunk is returned with its similarity score for quality assessment\n",
    "    return similarities[:top_n]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11d6bc",
   "metadata": {},
   "source": [
    "## Generation Phase\n",
    "\n",
    "In the generation phase, the chatbot generates a response based on the retrieved knowledge. We construct a prompt that includes the relevant chunks and instruct the model to only use that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query, top_n=3, verbose=True):\n",
    "    \"\"\"Ask a question and get a response based on retrieved knowledge.\n",
    "    \n",
    "    This is the GENERATION phase of RAG:\n",
    "    \n",
    "    USER QUESTION\n",
    "         ‚Üì\n",
    "    [Retrieve relevant chunks]  ‚Üê uses semantic search\n",
    "         ‚Üì\n",
    "    [Build context prompt]  ‚Üê \"Use only this knowledge to answer\"\n",
    "         ‚Üì\n",
    "    [Generate response with LLM]  ‚Üê Llama model reads context and answers\n",
    "         ‚Üì\n",
    "    ANSWER (grounded in Wikipedia)\n",
    "    \n",
    "    TWO-STAGE ARCHITECTURE:\n",
    "    Stage 1 (Retrieval): Find facts ‚Üí retrieve() function\n",
    "    Stage 2 (Generation): Synthesize facts ‚Üí LLM with context\n",
    "    \n",
    "    This separation allows independent optimization:\n",
    "    - Improve retrieval without changing LLM\n",
    "    - Improve prompting without reindexing\n",
    "    - Debug easily by testing each stage separately\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask (natural language string)\n",
    "        top_n: Number of relevant chunks to retrieve (trade-off: 3-5 typical)\n",
    "               More chunks = more context but slower and noisier\n",
    "        verbose: Whether to print retrieved knowledge and response\n",
    "    \n",
    "    Returns:\n",
    "        The chatbot's response as a string (answer to the query)\n",
    "    \"\"\"\n",
    "    # GENERATION PHASE STEP 1: RETRIEVE RELEVANT KNOWLEDGE\n",
    "    # Use semantic search to find chunks similar to the query\n",
    "    # These chunks become the \"context window\" for the LLM\n",
    "    retrieved_knowledge = retrieve(query, top_n=top_n)\n",
    "    \n",
    "    if verbose:\n",
    "        # DEBUGGING OUTPUT: Show what was retrieved\n",
    "        # Helps diagnose retrieval failures (low similarity scores indicate poor matches)\n",
    "        print('Retrieved knowledge:')\n",
    "        for i, (chunk, similarity) in enumerate(retrieved_knowledge):\n",
    "            # Extract article title from chunk format: \"Article: Title\\\\n\\\\n...\"\n",
    "            title_line = chunk.split('\\n')[0]\n",
    "            # Show snippet (first 200 chars) for human review\n",
    "            preview = chunk[:200].replace('\\n', ' ') + '...' if len(chunk) > 200 else chunk\n",
    "            # Similarity score: 1.0 = perfect match, 0.0 = unrelated\n",
    "            # Typically see 0.7-0.95 for relevant chunks\n",
    "            print(f'  [{i+1}] (similarity: {similarity:.3f}) {preview}')\n",
    "        print()\n",
    "    \n",
    "    # GENERATION PHASE STEP 2: BUILD INSTRUCTION PROMPT\n",
    "    # This is where we implement \"grounded generation\"\n",
    "    # The system message constrains the LLM: \"Use ONLY provided context, don't hallucinate\"\n",
    "    # This is the key to RAG: retrieval + constraint = grounded answers\n",
    "    instruction_prompt = f\"\"\"You are a helpful chatbot that answers questions based on Wikipedia articles.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information.\n",
    "If the context doesn't contain enough information to answer the question, say so.\n",
    "\n",
    "Context:\n",
    "{chr(10).join([f'{i+1}. {chunk.strip()}' for i, (chunk, _) in enumerate(retrieved_knowledge)])}\n",
    "\"\"\"\n",
    "    \n",
    "    # GENERATION PHASE STEP 3: SEND TO LANGUAGE MODEL FOR GENERATION\n",
    "    # The LLM has:\n",
    "    # - System prompt: constraints and role (\"use only provided context\")\n",
    "    # - Retrieved context: the facts it should use\n",
    "    # - User query: what they're asking about\n",
    "    # The model synthesizes these into a coherent answer\n",
    "    # Using stream=True for faster feedback (token-by-token generation)\n",
    "    stream = ollama.chat(\n",
    "        model=LANGUAGE_MODEL,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': instruction_prompt},\n",
    "            {'role': 'user', 'content': query},\n",
    "        ],\n",
    "        stream=True,  # Stream response token-by-token for faster feedback\n",
    "                      # Alternative: stream=False gets full response at once\n",
    "    )\n",
    "    \n",
    "    # GENERATION PHASE STEP 4: COLLECT AND DISPLAY THE RESPONSE\n",
    "    # As the LLM generates tokens, collect them into a complete response\n",
    "    # Streaming shows output in real-time (better UX) vs waiting for full response\n",
    "    if verbose:\n",
    "        print('Chatbot response:')\n",
    "    \n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        # Extract the text content from the chunk\n",
    "        content = chunk['message']['content']\n",
    "        response += content\n",
    "        if verbose:\n",
    "            # Print token-by-token as it arrives (better UX than waiting)\n",
    "            print(content, end='', flush=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n')  # ensure a newline after the streamed response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988ae81",
   "metadata": {},
   "source": [
    "## Try It Out!\n",
    "\n",
    "Now let's ask some questions. The quality of answers will depend on which articles were included in your dataset sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"Tell me about Albert Einstein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"What is Python programming language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "climate_question",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"How does photosynthesis work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370c0c5",
   "metadata": {},
   "source": [
    "## Interactive Chat\n",
    "\n",
    "You can also use this cell to ask your own questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask your own question here\n",
    "your_question = \"What is the solar system?\"\n",
    "ask_question(your_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_section",
   "metadata": {},
   "source": "## Export Embeddings for Vector Databases\n\nExport your embeddings to use with various vector database platforms. Choose the format that matches your target platform:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_embeddings(chunks=None, embeddings=None, output_path='embeddings_export.json', format='generic'):\n",
    "    \"\"\"\n",
    "    Export embeddings in a generic format compatible with multiple vector databases.\n",
    "    \n",
    "    This function removes vendor lock-in by providing a standard export format\n",
    "    that works with any PostgreSQL-compatible vector database.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of text chunks (optional, will use VECTOR_DB if not provided)\n",
    "        embeddings: List of embedding vectors (optional, will use VECTOR_DB if not provided)\n",
    "        output_path (str): Path to output JSON or SQL file\n",
    "        format (str): Export format - 'generic' (default), 'pgvector', or 'pinecone'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Export statistics (count, dimension, file_size_mb, format, path)\n",
    "    \n",
    "    Supports:\n",
    "        - PostgreSQL with pgvector (local, Neon, Supabase, RDS)\n",
    "        - Pinecone vector database\n",
    "        - Generic JSON for custom integrations\n",
    "        \n",
    "    Examples:\n",
    "        # Export as generic JSON\n",
    "        stats = export_embeddings(format='generic')\n",
    "        \n",
    "        # Export as PostgreSQL INSERT statements\n",
    "        stats = export_embeddings(format='pgvector')\n",
    "        \n",
    "        # Export for Pinecone\n",
    "        stats = export_embeddings(format='pinecone')\n",
    "    \"\"\"\n",
    "    # Use provided chunks/embeddings or default to VECTOR_DB\n",
    "    if chunks is None or embeddings is None:\n",
    "        if not VECTOR_DB:\n",
    "            raise ValueError('No embeddings available. Generate embeddings first or provide chunks and embeddings.')\n",
    "        chunks = [chunk for chunk, _ in VECTOR_DB]\n",
    "        embeddings = [emb for _, emb in VECTOR_DB]\n",
    "    \n",
    "    if len(chunks) != len(embeddings):\n",
    "        raise ValueError('chunks and embeddings must have the same length')\n",
    "    \n",
    "    embedding_dimension = len(embeddings[0]) if embeddings else 0\n",
    "    \n",
    "    if format == 'generic':\n",
    "        # Generic JSON format: standard structure without vendor lock-in\n",
    "        export_data = {\n",
    "            'metadata': {\n",
    "                'model': EMBEDDING_MODEL,\n",
    "                'dimension': embedding_dimension,\n",
    "                'count': len(embeddings),\n",
    "                'created_at': __import__('datetime').datetime.now().isoformat() + 'Z',\n",
    "                'format_type': 'generic'\n",
    "            },\n",
    "            'embeddings': [\n",
    "                {\n",
    "                    'id': f'chunk_{i}',\n",
    "                    'vector': embedding,\n",
    "                    'metadata': {\n",
    "                        'text': chunk,\n",
    "                        'source': chunk.split('\\n')[0].replace('Article: ', '') if chunk.startswith('Article: ') else 'unknown'\n",
    "                    }\n",
    "                }\n",
    "                for i, (chunk, embedding) in enumerate(zip(chunks, embeddings))\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        file_size_mb = __import__('os').path.getsize(output_path) / (1024 * 1024)\n",
    "        \n",
    "        print(f'‚úì Exported {len(embeddings)} embeddings in generic JSON format')\n",
    "        print(f'  Output: {output_path}')\n",
    "        print(f'  File size: {file_size_mb:.2f} MB')\n",
    "        print(f'  Dimension: {embedding_dimension}')\n",
    "        print(f'\\nThis format works with:')\n",
    "        print(f'  - PostgreSQL with pgvector (via JSON import)')\n",
    "        print(f'  - Neon PostgreSQL')\n",
    "        print(f'  - Supabase (PostgreSQL + pgvector)')\n",
    "        print(f'  - AWS RDS with pgvector')\n",
    "        print(f'  - Custom vector database integrations')\n",
    "        \n",
    "    elif format == 'pgvector':\n",
    "        # PostgreSQL pgvector format: SQL INSERT statements\n",
    "        sql_lines = ['-- PostgreSQL pgvector export', '-- Insert into embeddings table', '']\n",
    "        \n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            # Extract title for metadata\n",
    "            title = chunk.split('\\n')[0].replace('Article: ', '') if chunk.startswith('Article: ') else 'unknown'\n",
    "            \n",
    "            # Escape single quotes in text and title\n",
    "            safe_chunk = chunk.replace(\"'\", \"''\")\n",
    "            safe_title = title.replace(\"'\", \"''\")\n",
    "            \n",
    "            # Format embedding as PostgreSQL vector\n",
    "            vector_str = '[' + ','.join(str(v) for v in embedding) + ']'\n",
    "            \n",
    "            # Build INSERT statement\n",
    "            sql = f\"INSERT INTO embeddings (chunk_id, chunk_text, embedding, source) VALUES ('{i}', E'{safe_chunk}', '{vector_str}'::vector, '{safe_title}');\"\n",
    "            sql_lines.append(sql)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sql_lines))\n",
    "        \n",
    "        file_size_mb = __import__('os').path.getsize(output_path) / (1024 * 1024)\n",
    "        \n",
    "        print(f'‚úì Exported {len(embeddings)} embeddings as PostgreSQL INSERT statements')\n",
    "        print(f'  Output: {output_path}')\n",
    "        print(f'  File size: {file_size_mb:.2f} MB')\n",
    "        print(f'\\nTo import into PostgreSQL:')\n",
    "        print(f'  psql -U postgres -d your_database -f {output_path}')\n",
    "        print(f'\\nWorks with:')\n",
    "        print(f'  - Local PostgreSQL + pgvector')\n",
    "        print(f'  - Neon PostgreSQL')\n",
    "        print(f'  - Supabase')\n",
    "        print(f'  - AWS RDS with pgvector')\n",
    "        \n",
    "    elif format == 'pinecone':\n",
    "        # Pinecone format: vectors with metadata\n",
    "        export_data = {\n",
    "            'vectors': [\n",
    "                {\n",
    "                    'id': f'chunk_{i}',\n",
    "                    'values': embedding,\n",
    "                    'metadata': {\n",
    "                        'text': chunk,\n",
    "                        'source': chunk.split('\\n')[0].replace('Article: ', '') if chunk.startswith('Article: ') else 'unknown'\n",
    "                    }\n",
    "                }\n",
    "                for i, (chunk, embedding) in enumerate(zip(chunks, embeddings))\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        file_size_mb = __import__('os').path.getsize(output_path) / (1024 * 1024)\n",
    "        \n",
    "        print(f'‚úì Exported {len(embeddings)} embeddings in Pinecone format')\n",
    "        print(f'  Output: {output_path}')\n",
    "        print(f'  File size: {file_size_mb:.2f} MB')\n",
    "        print(f'\\nTo import into Pinecone:')\n",
    "        print(f'  1. Parse the JSON file')\n",
    "        print(f'  2. Use Pinecone upsert API to insert vectors')\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown format '{format}'. Use 'generic', 'pgvector', or 'pinecone'\")\n",
    "    \n",
    "    return {\n",
    "        'count': len(embeddings),\n",
    "        'dimension': embedding_dimension,\n",
    "        'file_size_mb': file_size_mb,\n",
    "        'format': format,\n",
    "        'path': output_path\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Generic export (recommended for portability)\n",
    "# stats = export_embeddings(format='generic')\n",
    "# print(f\"Exported {stats['count']} embeddings to {stats['path']}\")\n",
    "\n",
    "# PostgreSQL export\n",
    "# stats = export_embeddings(format='pgvector')\n",
    "\n",
    "# Pinecone export\n",
    "# stats = export_embeddings(format='pinecone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2d324",
   "metadata": {},
   "source": [
    "## Next Steps and Improvements\n",
    "\n",
    "### Upgrade to Persistent Storage with PostgreSQL\n",
    "\n",
    "**Current limitation**: Embeddings are lost when the notebook restarts, requiring 50+ minutes to regenerate.\n",
    "\n",
    "**Solution**: Upgrade to the advanced version with PostgreSQL + pgvector:\n",
    "\n",
    "1. **See** `wikipedia-rag-tutorial-advanced.ipynb` for the PostgreSQL version\n",
    "2. **Benefits**:\n",
    "   - Generate embeddings once, reuse across experiments\n",
    "   - Store multiple embedding models for comparison\n",
    "   - Run analyses in minutes instead of regenerating\n",
    "   - Easy migration path to production databases\n",
    "\n",
    "3. **Quick start**:\n",
    "   ```bash\n",
    "   # Start PostgreSQL\n",
    "   docker run -d --name pgvector-rag \\\n",
    "     -e POSTGRES_PASSWORD=postgres \\\n",
    "     -e POSTGRES_DB=rag_db \\\n",
    "     -p 5432:5432 \\\n",
    "     -v pgvector_data:/var/lib/postgresql/data \\\n",
    "     pgvector/pgvector:pg16\n",
    "   \n",
    "   # Install PostgreSQL adapter\n",
    "   pip install psycopg2-binary\n",
    "   \n",
    "   # Open the advanced notebook\n",
    "   jupyter notebook wikipedia-rag-tutorial-advanced.ipynb\n",
    "   ```\n",
    "\n",
    "### Other RAG Improvements\n",
    "\n",
    "1. **Hybrid Search**: Combine vector similarity with keyword search (BM25)\n",
    "   - Better for specific terminology and exact matches\n",
    "   - Combine results using reciprocal rank fusion\n",
    "\n",
    "2. **Reranking**: Use a [reranking model](https://www.pinecone.io/learn/series/rag/rerankers/)\n",
    "   - Cross-encoder models for better relevance\n",
    "   - Re-score top 10-20 results from initial retrieval\n",
    "\n",
    "3. **Query Expansion**: Generate multiple query variations\n",
    "   - Use LLM to create related questions\n",
    "   - Retrieve for each and merge results\n",
    "\n",
    "4. **Better Chunking**:\n",
    "   - Semantic chunking (split by meaning)\n",
    "   - Overlapping chunks for better context\n",
    "   - Parent-child chunks (retrieve child, return parent)\n",
    "\n",
    "5. **Citation Support**:\n",
    "   - Track which chunks were used\n",
    "   - Provide Wikipedia URLs as sources\n",
    "   - Show confidence scores\n",
    "\n",
    "### Advanced RAG Patterns\n",
    "\n",
    "- **Graph RAG**: Build knowledge graphs from Wikipedia links\n",
    "- **Agentic RAG**: Let the LLM decide when to retrieve more information\n",
    "- **Multi-hop RAG**: Follow reasoning chains across multiple documents\n",
    "- **RAG Fusion**: Combine multiple retrieval strategies\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [HuggingFace RAG Guide](https://huggingface.co/blog/ngxson/make-your-own-rag)\n",
    "- [Pinecone Learning Center](https://www.pinecone.io/learn/)\n",
    "- Our documentation: See `POSTGRESQL_SETUP.md` for detailed setup instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_section",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "View statistics about your loaded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats():\n",
    "    \"\"\"Print statistics about the current dataset.\"\"\"\n",
    "    total_chars = sum(len(chunk) for chunk in dataset)\n",
    "    avg_chunk_size = total_chars / len(dataset) if dataset else 0\n",
    "    \n",
    "    # Count unique articles\n",
    "    articles = set()\n",
    "    for chunk in dataset:\n",
    "        if chunk.startswith('Article: '):\n",
    "            title = chunk.split('\\n')[0].replace('Article: ', '')\n",
    "            articles.add(title)\n",
    "    \n",
    "    print('Dataset Statistics:')\n",
    "    print(f'  Total chunks: {len(dataset):,}')\n",
    "    print(f'  Unique articles: {len(articles):,}')\n",
    "    print(f'  Total characters: {total_chars:,}')\n",
    "    print(f'  Average chunk size: {avg_chunk_size:.0f} characters')\n",
    "    print(f'  Estimated size: {sys.getsizeof(str(dataset)) / (1024*1024):.2f} MB')\n",
    "    print(f'\\n  Embeddings in database: {len(VECTOR_DB):,}')\n",
    "    print(f'  Embedding dimension: {len(VECTOR_DB[0][1]) if VECTOR_DB else 0}')\n",
    "\n",
    "print_dataset_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}