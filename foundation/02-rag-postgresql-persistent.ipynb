{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4644e21",
   "metadata": {},
   "source": [
    "# Foundation 02: RAG with PostgreSQL (Persistent Storage)\n",
    "\n",
    "This notebook demonstrates a Retrieval-Augmented Generation (RAG) system using Simple Wikipedia articles with **persistent PostgreSQL storage** and **pgvector** for efficient similarity search.\n",
    "\n",
    "This is the **persistent version** with durable embeddings - perfect for production systems and reusable embedding sets.\n",
    "\n",
    "**Want to start simpler?** See `foundation/01-basic-rag-in-memory.ipynb` for the in-memory version without database setup.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Set up PostgreSQL with pgvector extension for vector storage\n",
    "2. Generate and store embeddings persistently\n",
    "3. Implement efficient similarity search with pgvector\n",
    "4. **Register embeddings in the registry** for reuse across notebooks\n",
    "5. Avoid regenerating embeddings with data preservation settings\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "Before running this notebook, you need to:\n",
    "\n",
    "1. **Install Ollama** from [ollama.com](https://ollama.com/)\n",
    "2. **Download the required models** by running these commands in your terminal:\n",
    "\n",
    "```bash\n",
    "ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\n",
    "ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n",
    "```\n",
    "\n",
    "3. **Install the required Python packages**:\n",
    "\n",
    "```bash\n",
    "pip install ollama datasets jupyter psycopg2-binary\n",
    "```\n",
    "\n",
    "4. **Set up PostgreSQL with pgvector** - See `POSTGRESQL_SETUP.md` for detailed instructions\n",
    "\n",
    "## Learning Progression\n",
    "\n",
    "- ‚úÖ **You are here:** foundation/02 - Persistent storage with PostgreSQL\n",
    "- ‚è≠Ô∏è  **Next:** intermediate/03 - Loading and reusing embeddings from registry\n",
    "- üéØ **Path:** See `LEARNING_ROADMAP.md` for complete learning paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3198d7f",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e96af9",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the target dataset size. The script will download articles until it reaches approximately this size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc326412",
   "metadata": {},
   "source": [
    "## Install Additional Dependencies\n",
    "\n",
    "If you plan to use PostgreSQL for persistent storage, install the additional dependency:\n",
    "\n",
    "```bash\n",
    "pip install psycopg2-binary\n",
    "```\n",
    "\n",
    "Or if you're using a virtual environment (recommended):\n",
    "\n",
    "```bash\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "pip install psycopg2-binary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target dataset size in MB (adjust as needed: 10, 20, 30, 40, 50)\n",
    "TARGET_SIZE_MB = 10\n",
    "\n",
    "# Maximum chunk size in characters (for splitting long articles)\n",
    "MAX_CHUNK_SIZE = 1000\n",
    "\n",
    "# Whether to save the dataset locally for reuse\n",
    "SAVE_LOCALLY = True\n",
    "LOCAL_DATASET_PATH = f'wikipedia_dataset_{TARGET_SIZE_MB}mb.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b99ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL configuration\n",
    "POSTGRES_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'rag_db',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "}\n",
    "\n",
    "# Table name for this embedding model (allows storing multiple models)\n",
    "# Table name will be: embeddings_{EMBEDDING_MODEL_ALIAS}\n",
    "EMBEDDING_MODEL_ALIAS = 'bge_base_en_v1.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b8952",
   "metadata": {},
   "source": [
    "## Data Preservation Settings\n",
    "\n",
    "Control how the notebook handles existing embeddings in the database:\n",
    "\n",
    "- **`None`** (default): Prompt interactively when existing data is found\n",
    "- **`True`**: Always preserve existing embeddings (useful for analysis/reuse)\n",
    "- **`False`**: Always overwrite existing embeddings (useful for fresh experiments)\n",
    "\n",
    "This prevents accidentally losing hours of embedding generation work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad28877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control what happens if embeddings already exist in the database\n",
    "# None = prompt interactively (recommended for first-time users)\n",
    "# True = always preserve existing data (for analysis/reuse)\n",
    "# False = always overwrite (for fresh experiments)\n",
    "PRESERVE_EXISTING_EMBEDDINGS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_loading",
   "metadata": {},
   "source": [
    "## Load and Filter the Wikipedia Dataset\n",
    "\n",
    "We'll use Simple Wikipedia, which has cleaner, more concise articles. The dataset will be filtered to approximately your target size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_size_mb(text):\n",
    "    \"\"\"Estimate the size of text in megabytes.\"\"\"\n",
    "    return sys.getsizeof(text) / (1024 * 1024)\n",
    "\n",
    "def chunk_text(text, max_size=1000):\n",
    "    \"\"\"Split text into chunks of approximately max_size characters.\n",
    "    \n",
    "    Tries to break at paragraph boundaries when possible.\n",
    "    \"\"\"\n",
    "    if len(text) <= max_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    current_chunk = ''\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # If adding this paragraph would exceed max_size\n",
    "        if len(current_chunk) + len(paragraph) > max_size:\n",
    "            if current_chunk:  # Save current chunk if not empty\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = ''\n",
    "            \n",
    "            # If single paragraph is too large, split it\n",
    "            if len(paragraph) > max_size:\n",
    "                sentences = paragraph.split('. ')\n",
    "                for sentence in sentences:\n",
    "                    if len(current_chunk) + len(sentence) > max_size:\n",
    "                        if current_chunk:\n",
    "                            chunks.append(current_chunk.strip())\n",
    "                        current_chunk = sentence + '. '\n",
    "                    else:\n",
    "                        current_chunk += sentence + '. '\n",
    "            else:\n",
    "                current_chunk = paragraph\n",
    "        else:\n",
    "            current_chunk += '\\n\\n' + paragraph if current_chunk else paragraph\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_wikipedia_dataset(target_size_mb, local_path=None):\n",
    "    \"\"\"Load and filter Wikipedia dataset to target size.\n",
    "    \n",
    "    Args:\n",
    "        target_size_mb: Target dataset size in megabytes\n",
    "        local_path: Path to save/load dataset locally\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Try to load from local cache first\n",
    "    if local_path:\n",
    "        try:\n",
    "            print(f'Attempting to load cached dataset from {local_path}...')\n",
    "            with open(local_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f'‚úì Loaded {len(data[\"chunks\"])} chunks from cache')\n",
    "                print(f'  Estimated size: {data[\"size_mb\"]:.2f} MB')\n",
    "                return data['chunks']\n",
    "        except FileNotFoundError:\n",
    "            print('No cached dataset found, downloading from HuggingFace...')\n",
    "    \n",
    "    # Load Simple Wikipedia dataset\n",
    "    print('Loading Simple Wikipedia dataset (this may take a minute)...')\n",
    "    dataset = load_dataset('wikimedia/wikipedia', '20231101.simple', split='train', streaming=True)\n",
    "    \n",
    "    chunks = []\n",
    "    current_size_mb = 0\n",
    "    target_bytes = target_size_mb * 1024 * 1024\n",
    "    article_count = 0\n",
    "    \n",
    "    print(f'\\nCollecting articles (target: {target_size_mb} MB)...')\n",
    "    \n",
    "    for article in dataset:\n",
    "        # Skip very short articles\n",
    "        if len(article['text']) < 200:\n",
    "            continue\n",
    "        \n",
    "        # Create metadata-enriched chunks\n",
    "        article_chunks = chunk_text(article['text'], MAX_CHUNK_SIZE)\n",
    "        \n",
    "        for chunk in article_chunks:\n",
    "            # Add title context to help with retrieval\n",
    "            enriched_chunk = f\"Article: {article['title']}\\n\\n{chunk}\"\n",
    "            chunk_size = sys.getsizeof(enriched_chunk)\n",
    "            \n",
    "            chunks.append(enriched_chunk)\n",
    "            current_size_mb += chunk_size\n",
    "            \n",
    "            # Check if we've reached target size\n",
    "            if current_size_mb >= target_bytes:\n",
    "                break\n",
    "        \n",
    "        article_count += 1\n",
    "        \n",
    "        # Progress update every 50 articles\n",
    "        if article_count % 50 == 0:\n",
    "            print(f'  Progress: {current_size_mb / (1024*1024):.2f} MB ({article_count} articles, {len(chunks)} chunks)')\n",
    "        \n",
    "        if current_size_mb >= target_bytes:\n",
    "            break\n",
    "    \n",
    "    final_size_mb = current_size_mb / (1024 * 1024)\n",
    "    print(f'\\n‚úì Dataset loaded: {len(chunks)} chunks from {article_count} articles')\n",
    "    print(f'  Estimated size: {final_size_mb:.2f} MB')\n",
    "    \n",
    "    # Save locally if requested\n",
    "    if local_path:\n",
    "        print(f'\\nSaving dataset to {local_path}...')\n",
    "        with open(local_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'size_mb': final_size_mb,\n",
    "                'chunk_count': len(chunks),\n",
    "                'article_count': article_count,\n",
    "                'chunks': chunks\n",
    "            }, f, ensure_ascii=False)\n",
    "        print('‚úì Dataset saved for future use')\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_wikipedia_dataset(\n",
    "    TARGET_SIZE_MB, \n",
    "    LOCAL_DATASET_PATH if SAVE_LOCALLY else None\n",
    ")\n",
    "\n",
    "print(f'\\nReady to build vector database with {len(dataset)} chunks!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database helper functions for PostgreSQL storage\n",
    "\n",
    "class PostgreSQLVectorDB:\n",
    "    \"\"\"Helper class to manage embeddings in PostgreSQL with pgvector.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, table_name, preserve_existing=None):\n",
    "        \"\"\"Initialize database connection.\n",
    "        \n",
    "        Args:\n",
    "            config: Dictionary with host, port, database, user, password\n",
    "            table_name: Name of the table for this embedding model\n",
    "            preserve_existing: If True, preserve existing data; if False, overwrite;\n",
    "                             if None, prompt user interactively\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.table_name = table_name\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "        self.setup_table(preserve_existing)\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection.\"\"\"\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(\n",
    "                host=self.config['host'],\n",
    "                port=self.config['port'],\n",
    "                database=self.config['database'],\n",
    "                user=self.config['user'],\n",
    "                password=self.config['password']\n",
    "            )\n",
    "            print(f'‚úì Connected to PostgreSQL at {self.config[\"host\"]}:{self.config[\"port\"]}')\n",
    "        except psycopg2.OperationalError as e:\n",
    "            print(f'‚úó Failed to connect to PostgreSQL: {e}')\n",
    "            print('Make sure PostgreSQL is running with pgvector support.')\n",
    "            print('Start it with: docker run -d --name pgvector-rag \\\\')\n",
    "            print('  -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=rag_db \\\\')\n",
    "            print('  -p 5432:5432 -v pgvector_data:/var/lib/postgresql/data \\\\')\n",
    "            print('  pgvector/pgvector:pg16')\n",
    "            raise\n",
    "    \n",
    "    def setup_table(self, preserve_existing=None):\n",
    "        \"\"\"Create table or check for existing data.\n",
    "        \n",
    "        Args:\n",
    "            preserve_existing: If True, preserve existing data; if False, overwrite;\n",
    "                             if None, prompt user interactively\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            # Enable pgvector extension\n",
    "            cur.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "            \n",
    "            # Check if table exists and has data\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT EXISTS (\n",
    "                    SELECT FROM information_schema.tables \n",
    "                    WHERE table_name = '{self.table_name}'\n",
    "                )\n",
    "            \"\"\")\n",
    "            table_exists = cur.fetchone()[0]\n",
    "            \n",
    "            existing_count = 0\n",
    "            if table_exists:\n",
    "                cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n",
    "                existing_count = cur.fetchone()[0]\n",
    "            \n",
    "            # Determine whether to preserve or overwrite\n",
    "            should_drop = False\n",
    "            \n",
    "            if existing_count > 0:\n",
    "                print(f'\\n‚ö†Ô∏è  Table \"{self.table_name}\" already exists with {existing_count:,} embeddings')\n",
    "                \n",
    "                if preserve_existing is None:\n",
    "                    # Interactive prompt\n",
    "                    while True:\n",
    "                        response = input('\\nDo you want to (p)reserve existing data or (o)verwrite it? [p/o]: ').lower().strip()\n",
    "                        if response in ['p', 'preserve']:\n",
    "                            preserve_existing = True\n",
    "                            break\n",
    "                        elif response in ['o', 'overwrite']:\n",
    "                            preserve_existing = False\n",
    "                            break\n",
    "                        else:\n",
    "                            print('Please enter \"p\" for preserve or \"o\" for overwrite')\n",
    "                \n",
    "                if preserve_existing:\n",
    "                    print(f'‚úì Preserving existing {existing_count:,} embeddings')\n",
    "                    self.conn.commit()\n",
    "                    return\n",
    "                else:\n",
    "                    print(f'‚ö†Ô∏è  Dropping table and recreating from scratch...')\n",
    "                    should_drop = True\n",
    "            else:\n",
    "                should_drop = True\n",
    "            \n",
    "            # Drop and recreate if needed\n",
    "            if should_drop:\n",
    "                cur.execute(f'DROP TABLE IF EXISTS {self.table_name} CASCADE')\n",
    "                \n",
    "                # Create table with vector column\n",
    "                cur.execute(f'''\n",
    "                    CREATE TABLE {self.table_name} (\n",
    "                        id SERIAL PRIMARY KEY,\n",
    "                        chunk_text TEXT NOT NULL,\n",
    "                        embedding vector(768),\n",
    "                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                    )\n",
    "                ''')\n",
    "                \n",
    "                # Create index for fast similarity search\n",
    "                index_name = f'{self.table_name}_embedding_idx'\n",
    "                cur.execute(f'''\n",
    "                    CREATE INDEX {index_name}\n",
    "                    ON {self.table_name} USING hnsw (embedding vector_cosine_ops)\n",
    "                ''')\n",
    "                \n",
    "                self.conn.commit()\n",
    "                print(f'‚úì Table \"{self.table_name}\" created (fresh start)')\n",
    "    \n",
    "    def insert_embedding(self, chunk, embedding):\n",
    "        \"\"\"Insert a chunk and its embedding into the database.\n",
    "        \n",
    "        Args:\n",
    "            chunk: The text chunk\n",
    "            embedding: The embedding vector (list of floats)\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                INSERT INTO {self.table_name} (chunk_text, embedding)\n",
    "                VALUES (%s, %s)\n",
    "            ''', (chunk, embedding))\n",
    "            self.conn.commit()\n",
    "    \n",
    "    def insert_batch(self, chunks_embeddings):\n",
    "        \"\"\"Batch insert multiple chunks and embeddings.\n",
    "        \n",
    "        Args:\n",
    "            chunks_embeddings: List of (chunk, embedding) tuples\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            execute_values(cur, f'''\n",
    "                INSERT INTO {self.table_name} (chunk_text, embedding)\n",
    "                VALUES %s\n",
    "            ''', chunks_embeddings, page_size=100)\n",
    "            self.conn.commit()\n",
    "    \n",
    "    def get_chunk_count(self):\n",
    "        \"\"\"Get the number of stored chunks.\"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n",
    "            return cur.fetchone()[0]\n",
    "    \n",
    "    def get_all_embeddings(self):\n",
    "        \"\"\"Get all chunks and embeddings from the database.\n",
    "        \n",
    "        Returns:\n",
    "            List of (chunk_text, embedding) tuples\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                SELECT chunk_text, embedding\n",
    "                FROM {self.table_name}\n",
    "                ORDER BY id\n",
    "            ''')\n",
    "            results = cur.fetchall()\n",
    "            # Convert pgvector arrays back to Python lists\n",
    "            return [(chunk, list(embedding)) for chunk, embedding in results]\n",
    "    \n",
    "    def get_embedding_dimension(self):\n",
    "        \"\"\"Get the dimension of the stored embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            Integer dimension, or 0 if no embeddings exist\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                SELECT embedding\n",
    "                FROM {self.table_name}\n",
    "                LIMIT 1\n",
    "            ''')\n",
    "            result = cur.fetchone()\n",
    "            if result and result[0]:\n",
    "                return len(result[0])\n",
    "            return 0\n",
    "    \n",
    "    def similarity_search(self, query_embedding, top_n=3):\n",
    "        \"\"\"Find most similar chunks using pgvector.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: The query embedding vector\n",
    "            top_n: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (chunk_text, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                SELECT chunk_text, \n",
    "                       1 - (embedding <=> %s::vector) as similarity\n",
    "                FROM {self.table_name}\n",
    "                ORDER BY embedding <=> %s::vector\n",
    "                LIMIT %s\n",
    "            ''', (query_embedding, query_embedding, top_n))\n",
    "            \n",
    "            results = cur.fetchall()\n",
    "            return [(chunk, score) for chunk, score in results]\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "\n",
    "def get_storage_backend(backend_type, config=None, table_name=None):\n",
    "    \"\"\"Factory function to get the appropriate storage backend.\n",
    "    \n",
    "    Args:\n",
    "        backend_type: 'memory', 'json', or 'postgresql'\n",
    "        config: PostgreSQL config dict (required if backend_type is 'postgresql')\n",
    "        table_name: Table name (required if backend_type is 'postgresql')\n",
    "    \n",
    "    Returns:\n",
    "        Storage backend instance\n",
    "    \"\"\"\n",
    "    if backend_type == 'postgresql':\n",
    "        if not config or not table_name:\n",
    "            raise ValueError('PostgreSQL backend requires config and table_name')\n",
    "        return PostgreSQLVectorDB(config, table_name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample_data",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's look at a few examples from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample chunks from the dataset:\\n')\n",
    "for i, chunk in enumerate(dataset[:3]):\n",
    "    print(f'--- Chunk {i+1} ---')\n",
    "    print(chunk[:300] + '...' if len(chunk) > 300 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a326fc",
   "metadata": {},
   "source": [
    "## Configure Models\n",
    "\n",
    "We'll use two models:\n",
    "- **Embedding Model**: Converts text into vector representations\n",
    "- **Language Model**: Generates responses based on retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a7821",
   "metadata": {},
   "source": [
    "## Implement the Vector Database\n",
    "\n",
    "### Indexing Phase\n",
    "\n",
    "In the indexing phase, we:\n",
    "1. Break the dataset into chunks (already done during loading)\n",
    "2. Calculate embedding vectors for each chunk\n",
    "3. Store chunks with their embeddings in PostgreSQL using pgvector\n",
    "\n",
    "The embeddings are stored in PostgreSQL with the `vector` type for efficient similarity search.\n",
    "\n",
    "The embedding is a list of floats, for example: `[0.1, 0.04, -0.34, 0.21, ...]`\n",
    "\n",
    "**Note**: If embeddings already exist in the database, you'll be prompted to either preserve or overwrite them. This prevents accidentally losing hours of embedding generation work!\n",
    "\n",
    "**Timing**: Generating embeddings may take a few minutes depending on your dataset size (approximately 1-2 minutes per 100 chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PostgreSQL storage backend\n",
    "table_name = f'embeddings_{EMBEDDING_MODEL_ALIAS.replace(\".\", \"_\")}'\n",
    "PG_DB = PostgreSQLVectorDB(POSTGRES_CONFIG, table_name, preserve_existing=PRESERVE_EXISTING_EMBEDDINGS)\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "    \"\"\"Add a chunk and its embedding to the PostgreSQL vector database.\"\"\"\n",
    "    embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "    PG_DB.insert_embedding(chunk, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df5ca8",
   "metadata": {},
   "source": [
    "Now let's populate our vector database with all chunks from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if embeddings already exist\n",
    "embedding_count = PG_DB.get_chunk_count()\n",
    "\n",
    "if embedding_count > 0:\n",
    "    print(f'‚úì Using existing {embedding_count:,} embeddings from database')\n",
    "    print(f'  Skipping embedding generation (already complete)')\n",
    "else:\n",
    "    print(f'Building vector database with {len(dataset)} chunks...')\n",
    "    print('Storing embeddings in PostgreSQL...\\n')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, chunk in enumerate(dataset):\n",
    "        add_chunk_to_database(chunk)\n",
    "        \n",
    "        # Progress update every 50 chunks\n",
    "        if (i + 1) % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i + 1) / elapsed\n",
    "            remaining = (len(dataset) - (i + 1)) / rate if rate > 0 else 0\n",
    "            print(f'Embedded {i+1}/{len(dataset)} chunks ({(i+1)/len(dataset)*100:.1f}%) - '\n",
    "                  f'~{remaining/60:.1f} min remaining')\n",
    "    \n",
    "    embedding_count = PG_DB.get_chunk_count()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'\\n‚úì Vector database ready with {embedding_count:,} embeddings in PostgreSQL!')\n",
    "    print(f'  Total time: {total_time/60:.1f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe42c7",
   "metadata": {},
   "source": [
    "## Register Embeddings in Registry\n",
    "\n",
    "Now that embeddings are generated, let's register them in the embedding registry for reuse in other notebooks. This enables the `load_or_generate()` pattern used in advanced technique notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82039878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register these embeddings in the registry for reuse\n",
    "# This enables other notebooks to discover and load these embeddings via load_or_generate()\n",
    "\n",
    "# Get embedding dimension from the database\n",
    "embedding_dimension = PG_DB.get_embedding_dimension()\n",
    "\n",
    "# Build metadata about this embedding set\n",
    "metadata = {\n",
    "    'target_size_mb': TARGET_SIZE_MB,\n",
    "    'max_chunk_size': MAX_CHUNK_SIZE,\n",
    "    'dataset_source': 'wikimedia/wikipedia (20231101.simple)',\n",
    "    'chunk_format': 'Article: {title}\\\\n\\\\n{text}',\n",
    "    'local_cache_path': LOCAL_DATASET_PATH if SAVE_LOCALLY else None,\n",
    "    'postgres_table': table_name,\n",
    "    'preserve_existing': PRESERVE_EXISTING_EMBEDDINGS\n",
    "}\n",
    "\n",
    "# Import registry utilities (inline for this notebook)\n",
    "# In production, these would be in a shared module\n",
    "# For learning purposes, we show the pattern here\n",
    "\n",
    "def register_embedding(conn, model_alias, model_name, dimension, count, \n",
    "                       source_dataset, chunk_size, metadata_json=None):\n",
    "    \"\"\"Register embedding model in the registry with metadata.\n",
    "    \n",
    "    This enables discovery and reuse across notebooks via the registry pattern.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute('''\n",
    "            INSERT INTO embedding_registry \n",
    "                (model_alias, model_name, dimension, embedding_count, \n",
    "                 chunk_source_dataset, chunk_size_config, metadata_json, \n",
    "                 created_at, last_accessed)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (model_alias) \n",
    "            DO UPDATE SET\n",
    "                embedding_count = EXCLUDED.embedding_count,\n",
    "                last_accessed = EXCLUDED.last_accessed,\n",
    "                metadata_json = EXCLUDED.metadata_json\n",
    "            RETURNING id\n",
    "        ''', (\n",
    "            model_alias,\n",
    "            model_name,\n",
    "            dimension,\n",
    "            count,\n",
    "            source_dataset,\n",
    "            chunk_size,\n",
    "            json.dumps(metadata_json) if metadata_json else None,\n",
    "            datetime.now(),\n",
    "            datetime.now()\n",
    "        ))\n",
    "        \n",
    "        registry_id = cur.fetchone()[0]\n",
    "        conn.commit()\n",
    "        return registry_id\n",
    "\n",
    "# Register the embeddings\n",
    "try:\n",
    "    registry_id = register_embedding(\n",
    "        conn=PG_DB.conn,\n",
    "        model_alias=EMBEDDING_MODEL_ALIAS,\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        dimension=embedding_dimension,\n",
    "        count=embedding_count,\n",
    "        source_dataset='wikimedia/wikipedia (20231101.simple)',\n",
    "        chunk_size=MAX_CHUNK_SIZE,\n",
    "        metadata_json=metadata\n",
    "    )\n",
    "    \n",
    "    print(f'\\n‚úì Registered embeddings in registry (ID: {registry_id})')\n",
    "    print(f'  Model alias: {EMBEDDING_MODEL_ALIAS}')\n",
    "    print(f'  Dimension: {embedding_dimension}')\n",
    "    print(f'  Count: {embedding_count:,}')\n",
    "    print(f'\\nOther notebooks can now discover and load these embeddings using:')\n",
    "    print(f'  load_or_generate(db, \"{EMBEDDING_MODEL}\", \"{EMBEDDING_MODEL_ALIAS}\", ...)')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'\\nNote: Could not register in registry (table may not exist yet).')\n",
    "    print(f'Run foundation/00-setup-postgres-schema.ipynb first to create registry tables.')\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864c65a",
   "metadata": {},
   "source": [
    "## Implement the Retrieval Function\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "To find the most relevant chunks, we need to compare vector similarity. We'll use cosine similarity, which measures how \"close\" two vectors are in the vector space. Higher cosine similarity means more similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11352532",
   "metadata": {},
   "source": [
    "### Retrieval Function\n",
    "\n",
    "The retrieval function:\n",
    "1. Converts the query into an embedding vector\n",
    "2. Compares it against all vectors in the database\n",
    "3. Returns the top N most relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99953b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    \"\"\"Retrieve the top N most relevant chunks for a given query using PostgreSQL pgvector.\"\"\"\n",
    "    query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    return PG_DB.similarity_search(query_embedding, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11d6bc",
   "metadata": {},
   "source": [
    "## Generation Phase\n",
    "\n",
    "In the generation phase, the chatbot generates a response based on the retrieved knowledge. We construct a prompt that includes the relevant chunks and instruct the model to only use that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query, top_n=3, verbose=True):\n",
    "    \"\"\"Ask a question and get a response based on retrieved knowledge.\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask\n",
    "        top_n: Number of relevant chunks to retrieve\n",
    "        verbose: Whether to print retrieved knowledge\n",
    "    \n",
    "    Returns:\n",
    "        The chatbot's response as a string\n",
    "    \"\"\"\n",
    "    # Retrieve relevant knowledge\n",
    "    retrieved_knowledge = retrieve(query, top_n=top_n)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Retrieved knowledge:')\n",
    "        for i, (chunk, similarity) in enumerate(retrieved_knowledge):\n",
    "            # Extract title from chunk\n",
    "            title_line = chunk.split('\\n')[0]\n",
    "            preview = chunk[:200].replace('\\n', ' ') + '...' if len(chunk) > 200 else chunk\n",
    "            print(f'  [{i+1}] (similarity: {similarity:.3f}) {preview}')\n",
    "        print()\n",
    "    \n",
    "    # Construct the instruction prompt with retrieved context\n",
    "    instruction_prompt = f'''You are a helpful chatbot that answers questions based on Wikipedia articles.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information.\n",
    "If the context doesn't contain enough information to answer the question, say so.\n",
    "\n",
    "Context:\n",
    "{chr(10).join([f'{i+1}. {chunk.strip()}' for i, (chunk, _) in enumerate(retrieved_knowledge)])}\n",
    "'''\n",
    "    \n",
    "    # Generate response\n",
    "    stream = ollama.chat(\n",
    "        model=LANGUAGE_MODEL,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': instruction_prompt},\n",
    "            {'role': 'user', 'content': query},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    # Collect and print the response\n",
    "    if verbose:\n",
    "        print('Chatbot response:')\n",
    "    \n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        response += content\n",
    "        if verbose:\n",
    "            print(content, end='', flush=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n')  # ensure a newline after the streamed response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988ae81",
   "metadata": {},
   "source": [
    "## Try It Out!\n",
    "\n",
    "Now let's ask some questions. The quality of answers will depend on which articles were included in your dataset sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"Tell me about Albert Einstein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"What is Python programming language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "climate_question",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"How does photosynthesis work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370c0c5",
   "metadata": {},
   "source": [
    "## Interactive Chat\n",
    "\n",
    "You can also use this cell to ask your own questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask your own question here\n",
    "your_question = \"What is the solar system?\"\n",
    "ask_question(your_question)"
   ]
  },
  {
   "cell_type": "code",
   "id": "export_cell",
   "metadata": {},
   "source": [
    "def export_embeddings(output_path='embeddings_export.json', format='generic'):\n",
    "    \"\"\"\n",
    "    Export embeddings from PostgreSQL in a generic format compatible with multiple vector databases.\n",
    "    \n",
    "    This function removes vendor lock-in by providing a standard export format\n",
    "    that works with any PostgreSQL-compatible vector database.\n",
    "    \n",
    "    Args:\n",
    "        output_path (str): Path to output JSON or SQL file\n",
    "        format (str): Export format - 'generic' (default), 'pgvector', or 'pinecone'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Export statistics (count, dimension, file_size_mb, format, path)\n",
    "    \n",
    "    Supports:\n",
    "        - PostgreSQL with pgvector (local, Neon, Supabase, RDS)\n",
    "        - Pinecone vector database\n",
    "        - Generic JSON for custom integrations\n",
    "        \n",
    "    Examples:\n",
    "        # Export as generic JSON\n",
    "        stats = export_embeddings(format='generic')\n",
    "        \n",
    "        # Export as PostgreSQL INSERT statements\n",
    "        stats = export_embeddings(format='pgvector')\n",
    "        \n",
    "        # Export for Pinecone\n",
    "        stats = export_embeddings(format='pinecone')\n",
    "    \"\"\"\n",
    "    # Get all embeddings from PostgreSQL\n",
    "    all_embeddings = PG_DB.get_all_embeddings()\n",
    "    \n",
    "    if not all_embeddings:\n",
    "        raise ValueError('No embeddings found in database. Generate embeddings first.')\n",
    "    \n",
    "    # Separate chunks and embeddings\n",
    "    chunks = [chunk for chunk, _ in all_embeddings]\n",
    "    embeddings = [emb for _, emb in all_embeddings]\n",
    "    embedding_dimension = len(embeddings[0]) if embeddings else 0\n",
    "    \n",
    "    if format == 'generic':\n",
    "        # Generic JSON format: standard structure without vendor lock-in\n",
    "        export_data = {\n",
    "            'metadata': {\n",
    "                'model': EMBEDDING_MODEL,\n",
    "                'dimension': embedding_dimension,\n",
    "                'count': len(embeddings),\n",
    "                'created_at': __import__('datetime').datetime.now().isoformat() + 'Z',\n",
    "                'format_type': 'generic',\n",
    "                'postgres_table': table_name,\n",
    "                'dataset_source': 'wikimedia/wikipedia (20231101.simple)'\n",
    "            },\n",
    "            'embeddings': [\n",
    "                {\n",
    "                    'id': f'chunk_{i}',\n",
    "                    'vector': embedding,\n",
    "                    'metadata': {\n",
    "                        'text': chunk,\n",
    "                        'source': chunk.split('\\n')[0].replace('Article: ', '') if chunk.startswith('Article: ') else 'unknown'\n",
    "                    }\n",
    "                }\n",
    "                for i, (chunk, embedding) in enumerate(zip(chunks, embeddings))\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        file_size_mb = __import__('os').path.getsize(output_path) / (1024 * 1024)\n",
    "        \n",
    "        print(f'‚úì Exported {len(embeddings)} embeddings in generic JSON format')\n",
    "        print(f'  Output: {output_path}')\n",
    "        print(f'  File size: {file_size_mb:.2f} MB')\n",
    "        print(f'  Dimension: {embedding_dimension}')\n",
    "        print(f'\\nThis format works with:')\n",
    "        print(f'  - PostgreSQL with pgvector (via JSON import)')\n",
    "        print(f'  - Neon PostgreSQL')\n",
    "        print(f'  - Supabase (PostgreSQL + pgvector)')\n",
    "        print(f'  - AWS RDS with pgvector')\n",
    "        print(f'  - Custom vector database integrations')\n",
    "        \n",
    "    elif format == 'pgvector':\n",
    "        # PostgreSQL pgvector format: SQL INSERT statements\n",
    "        sql_lines = ['-- PostgreSQL pgvector export', '-- Insert into embeddings table', '']\n",
    "        \n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            # Extract title for metadata\n",
    "            title = chunk.split('\\n')[0].replace('Article: ', '') if chunk.startswith('Article: ') else 'unknown'\n",
    "            \n",
    "            # Escape single quotes in text and title\n",
    "            safe_chunk = chunk.replace(\"'\", \"''\")\n",
    "            safe_title = title.replace(\"'\", \"''\")\n",
    "            \n",
    "            # Format embedding as PostgreSQL vector\n",
    "            vector_str = '[' + ','.join(str(v) for v in embedding) + ']'\n",
    "            \n",
    "            # Build INSERT statement\n",
    "            sql = f\"INSERT INTO embeddings (chunk_id, chunk_text, embedding, source) VALUES ('{i}', E'{safe_chunk}', '{vector_str}'::vector, '{safe_title}');\"\n",
    "            sql_lines.append(sql)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sql_lines))\n",
    "        \n",
    "        file_size_mb = __import__('os').path.getsize(output_path) / (1024 * 1024)\n",
    "        \n",
    "        print(f'‚úì Exported {len(embeddings)} embeddings as PostgreSQL INSERT statements')\n",
    "        print(f'  Output: {output_path}')\n",
    "        print(f'  File size: {file_size_mb:.2f} MB')\n",
    "        print(f'\\nTo import into PostgreSQL:')\n",
    "        print(f'  psql -U postgres -d your_database -f {output_path}')\n",
    "        print(f'\\nWorks with:')\n",
    "        print(f'  - Local PostgreSQL + pgvector')\n",
    "        print(f'  - Neon PostgreSQL')\n",
    "        print(f'  - Supabase')\n",
    "        print(f'  - AWS RDS with pgvector')\n",
    "        \n",
    "    elif format == 'pinecone':\n",
    "        # Pinecone format: vectors with metadata\n",
    "        export_data = {\n",
    "            'vectors': [\n",
    "                {\n",
    "                    'id': f'chunk_{i}',\n",
    "                    'values': embedding,\n",
    "                    'metadata': {\n",
    "                        'text': chunk,\n",
    "                        'source': chunk.split('\\n')[0].replace('Article: ', '') if chunk.startswith('Article: ') else 'unknown'\n",
    "                    }\n",
    "                }\n",
    "                for i, (chunk, embedding) in enumerate(zip(chunks, embeddings))\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        file_size_mb = __import__('os').path.getsize(output_path) / (1024 * 1024)\n",
    "        \n",
    "        print(f'‚úì Exported {len(embeddings)} embeddings in Pinecone format')\n",
    "        print(f'  Output: {output_path}')\n",
    "        print(f'  File size: {file_size_mb:.2f} MB')\n",
    "        print(f'\\nTo import into Pinecone:')\n",
    "        print(f'  1. Parse the JSON file')\n",
    "        print(f'  2. Use Pinecone upsert API to insert vectors')\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown format '{format}'. Use 'generic', 'pgvector', or 'pinecone'\")\n",
    "    \n",
    "    return {\n",
    "        'count': len(embeddings),\n",
    "        'dimension': embedding_dimension,\n",
    "        'file_size_mb': file_size_mb,\n",
    "        'format': format,\n",
    "        'path': output_path\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Generic export (recommended for portability)\n",
    "# stats = export_embeddings(format='generic')\n",
    "# print(f\"Exported {stats['count']} embeddings to {stats['path']}\")\n",
    "\n",
    "# PostgreSQL export\n",
    "# stats = export_embeddings(format='pgvector')\n",
    "\n",
    "# Pinecone export\n",
    "# stats = export_embeddings(format='pinecone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77f5ee",
   "metadata": {},
   "source": [
    "## Load Embeddings from PostgreSQL\n",
    "\n",
    "If you've previously generated embeddings and stored them in PostgreSQL, you can load them without regenerating:\n",
    "\n",
    "**Use this in a new notebook to:**\n",
    "- Run experiments with existing embeddings (avoiding 50+ minute regeneration)\n",
    "- Compare different embedding models stored in different tables\n",
    "- Analyze embedding quality without reprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ef9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_from_postgres(config, embedding_model_alias):\n",
    "    \"\"\"Load previously generated embeddings from PostgreSQL.\n",
    "    \n",
    "    Useful for running new experiments without regenerating embeddings.\n",
    "    \n",
    "    Args:\n",
    "        config: PostgreSQL connection config\n",
    "        embedding_model_alias: Alias used when the embeddings were generated\n",
    "    \n",
    "    Returns:\n",
    "        PostgreSQLVectorDB instance ready for retrieval\n",
    "    \"\"\"\n",
    "    table_name = f'embeddings_{embedding_model_alias.replace(\".\", \"_\")}'\n",
    "    \n",
    "    try:\n",
    "        db = PostgreSQLVectorDB(config, table_name)\n",
    "        count = db.get_chunk_count()\n",
    "        print(f'‚úì Loaded {count} embeddings from table \"{table_name}\"')\n",
    "        return db\n",
    "    except psycopg2.ProgrammingError:\n",
    "        print(f'‚úó Table \"{table_name}\" not found in database')\n",
    "        print('Run the main notebook first to generate and store embeddings.')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'‚úó Error loading embeddings: {e}')\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example: Uncomment to load existing embeddings in a new notebook\n",
    "# loaded_db = load_embeddings_from_postgres(POSTGRES_CONFIG, 'bge_base_en_v1.5')\n",
    "# Then use: loaded_db.similarity_search(query_embedding, top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2d324",
   "metadata": {},
   "source": [
    "## Advanced RAG Improvements\n",
    "\n",
    "### 1. Hybrid Search\n",
    "Combine vector similarity with BM25 keyword search for better coverage:\n",
    "- Use PostgreSQL full-text search alongside pgvector\n",
    "- Implement reciprocal rank fusion (RRF) to merge results\n",
    "- Example: Fall back to BM25 when vector similarity confidence is low\n",
    "\n",
    "### 2. Reranking\n",
    "Add a secondary ranking step to improve result quality:\n",
    "- Use [cross-encoder models](https://www.pinecone.io/learn/series/rag/rerankers/) for relevance scoring\n",
    "- Re-score top 10-20 results from initial vector retrieval\n",
    "- Example: Use a lightweight reranker before sending to the LLM\n",
    "\n",
    "### 3. Query Expansion\n",
    "Generate multiple query variations to improve recall:\n",
    "- Use the LLM to create semantically related questions\n",
    "- Retrieve for each variant and merge results\n",
    "- Helps catch relevant content with different phrasing\n",
    "\n",
    "### 4. Metadata Filtering\n",
    "Add structured filters to your retrieval:\n",
    "```sql\n",
    "SELECT * FROM embeddings_bge_base_en_v1_5\n",
    "WHERE category = 'science'\n",
    "ORDER BY embedding <=> query_embedding\n",
    "LIMIT 5;\n",
    "```\n",
    "- Filter by article category, date, source, or other metadata\n",
    "- Enables domain-specific or time-scoped searches\n",
    "\n",
    "### 5. Better Chunking Strategies\n",
    "Improve how you split documents:\n",
    "- **Semantic chunking**: Split by meaning rather than just character count\n",
    "- **Overlapping chunks**: Include context boundaries for better retrieval\n",
    "- **Parent-child chunks**: Retrieve child chunks, but return parent context\n",
    "\n",
    "### 6. Citation and Source Tracking\n",
    "Make your RAG system more transparent:\n",
    "- Store source URLs or document IDs with each chunk\n",
    "- Return citations alongside generated answers\n",
    "- Implement confidence scores based on retrieval similarity\n",
    "\n",
    "## Advanced RAG Architectures\n",
    "\n",
    "- **Graph RAG**: Build knowledge graphs from document links and relationships\n",
    "- **Agentic RAG**: Let the LLM decide when and what to retrieve dynamically\n",
    "- **Multi-hop RAG**: Follow reasoning chains across related documents for complex questions\n",
    "- **RAG Fusion**: Combine multiple retrieval strategies (dense + sparse + semantic)\n",
    "\n",
    "## Performance and Scale\n",
    "\n",
    "- **Batch Processing**: Use `insert_batch()` for faster embeddings indexing\n",
    "- **Connection Pooling**: Use `pgbouncer` to manage concurrent database connections efficiently\n",
    "- **Caching**: Cache frequently asked questions with Redis to avoid redundant retrievals\n",
    "- **Monitoring**: Track query latency, retrieval quality, and generation accuracy\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "Measure and improve your RAG system quality:\n",
    "\n",
    "### Retrieval Metrics\n",
    "- **Precision@K**: What percentage of top-K results are relevant?\n",
    "- **Recall@K**: What percentage of all relevant documents appear in top-K?\n",
    "- **Mean Reciprocal Rank (MRR)**: How high is the rank of the first relevant result?\n",
    "- **Normalized Discounted Cumulative Gain (NDCG)**: How well-ranked are the relevant results?\n",
    "\n",
    "### Generation Metrics\n",
    "- **BLEU/ROUGE**: Compare against ground truth if available\n",
    "- **LLM-as-judge**: Use another LLM to score relevance and helpfulness\n",
    "- **Factual consistency**: Check if generated answers contradict the source material\n",
    "- **Latency**: Track end-to-end response time\n",
    "- **Token efficiency**: Monitor input/output token usage and cost\n",
    "\n",
    "## Resources for Deeper Learning\n",
    "\n",
    "- [HuggingFace RAG Guide](https://huggingface.co/blog/ngxson/make-your-own-rag)\n",
    "- [Pinecone Learning Center](https://www.pinecone.io/learn/)\n",
    "- [pgvector Documentation](https://github.com/pgvector/pgvector)\n",
    "- Advanced Papers: REALM, DPR, ColBERT, and Fusion-in-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_section",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "View statistics about your loaded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats():\n",
    "    \"\"\"Print statistics about the current dataset.\"\"\"\n",
    "    total_chars = sum(len(chunk) for chunk in dataset)\n",
    "    avg_chunk_size = total_chars / len(dataset) if dataset else 0\n",
    "    \n",
    "    # Count unique articles\n",
    "    articles = set()\n",
    "    for chunk in dataset:\n",
    "        if chunk.startswith('Article: '):\n",
    "            title = chunk.split('\\n')[0].replace('Article: ', '')\n",
    "            articles.add(title)\n",
    "    \n",
    "    print('Dataset Statistics:')\n",
    "    print(f'  Total chunks: {len(dataset):,}')\n",
    "    print(f'  Unique articles: {len(articles):,}')\n",
    "    print(f'  Total characters: {total_chars:,}')\n",
    "    print(f'  Average chunk size: {avg_chunk_size:.0f} characters')\n",
    "    print(f'  Estimated size: {sys.getsizeof(str(dataset)) / (1024*1024):.2f} MB')\n",
    "    print(f'\\n  Embeddings in database: {PG_DB.get_chunk_count():,}')\n",
    "    print(f'  Embedding dimension: {PG_DB.get_embedding_dimension()}')\n",
    "\n",
    "print_dataset_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}