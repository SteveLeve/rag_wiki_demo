{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9efe2f3c",
      "metadata": {},
      "source": [
        "# Intermediate 04: Comparing Embedding Models\n",
        "\n",
        "Learn how to objectively compare different embedding models using retrieval quality metrics.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- **Discover** multiple embedding models stored in the registry\n",
        "- **Load and retrieve** from multiple models in parallel\n",
        "- **Compute metrics** to measure retrieval quality (Precision, Recall, MRR, NDCG)\n",
        "- **Visualize** comparisons with charts and tables\n",
        "- **Analyze trade-offs** and make recommendations\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. Run `foundation/02-rag-postgresql-persistent.ipynb` to generate and store embeddings for the **base model**\n",
        "2. (Optional) For comparison with a second model:\n",
        "   - Install: `ollama pull hf.co/CompendiumLabs/bge-small-en-v1.5-gguf`\n",
        "   - Edit foundation/02 to use the second model\n",
        "   - Run foundation/02 again to generate embeddings for the second model\n",
        "   - Return here to compare!\n",
        "\n",
        "## Learning Outcomes\n",
        "\n",
        "By the end of this notebook, you'll understand:\n",
        "\n",
        "- How embedding models differ in retrieval quality\n",
        "- How to measure quality using information retrieval metrics\n",
        "- Speed vs. quality trade-offs in embedding selection\n",
        "- How to make data-driven recommendations for model selection\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup_md",
      "metadata": {},
      "source": [
        "## Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5ce697",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ollama\n",
        "import psycopg2\n",
        "import psycopg2.extras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Set\n",
        "from collections import defaultdict\n",
        "\n",
        "# Configure visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "POSTGRES_CONFIG = {\n",
        "    'host': 'localhost',\n",
        "    'port': 5432,\n",
        "    'database': 'rag_db',\n",
        "    'user': 'postgres',\n",
        "    'password': 'postgres',\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72df21d3",
      "metadata": {},
      "source": [
        "## Part 1: List Available Models\n",
        "\n",
        "**What we're doing:** Query the embedding registry to find all available embedding models.\n",
        "\n",
        "The registry table stores metadata about every embedding model that has been generated:\n",
        "- `model_alias`: Short identifier (e.g., 'bge_base_en_v1_5')\n",
        "- `model_name`: Full model name (e.g., 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf')\n",
        "- `dimension`: Vector size (e.g., 768 or 384)\n",
        "- `embedding_count`: Number of chunks stored\n",
        "- `created_at`: When embeddings were generated\n",
        "\n",
        "We'll format this as a clean pandas DataFrame for easy reading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8194cb70",
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_available_models(postgres_config: Dict) -> Tuple[pd.DataFrame, psycopg2.extensions.connection]:\n",
        "    \"\"\"Query embedding_registry to discover all available embedding models.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (DataFrame with models, PostgreSQL connection)\n",
        "    \"\"\"\n",
        "    conn = psycopg2.connect(\n",
        "        host=postgres_config['host'],\n",
        "        port=postgres_config['port'],\n",
        "        database=postgres_config['database'],\n",
        "        user=postgres_config['user'],\n",
        "        password=postgres_config['password']\n",
        "    )\n",
        "    \n",
        "    query = '''\n",
        "        SELECT \n",
        "            model_alias, \n",
        "            model_name, \n",
        "            dimension, \n",
        "            embedding_count, \n",
        "            created_at,\n",
        "            chunk_size_config\n",
        "        FROM embedding_registry\n",
        "        ORDER BY created_at DESC\n",
        "    '''\n",
        "    \n",
        "    available = pd.read_sql(query, conn)\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"AVAILABLE EMBEDDING MODELS IN REGISTRY\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if available.empty:\n",
        "        print(\"\\n⚠️  No embedding models found in registry!\")\n",
        "        print(\"\\nTo populate the registry:\")\n",
        "        print(\"  1. Run foundation/02-rag-postgresql-persistent.ipynb\")\n",
        "        print(\"  2. For a second model for comparison:\")\n",
        "        print(\"     - ollama pull hf.co/CompendiumLabs/bge-small-en-v1.5-gguf\")\n",
        "        print(\"     - Edit foundation/02 to use the new model\")\n",
        "        print(\"     - Run foundation/02 again\")\n",
        "    else:\n",
        "        print()\n",
        "        print(available.to_string(index=False))\n",
        "        print(f\"\\nTotal: {len(available)} model(s) available\")\n",
        "    \n",
        "    print()\n",
        "    return available, conn\n",
        "\n",
        "# Discover available models\n",
        "available_models, conn = list_available_models(POSTGRES_CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part1_check",
      "metadata": {},
      "source": [
        "### Check for Sufficient Models\n",
        "\n",
        "For meaningful comparison, we need at least 2 embedding models. If you only have 1, the instructional text above shows how to generate a second model.\n",
        "\n",
        "For now, we'll proceed with whatever models are available (analysis will note if only 1 model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check_models",
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(available_models) == 0:\n",
        "    print(\"✗ STOP: No embedding models found. Run foundation/02 first.\")\n",
        "elif len(available_models) == 1:\n",
        "    print(f\"⚠️  Only 1 model available: {available_models.iloc[0]['model_alias']}\")\n",
        "    print(\"   For meaningful comparison, generate a second model (see instructions above)\")\n",
        "else:\n",
        "    print(f\"✓ {len(available_models)} models available for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ed4b73",
      "metadata": {},
      "source": [
        "## Part 2: Load and Compare Retrieval Results\n",
        "\n",
        "**What we're doing:** For each embedding model in the registry:\n",
        "1. Load the model's embeddings from PostgreSQL\n",
        "2. Define a set of test queries\n",
        "3. Retrieve top-5 chunks for each query\n",
        "4. Store results for metric computation\n",
        "\n",
        "### Create the Vector Database Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "182c356d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PostgreSQLVectorDB:\n",
        "    \"\"\"Helper class to query embeddings from PostgreSQL using pgvector.\n",
        "    \n",
        "    This class provides similarity search on pre-computed embeddings stored in\n",
        "    a PostgreSQL table with pgvector extension.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict, table_name: str):\n",
        "        \"\"\"Initialize database connection.\n",
        "        \n",
        "        Args:\n",
        "            config: PostgreSQL config dict with host, port, database, user, password\n",
        "            table_name: Name of table storing embeddings\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.table_name = table_name\n",
        "        self.conn = psycopg2.connect(\n",
        "            host=config['host'],\n",
        "            port=config['port'],\n",
        "            database=config['database'],\n",
        "            user=config['user'],\n",
        "            password=config['password']\n",
        "        )\n",
        "    \n",
        "    def get_chunk_count(self) -> int:\n",
        "        \"\"\"Get total number of chunks stored.\n",
        "        \n",
        "        Returns:\n",
        "            Integer count of embeddings in table\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n",
        "            return cur.fetchone()[0]\n",
        "    \n",
        "    def similarity_search(self, query_embedding: List[float], top_n: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Find most similar chunks using cosine similarity (pgvector).\n",
        "        \n",
        "        Args:\n",
        "            query_embedding: Query vector (list of floats)\n",
        "            top_n: Number of results to return\n",
        "        \n",
        "        Returns:\n",
        "            List of (chunk_text, similarity_score) tuples, sorted by similarity descending\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(f'''\n",
        "                SELECT chunk_text, 1 - (embedding <=> %s::vector) as similarity\n",
        "                FROM {self.table_name}\n",
        "                ORDER BY embedding <=> %s::vector\n",
        "                LIMIT %s\n",
        "            ''', (query_embedding, query_embedding, top_n))\n",
        "            return [(chunk, float(score)) for chunk, score in cur.fetchall()]\n",
        "    \n",
        "    def get_chunk_ids_for_similarity_search(self, query_embedding: List[float], top_n: int = 5) -> List[int]:\n",
        "        \"\"\"Find most similar chunks and return their IDs (for metrics computation).\n",
        "        \n",
        "        Args:\n",
        "            query_embedding: Query vector\n",
        "            top_n: Number of results\n",
        "        \n",
        "        Returns:\n",
        "            List of chunk IDs in order of similarity\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(f'''\n",
        "                SELECT id\n",
        "                FROM {self.table_name}\n",
        "                ORDER BY embedding <=> %s::vector\n",
        "                LIMIT %s\n",
        "            ''', (query_embedding, top_n))\n",
        "            return [row[0] for row in cur.fetchall()]\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Close database connection.\"\"\"\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "\n",
        "print(\"Vector DB class loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test_queries_md",
      "metadata": {},
      "source": [
        "### Define Test Queries\n",
        "\n",
        "These are representative questions that we'll use to test both models.\n",
        "Good test queries are:\n",
        "- **Diverse**: Cover different topics (science, history, geography, etc.)\n",
        "- **Specific enough**: Not too vague (\"What is X?\" not \"Tell me about things\")\n",
        "- **Real use cases**: Questions users would actually ask\n",
        "\n",
        "The quality of comparison depends on representative test queries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test_queries",
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_QUERIES = [\n",
        "    \"What is photosynthesis?\",\n",
        "    \"When was World War 2?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"How does the human heart work?\",\n",
        "    \"What is quantum mechanics?\",\n",
        "    \"Who invented the telephone?\",\n",
        "    \"What is climate change?\",\n",
        "    \"Where is Mount Everest?\",\n",
        "    \"What is the Roman Empire?\",\n",
        "    \"How does DNA work?\",\n",
        "]\n",
        "\n",
        "print(f\"Test Query Set: {len(TEST_QUERIES)} queries\")\n",
        "print(\"\\nQueries:\")\n",
        "for i, query in enumerate(TEST_QUERIES, 1):\n",
        "    print(f\"  {i:2}. {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "retrieve_md",
      "metadata": {},
      "source": [
        "### Retrieve from All Models\n",
        "\n",
        "For each model in the registry:\n",
        "1. Create a PostgreSQLVectorDB instance pointing to that model's table\n",
        "2. For each test query:\n",
        "   - Generate query embedding\n",
        "   - Retrieve top-5 chunks\n",
        "   - Store results for metric computation\n",
        "\n",
        "We track:\n",
        "- Chunks retrieved (text and score)\n",
        "- Chunk IDs for later relevance labeling\n",
        "- Query latency (time to compute embedding + retrieve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "retrieve_results",
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_from_all_models(\n",
        "    available_models: pd.DataFrame,\n",
        "    postgres_config: Dict,\n",
        "    test_queries: List[str],\n",
        "    top_n: int = 5\n",
        ") -> Tuple[Dict, Dict]:\n",
        "    \"\"\"Retrieve top-N chunks from all available models for all test queries.\n",
        "    \n",
        "    Args:\n",
        "        available_models: DataFrame from list_available_models()\n",
        "        postgres_config: PostgreSQL connection config\n",
        "        test_queries: List of test queries\n",
        "        top_n: Number of chunks to retrieve per query\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of:\n",
        "        - results: Dict[model_alias][query] = [(chunk_text, score), ...]\n",
        "        - latencies: Dict[model_alias][query] = elapsed_time_ms\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    latencies = defaultdict(dict)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"RETRIEVING FROM ALL MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for _, row in available_models.iterrows():\n",
        "        model_alias = row['model_alias']\n",
        "        model_name = row['model_name']\n",
        "        \n",
        "        # Connect to this model's table\n",
        "        table_name = f\"embeddings_{model_alias.replace('.', '_')}\"\n",
        "        \n",
        "        try:\n",
        "            db = PostgreSQLVectorDB(postgres_config, table_name)\n",
        "            chunk_count = db.get_chunk_count()\n",
        "            \n",
        "            print(f\"\\nModel: {model_alias}\")\n",
        "            print(f\"  Table: {table_name}\")\n",
        "            print(f\"  Chunks: {chunk_count:,}\")\n",
        "            print(f\"  Retrieving from {len(test_queries)} queries...\")\n",
        "            \n",
        "            results[model_alias] = {}\n",
        "            \n",
        "            for query in test_queries:\n",
        "                # Time the retrieval (embedding + search)\n",
        "                start = time.time()\n",
        "                \n",
        "                # Generate query embedding\n",
        "                query_embedding = ollama.embed(model=model_name, input=query)['embeddings'][0]\n",
        "                \n",
        "                # Retrieve top-N chunks\n",
        "                retrieved = db.similarity_search(query_embedding, top_n=top_n)\n",
        "                \n",
        "                elapsed_ms = (time.time() - start) * 1000\n",
        "                latencies[model_alias][query] = elapsed_ms\n",
        "                \n",
        "                # Store results\n",
        "                results[model_alias][query] = retrieved\n",
        "            \n",
        "            db.close()\n",
        "            \n",
        "            # Summary for this model\n",
        "            avg_latency = np.mean(list(latencies[model_alias].values()))\n",
        "            print(f\"  ✓ Complete (avg latency: {avg_latency:.0f}ms)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error loading model: {e}\")\n",
        "    \n",
        "    return results, dict(latencies)\n",
        "\n",
        "# Retrieve from all models\n",
        "if len(available_models) > 0:\n",
        "    retrieval_results, retrieval_latencies = retrieve_from_all_models(\n",
        "        available_models,\n",
        "        POSTGRES_CONFIG,\n",
        "        TEST_QUERIES,\n",
        "        top_n=5\n",
        "    )\n",
        "    print(f\"\\n✓ Retrieved results from {len(retrieval_results)} model(s)\")\n",
        "else:\n",
        "    retrieval_results = {}\n",
        "    retrieval_latencies = {}\n",
        "    print(\"⚠️  Skipping retrieval (no models available)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part2_sample",
      "metadata": {},
      "source": [
        "### View Sample Results\n",
        "\n",
        "Let's look at what one model retrieved for a sample query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "view_sample",
      "metadata": {},
      "outputs": [],
      "source": [
        "if retrieval_results:\n",
        "    sample_model = list(retrieval_results.keys())[0]\n",
        "    sample_query = TEST_QUERIES[0]\n",
        "    \n",
        "    print(f\"Sample: Model '{sample_model}' retrieving for query '{sample_query}'\")\n",
        "    print()\n",
        "    \n",
        "    retrieved = retrieval_results[sample_model][sample_query]\n",
        "    for i, (chunk, score) in enumerate(retrieved, 1):\n",
        "        # Extract title from chunk\n",
        "        lines = chunk.split('\\n')\n",
        "        title = lines[0] if lines[0].startswith('Article:') else 'Unknown'\n",
        "        \n",
        "        # Show preview\n",
        "        preview = chunk[:250].replace('\\n', ' ') + '...'\n",
        "        print(f\"[{i}] Similarity: {score:.4f}\")\n",
        "        print(f\"    {title}\")\n",
        "        print(f\"    {preview}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d73e62",
      "metadata": {},
      "source": [
        "## Part 3: Compute Comparison Metrics\n",
        "\n",
        "**What we're doing:** Define metric functions to measure retrieval quality.\n",
        "\n",
        "### Information Retrieval Metrics\n",
        "\n",
        "Since we don't have ground truth relevance labels yet, we'll use **overlap-based evaluation**:\n",
        "- Assume chunks that appear in multiple models' results are likely relevant\n",
        "- Measure **coverage**: Do both models retrieve similar relevant chunks?\n",
        "- Measure **ranking quality**: Does each model rank relevant chunks high?\n",
        "\n",
        "In a production system, you would:\n",
        "1. Have human annotators label which chunks are relevant to each query\n",
        "2. Use those labels to compute true precision/recall\n",
        "3. Compare against that ground truth\n",
        "\n",
        "### Metric Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metric_functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_precision_at_k(\n",
        "    retrieved_ids: List[int],\n",
        "    relevant_ids: Set[int],\n",
        "    k: int = 5\n",
        ") -> float:\n",
        "    \"\"\"Compute Precision@K: what % of top-K results are relevant?\n",
        "    \n",
        "    Formula: |{relevant results in top-K}| / K\n",
        "    \n",
        "    Interpretation:\n",
        "    - Precision = 0.8 means 80% of top-5 results are relevant\n",
        "    - Good for: \"How many of the results I saw were useful?\"\n",
        "    \n",
        "    Args:\n",
        "        retrieved_ids: List of chunk IDs returned by retrieval (in order)\n",
        "        relevant_ids: Set of chunk IDs that are actually relevant\n",
        "        k: Top-K threshold\n",
        "    \n",
        "    Returns:\n",
        "        Float between 0 and 1\n",
        "    \"\"\"\n",
        "    retrieved_k = retrieved_ids[:k]\n",
        "    matches = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_ids)\n",
        "    return matches / k if k > 0 else 0.0\n",
        "\n",
        "\n",
        "def compute_recall_at_k(\n",
        "    retrieved_ids: List[int],\n",
        "    relevant_ids: Set[int],\n",
        "    k: int = 5\n",
        ") -> float:\n",
        "    \"\"\"Compute Recall@K: what % of all relevant chunks did we find?\n",
        "    \n",
        "    Formula: |{relevant results in top-K}| / |all relevant items|\n",
        "    \n",
        "    Interpretation:\n",
        "    - Recall = 0.6 means we found 60% of all relevant chunks in top-5\n",
        "    - Good for: \"Did I find all the useful results?\"\n",
        "    \n",
        "    Args:\n",
        "        retrieved_ids: List of chunk IDs returned by retrieval\n",
        "        relevant_ids: Set of chunk IDs that are actually relevant\n",
        "        k: Top-K threshold\n",
        "    \n",
        "    Returns:\n",
        "        Float between 0 and 1 (or 0 if no relevant items exist)\n",
        "    \"\"\"\n",
        "    if len(relevant_ids) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    retrieved_k = retrieved_ids[:k]\n",
        "    matches = sum(1 for chunk_id in retrieved_k if chunk_id in relevant_ids)\n",
        "    return matches / len(relevant_ids)\n",
        "\n",
        "\n",
        "def compute_mrr(\n",
        "    retrieved_ids: List[int],\n",
        "    relevant_ids: Set[int]\n",
        ") -> float:\n",
        "    \"\"\"Compute Mean Reciprocal Rank: where is the first relevant result?\n",
        "    \n",
        "    Formula: 1 / (position of first relevant result)\n",
        "    \n",
        "    Interpretation:\n",
        "    - MRR = 1.0 means first result is relevant (position 1, 1/1 = 1.0)\n",
        "    - MRR = 0.5 means first relevant result is at position 2 (1/2 = 0.5)\n",
        "    - MRR = 0.0 means no relevant result found\n",
        "    - Good for: \"How quickly did I find a relevant result?\"\n",
        "    \n",
        "    Args:\n",
        "        retrieved_ids: List of chunk IDs returned by retrieval\n",
        "        relevant_ids: Set of chunk IDs that are actually relevant\n",
        "    \n",
        "    Returns:\n",
        "        Float between 0 and 1\n",
        "    \"\"\"\n",
        "    for position, chunk_id in enumerate(retrieved_ids, 1):\n",
        "        if chunk_id in relevant_ids:\n",
        "            return 1.0 / position\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def compute_ndcg_at_k(\n",
        "    retrieved_ids: List[int],\n",
        "    relevant_ids: Set[int],\n",
        "    k: int = 5\n",
        ") -> float:\n",
        "    \"\"\"Compute Normalized Discounted Cumulative Gain: ranking quality metric.\n",
        "    \n",
        "    NDCG measures how well-ranked the results are:\n",
        "    - Relevant items should appear earlier (higher discount for lower positions)\n",
        "    - Normalized against an ideal ranking (best possible score = 1.0)\n",
        "    \n",
        "    Formula:\n",
        "    - DCG = sum of (relevance / log2(position + 1)) for each result\n",
        "    - IDCG = DCG of perfect ranking (all relevant items first)\n",
        "    - NDCG = DCG / IDCG\n",
        "    \n",
        "    Interpretation:\n",
        "    - NDCG = 0.9 means ranking is 90% as good as ideal\n",
        "    - Penalizes relevant items appearing low in ranking more than Precision/Recall\n",
        "    - Good for: \"How well-ordered are the results?\"\n",
        "    \n",
        "    Args:\n",
        "        retrieved_ids: List of chunk IDs returned by retrieval\n",
        "        relevant_ids: Set of chunk IDs that are actually relevant\n",
        "        k: Top-K threshold\n",
        "    \n",
        "    Returns:\n",
        "        Float between 0 and 1\n",
        "    \"\"\"\n",
        "    def dcg_score(relevances: List[int]) -> float:\n",
        "        \"\"\"Compute Discounted Cumulative Gain.\"\"\"\n",
        "        return sum((rel) / math.log2(i + 2) for i, rel in enumerate(relevances))\n",
        "    \n",
        "    # Get top-K results\n",
        "    retrieved_k = retrieved_ids[:k]\n",
        "    \n",
        "    # Binary relevance: 1 if relevant, 0 if not\n",
        "    relevances = [1 if chunk_id in relevant_ids else 0 for chunk_id in retrieved_k]\n",
        "    \n",
        "    # Compute actual DCG\n",
        "    dcg = dcg_score(relevances)\n",
        "    \n",
        "    # Compute ideal DCG (perfect ranking: all relevant items first)\n",
        "    ideal_relevances = sorted(relevances, reverse=True)\n",
        "    idcg = dcg_score(ideal_relevances)\n",
        "    \n",
        "    # Normalize\n",
        "    if idcg == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    return dcg / idcg\n",
        "\n",
        "\n",
        "def compute_overlap_based_metrics(\n",
        "    retrieval_results: Dict,\n",
        "    top_n: int = 5,\n",
        "    min_model_agreement: int = 2\n",
        ") -> Dict:\n",
        "    \"\"\"Compute metrics using overlap-based relevance (chunks retrieved by multiple models).\n",
        "    \n",
        "    Since we don't have ground truth labels, we assume:\n",
        "    - Chunks retrieved by 2+ models are likely relevant (high overlap = high quality)\n",
        "    - Chunks retrieved by only 1 model are less reliable\n",
        "    \n",
        "    Args:\n",
        "        retrieval_results: Dict[model][query] = [(chunk_text, score), ...]\n",
        "        top_n: Top-N threshold for metrics\n",
        "        min_model_agreement: Minimum number of models that must retrieve a chunk\n",
        "    \n",
        "    Returns:\n",
        "        Dict of computed metrics per model per query\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # Only relevant if we have multiple models\n",
        "    if len(retrieval_results) < 2:\n",
        "        print(\"⚠️  Overlap-based metrics require 2+ models. Using alternative evaluation...\")\n",
        "        # Single model: use keyword-based relevance instead\n",
        "        return compute_keyword_based_metrics(retrieval_results)\n",
        "    \n",
        "    # For each query, find chunks retrieved by multiple models\n",
        "    for query_idx, query in enumerate(TEST_QUERIES):\n",
        "        # Collect chunk texts per model\n",
        "        chunks_by_model = {}\n",
        "        for model_alias in retrieval_results:\n",
        "            retrieved = retrieval_results[model_alias].get(query, [])\n",
        "            chunks_by_model[model_alias] = [chunk_text for chunk_text, _ in retrieved]\n",
        "        \n",
        "        # Find overlap: chunks retrieved by 2+ models\n",
        "        relevant_chunks = set()\n",
        "        chunk_counts = {}\n",
        "        \n",
        "        for model_alias, chunks in chunks_by_model.items():\n",
        "            for chunk in chunks:\n",
        "                chunk_counts[chunk] = chunk_counts.get(chunk, 0) + 1\n",
        "        \n",
        "        # Mark chunks as relevant if retrieved by min_model_agreement or more models\n",
        "        for chunk, count in chunk_counts.items():\n",
        "            if count >= min_model_agreement:\n",
        "                relevant_chunks.add(chunk)\n",
        "        \n",
        "        # Compute metrics for each model\n",
        "        for model_alias in retrieval_results:\n",
        "            if query not in metrics:\n",
        "                metrics[query] = {}\n",
        "            \n",
        "            retrieved = retrieval_results[model_alias].get(query, [])\n",
        "            retrieved_chunks = [chunk_text for chunk_text, _ in retrieved]\n",
        "            \n",
        "            # Count matches\n",
        "            matches = sum(1 for chunk in retrieved_chunks if chunk in relevant_chunks)\n",
        "            \n",
        "            # Compute metrics\n",
        "            precision = matches / top_n if top_n > 0 else 0.0\n",
        "            recall = matches / len(relevant_chunks) if len(relevant_chunks) > 0 else 0.0\n",
        "            \n",
        "            if model_alias not in metrics[query]:\n",
        "                metrics[query][model_alias] = {}\n",
        "            \n",
        "            metrics[query][model_alias] = {\n",
        "                'precision@5': precision,\n",
        "                'recall@5': recall,\n",
        "                'overlap': matches\n",
        "            }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "def compute_keyword_based_metrics(retrieval_results: Dict) -> Dict:\n",
        "    \"\"\"Compute metrics using simple keyword matching as proxy for relevance.\n",
        "    \n",
        "    This is a fallback when we have only 1 model. We measure relevance as:\n",
        "    - Does the retrieved chunk contain keywords from the query?\n",
        "    - Is it from a specific/authoritative source?\n",
        "    \n",
        "    Args:\n",
        "        retrieval_results: Dict[model][query] = [(chunk_text, score), ...]\n",
        "    \n",
        "    Returns:\n",
        "        Dict of metrics per model per query\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    for query in TEST_QUERIES:\n",
        "        query_words = set(query.lower().split())\n",
        "        metrics[query] = {}\n",
        "        \n",
        "        for model_alias in retrieval_results:\n",
        "            retrieved = retrieval_results[model_alias].get(query, [])\n",
        "            \n",
        "            # Count chunks with query keyword matches\n",
        "            matches = 0\n",
        "            for chunk_text, score in retrieved:\n",
        "                chunk_lower = chunk_text.lower()\n",
        "                # Check if any query keyword appears in chunk\n",
        "                if any(word in chunk_lower for word in query_words):\n",
        "                    matches += 1\n",
        "            \n",
        "            precision = matches / len(retrieved) if len(retrieved) > 0 else 0.0\n",
        "            \n",
        "            metrics[query][model_alias] = {\n",
        "                'precision@5': precision,\n",
        "                'recall@5': precision,\n",
        "                'similarity_avg': np.mean([score for _, score in retrieved])\n",
        "            }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "print(\"Metric functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "compute_metrics_md",
      "metadata": {},
      "source": [
        "### Compute Metrics for All Models and Queries\n",
        "\n",
        "Now we'll compute metrics using overlap-based relevance (for 2+ models) or keyword-based (for 1 model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compute_metrics_exec",
      "metadata": {},
      "outputs": [],
      "source": [
        "if retrieval_results:\n",
        "    if len(retrieval_results) >= 2:\n",
        "        print(\"Computing overlap-based metrics (2+ models available)...\")\n",
        "    else:\n",
        "        print(\"Computing keyword-based metrics (single model)...\")\n",
        "    \n",
        "    metrics_by_query = compute_overlap_based_metrics(retrieval_results, top_n=5, min_model_agreement=2)\n",
        "    \n",
        "    # Aggregate metrics across all queries per model\n",
        "    metrics_by_model = defaultdict(lambda: defaultdict(list))\n",
        "    \n",
        "    for query, model_metrics in metrics_by_query.items():\n",
        "        for model_alias, metrics_dict in model_metrics.items():\n",
        "            for metric_name, value in metrics_dict.items():\n",
        "                metrics_by_model[model_alias][metric_name].append(value)\n",
        "    \n",
        "    # Compute averages\n",
        "    average_metrics = {}\n",
        "    for model_alias, metrics_dict in metrics_by_model.items():\n",
        "        average_metrics[model_alias] = {}\n",
        "        for metric_name, values in metrics_dict.items():\n",
        "            average_metrics[model_alias][metric_name] = np.mean(values) if values else 0.0\n",
        "    \n",
        "    print(\"Metrics computed\")\n",
        "else:\n",
        "    metrics_by_query = {}\n",
        "    average_metrics = {}\n",
        "    print(\"⚠️  Skipping metrics (no retrieval results)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part3_display",
      "metadata": {},
      "source": [
        "### Display Per-Query Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "display_query_metrics",
      "metadata": {},
      "outputs": [],
      "source": [
        "if metrics_by_query:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"METRICS BY QUERY AND MODEL\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for query in TEST_QUERIES[:3]:\n",
        "        if query in metrics_by_query:\n",
        "            print(f\"\\nQuery: '{query}'\")\n",
        "            query_metrics = metrics_by_query[query]\n",
        "            \n",
        "            for model_alias, metrics_dict in query_metrics.items():\n",
        "                print(f\"  {model_alias}:\")\n",
        "                for metric_name, value in metrics_dict.items():\n",
        "                    if isinstance(value, float):\n",
        "                        print(f\"    {metric_name}: {value:.4f}\")\n",
        "                    else:\n",
        "                        print(f\"    {metric_name}: {value}\")\n",
        "    \n",
        "    print(f\"\\n(Showing first 3 of {len(TEST_QUERIES)} queries)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part3_summary",
      "metadata": {},
      "source": [
        "### Aggregate Metrics Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aggregate_metrics",
      "metadata": {},
      "outputs": [],
      "source": [
        "if average_metrics:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"AVERAGE METRICS ACROSS ALL QUERIES\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "    \n",
        "    metrics_df = pd.DataFrame(average_metrics).T\n",
        "    print(metrics_df.to_string())\n",
        "    \n",
        "    # Add latency info\n",
        "    if retrieval_latencies:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"QUERY LATENCY (ms)\")\n",
        "        print(\"=\" * 80)\n",
        "        print()\n",
        "        \n",
        "        latency_df = pd.DataFrame(retrieval_latencies).T\n",
        "        latency_summary = latency_df.mean()\n",
        "        \n",
        "        for model_alias in latency_summary.index:\n",
        "            avg_latency = latency_summary[model_alias]\n",
        "            print(f\"  {model_alias}: {avg_latency:.1f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d1e49c",
      "metadata": {},
      "source": [
        "## Part 4: Visualize Comparisons\n",
        "\n",
        "**What we're doing:** Create visualizations to compare models clearly.\n",
        "\n",
        "We'll create:\n",
        "1. **Bar chart**: Metrics comparison (Precision@5, Recall@5)\n",
        "2. **Speed vs Quality scatter**: Latency vs Precision trade-off\n",
        "3. **Metrics table**: Side-by-side comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_1_bar_chart",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_metrics_comparison(average_metrics: Dict, retrieval_latencies: Dict):\n",
        "    \"\"\"Create bar chart comparing key metrics across models.\n",
        "    \n",
        "    Args:\n",
        "        average_metrics: Dict[model_alias] = {metric_name: value}\n",
        "        retrieval_latencies: Dict[model_alias] = {query: latency_ms}\n",
        "    \"\"\"\n",
        "    if not average_metrics:\n",
        "        print(\"No metrics to visualize\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    models = list(average_metrics.keys())\n",
        "    \n",
        "    # Plot 1: Precision@5 comparison\n",
        "    if 'precision@5' in list(average_metrics.values())[0]:\n",
        "        precision_values = [average_metrics[m].get('precision@5', 0) for m in models]\n",
        "        \n",
        "        axes[0].bar(models, precision_values, color='steelblue', alpha=0.8)\n",
        "        axes[0].set_ylabel('Precision@5', fontsize=12, fontweight='bold')\n",
        "        axes[0].set_title('Retrieval Quality: Precision@5', fontsize=13, fontweight='bold')\n",
        "        axes[0].set_ylim(0, 1.0)\n",
        "        axes[0].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for i, v in enumerate(precision_values):\n",
        "            axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    # Plot 2: Average Latency comparison\n",
        "    if retrieval_latencies:\n",
        "        latency_values = [\n",
        "            np.mean(list(retrieval_latencies[m].values())) \n",
        "            for m in models \n",
        "            if m in retrieval_latencies\n",
        "        ]\n",
        "        \n",
        "        axes[1].bar(models[:len(latency_values)], latency_values, color='coral', alpha=0.8)\n",
        "        axes[1].set_ylabel('Latency (ms)', fontsize=12, fontweight='bold')\n",
        "        axes[1].set_title('Query Speed: Average Latency', fontsize=13, fontweight='bold')\n",
        "        axes[1].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Add value labels\n",
        "        for i, v in enumerate(latency_values):\n",
        "            axes[1].text(i, v + 5, f'{v:.0f}ms', ha='center', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if average_metrics:\n",
        "    plot_metrics_comparison(average_metrics, retrieval_latencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viz_2_tradeoff",
      "metadata": {},
      "source": [
        "### Speed vs Quality Trade-off\n",
        "\n",
        "Plot each model as a point showing the trade-off between latency and quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_2_scatter",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_speed_quality_tradeoff(average_metrics: Dict, retrieval_latencies: Dict):\n",
        "    \"\"\"Create scatter plot showing speed vs quality trade-off.\n",
        "    \n",
        "    Each model is a point:\n",
        "    - X-axis: Average latency (ms) - lower is faster\n",
        "    - Y-axis: Average Precision@5 - higher is better\n",
        "    \n",
        "    The ideal model is top-left (fast + accurate).\n",
        "    \"\"\"\n",
        "    if not average_metrics or not retrieval_latencies:\n",
        "        print(\"Need both metrics and latencies to plot trade-off\")\n",
        "        return\n",
        "    \n",
        "    models = list(average_metrics.keys())\n",
        "    \n",
        "    # Extract data\n",
        "    latencies = []\n",
        "    qualities = []\n",
        "    \n",
        "    for model in models:\n",
        "        if model in retrieval_latencies:\n",
        "            avg_latency = np.mean(list(retrieval_latencies[model].values()))\n",
        "            latencies.append(avg_latency)\n",
        "            \n",
        "            quality = average_metrics[model].get('precision@5', 0)\n",
        "            qualities.append(quality)\n",
        "    \n",
        "    # Create scatter plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    # Plot points\n",
        "    scatter = ax.scatter(\n",
        "        latencies,\n",
        "        qualities,\n",
        "        s=300,\n",
        "        alpha=0.6,\n",
        "        c=range(len(models)),\n",
        "        cmap='viridis',\n",
        "        edgecolors='black',\n",
        "        linewidth=2\n",
        "    )\n",
        "    \n",
        "    # Annotate with model names\n",
        "    for i, model in enumerate(models):\n",
        "        if i < len(latencies):\n",
        "            ax.annotate(\n",
        "                model,\n",
        "                (latencies[i], qualities[i]),\n",
        "                xytext=(10, 10),\n",
        "                textcoords='offset points',\n",
        "                fontsize=10,\n",
        "                fontweight='bold',\n",
        "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3)\n",
        "            )\n",
        "    \n",
        "    ax.set_xlabel('Latency (ms)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Quality (Precision@5)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Speed vs Quality Trade-off', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add zones\n",
        "    ax.text(\n",
        "        0.98, 0.98,\n",
        "        'IDEAL\\n(Fast + Accurate)',\n",
        "        transform=ax.transAxes,\n",
        "        fontsize=10,\n",
        "        ha='right',\n",
        "        va='top',\n",
        "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5)\n",
        "    )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if len(average_metrics) > 0 and retrieval_latencies:\n",
        "    plot_speed_quality_tradeoff(average_metrics, retrieval_latencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viz_3_table",
      "metadata": {},
      "source": [
        "### Comprehensive Metrics Table\n",
        "\n",
        "Display a side-by-side comparison table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_3_table",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_table(average_metrics: Dict, retrieval_latencies: Dict) -> pd.DataFrame:\n",
        "    \"\"\"Create comprehensive comparison table with all metrics.\n",
        "    \n",
        "    Args:\n",
        "        average_metrics: Dict[model_alias] = {metric_name: value}\n",
        "        retrieval_latencies: Dict[model_alias] = {query: latency_ms}\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with models as rows, metrics as columns\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    \n",
        "    for model_alias in average_metrics.keys():\n",
        "        row = {'Model': model_alias}\n",
        "        \n",
        "        # Add metrics\n",
        "        for metric_name, value in average_metrics[model_alias].items():\n",
        "            if isinstance(value, float):\n",
        "                row[metric_name.title()] = f\"{value:.4f}\"\n",
        "            else:\n",
        "                row[metric_name.title()] = value\n",
        "        \n",
        "        # Add latency\n",
        "        if model_alias in retrieval_latencies:\n",
        "            avg_latency = np.mean(list(retrieval_latencies[model_alias].values()))\n",
        "            row['Avg Latency (ms)'] = f\"{avg_latency:.1f}\"\n",
        "        \n",
        "        rows.append(row)\n",
        "    \n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "if average_metrics:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"COMPREHENSIVE COMPARISON TABLE\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "    \n",
        "    comparison_df = create_comparison_table(average_metrics, retrieval_latencies)\n",
        "    print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part5_analysis",
      "metadata": {},
      "source": [
        "## Part 5: Analysis and Recommendations\n",
        "\n",
        "**What we're doing:** Analyze the comparison results and provide actionable insights.\n",
        "\n",
        "Key questions to answer:\n",
        "1. Which model has best quality (precision/recall)?\n",
        "2. Which model is fastest?\n",
        "3. What are the speed-quality trade-offs?\n",
        "4. For different use cases, which model should we recommend?\n",
        "5. Are there queries where models differ significantly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_and_recommend(\n",
        "    available_models: pd.DataFrame,\n",
        "    average_metrics: Dict,\n",
        "    retrieval_latencies: Dict,\n",
        "    metrics_by_query: Dict\n",
        "):\n",
        "    \"\"\"Analyze comparison results and provide recommendations.\n",
        "    \n",
        "    Args:\n",
        "        available_models: DataFrame of available models\n",
        "        average_metrics: Aggregated metrics\n",
        "        retrieval_latencies: Query latencies\n",
        "        metrics_by_query: Per-query metrics\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ANALYSIS & RECOMMENDATIONS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if not average_metrics:\n",
        "        print(\"\\n⚠️  Not enough data for analysis\")\n",
        "        return\n",
        "    \n",
        "    models = list(average_metrics.keys())\n",
        "    print(f\"\\nSummary: {len(models)} model(s) compared on {len(TEST_QUERIES)} test queries\\n\")\n",
        "    \n",
        "    # 1. Quality ranking\n",
        "    print(\"1. QUALITY RANKING (by Precision@5):\")\n",
        "    quality_ranking = sorted(\n",
        "        [(m, average_metrics[m].get('precision@5', 0)) for m in models],\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "    \n",
        "    for rank, (model, precision) in enumerate(quality_ranking, 1):\n",
        "        print(f\"   {rank}. {model}: {precision:.4f}\")\n",
        "    \n",
        "    # 2. Speed ranking\n",
        "    if retrieval_latencies:\n",
        "        print(\"\\n2. SPEED RANKING (by latency, lower is better):\")\n",
        "        speed_ranking = sorted(\n",
        "            [(m, np.mean(list(retrieval_latencies[m].values()))) for m in models if m in retrieval_latencies],\n",
        "            key=lambda x: x[1]\n",
        "        )\n",
        "        \n",
        "        for rank, (model, latency) in enumerate(speed_ranking, 1):\n",
        "            print(f\"   {rank}. {model}: {latency:.1f}ms\")\n",
        "    \n",
        "    # 3. Best model overall\n",
        "    if len(models) > 1:\n",
        "        best_quality = quality_ranking[0][0]\n",
        "        fastest = speed_ranking[0][0] if retrieval_latencies else None\n",
        "        \n",
        "        print(f\"\\n3. OVERALL WINNERS:\")\n",
        "        print(f\"   Best Quality: {best_quality} (Precision: {quality_ranking[0][1]:.4f})\")\n",
        "        if fastest:\n",
        "            print(f\"   Fastest: {fastest} ({speed_ranking[0][1]:.1f}ms per query)\")\n",
        "    \n",
        "    # 4. Trade-off analysis\n",
        "    if len(models) > 1:\n",
        "        print(f\"\\n4. TRADE-OFF ANALYSIS:\")\n",
        "        \n",
        "        # Calculate quality/latency ratio (efficiency)\n",
        "        efficiency = {}\n",
        "        for model in models:\n",
        "            if model in retrieval_latencies:\n",
        "                quality = average_metrics[model].get('precision@5', 0)\n",
        "                latency = np.mean(list(retrieval_latencies[model].values()))\n",
        "                if latency > 0:\n",
        "                    efficiency[model] = quality / latency\n",
        "        \n",
        "        if efficiency:\n",
        "            best_efficiency = max(efficiency.items(), key=lambda x: x[1])\n",
        "            print(f\"   Best Efficiency (Quality/Speed): {best_efficiency[0]}\")\n",
        "            print(f\"     Quality per millisecond: {best_efficiency[1]:.6f}\")\n",
        "    \n",
        "    # 5. Per-query analysis\n",
        "    print(f\"\\n5. PER-QUERY VARIABILITY:\")\n",
        "    \n",
        "    if metrics_by_query and len(models) > 1:\n",
        "        # Find queries where models differ most\n",
        "        query_differences = []\n",
        "        \n",
        "        for query, query_metrics in metrics_by_query.items():\n",
        "            if len(query_metrics) >= 2:\n",
        "                precisions = [query_metrics[m].get('precision@5', 0) for m in models if m in query_metrics]\n",
        "                if precisions:\n",
        "                    diff = max(precisions) - min(precisions)\n",
        "                    query_differences.append((query, diff, max(precisions), min(precisions)))\n",
        "        \n",
        "        if query_differences:\n",
        "            # Sort by difference\n",
        "            query_differences.sort(key=lambda x: x[1], reverse=True)\n",
        "            \n",
        "            print(f\"\\n   Queries with biggest model differences:\")\n",
        "            for query, diff, best, worst in query_differences[:3]:\n",
        "                print(f\"   - '{query}'\")\n",
        "                print(f\"     Best: {best:.4f}, Worst: {worst:.4f}, Diff: {diff:.4f}\")\n",
        "    \n",
        "    # 6. Recommendations\n",
        "    print(f\"\\n6. RECOMMENDATIONS:\")\n",
        "    \n",
        "    if len(models) == 1:\n",
        "        print(f\"   - Only 1 model available for comparison\")\n",
        "        print(f\"   - Generate a second model to enable comparative analysis\")\n",
        "        print(f\"   - See instructions at top of notebook\")\n",
        "    else:\n",
        "        best_quality_model = quality_ranking[0][0]\n",
        "        fastest_model = speed_ranking[0][0] if retrieval_latencies else None\n",
        "        \n",
        "        print(f\"\\n   Use-Case Recommendations:\")\n",
        "        print(f\"\\n   a) For Maximum Quality:\")\n",
        "        print(f\"      - Use: {best_quality_model}\")\n",
        "        print(f\"      - Precision@5: {quality_ranking[0][1]:.4f}\")\n",
        "        print(f\"      - Best for: Applications requiring high accuracy (e.g., research, legal)\")\n",
        "        \n",
        "        if fastest_model:\n",
        "            print(f\"\\n   b) For Maximum Speed:\")\n",
        "            print(f\"      - Use: {fastest_model}\")\n",
        "            print(f\"      - Latency: {speed_ranking[0][1]:.1f}ms per query\")\n",
        "            print(f\"      - Best for: Real-time applications, user-facing chatbots\")\n",
        "        \n",
        "        if efficiency:\n",
        "            best_eff_model = max(efficiency.items(), key=lambda x: x[1])[0]\n",
        "            print(f\"\\n   c) For Best Efficiency (Quality/Speed):\")\n",
        "            print(f\"      - Use: {best_eff_model}\")\n",
        "            print(f\"      - Best for: Balanced production systems\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "\n",
        "analyze_and_recommend(available_models, average_metrics, retrieval_latencies, metrics_by_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "next_steps",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you've compared embedding models:\n",
        "\n",
        "1. **Advanced Techniques (intermediate/05+)**: Use your chosen model to apply retrieval improvements\n",
        "   - Query expansion\n",
        "   - Reranking\n",
        "   - Hybrid search (vector + keyword)\n",
        "\n",
        "2. **Evaluation Lab**: Compare your techniques against the baseline\n",
        "   - Track metrics over time\n",
        "   - A/B test different approaches\n",
        "   - Measure real user satisfaction\n",
        "\n",
        "3. **Model Fine-tuning**: If you want better results\n",
        "   - Collect domain-specific training data\n",
        "   - Fine-tune an embedding model on your data\n",
        "   - Compare against base models\n",
        "\n",
        "4. **Production Deployment**: Use your recommendation\n",
        "   - Implement the recommended model\n",
        "   - Monitor quality metrics in production\n",
        "   - Update if better models become available"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf511616",
      "metadata": {},
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff202e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close database connections\n",
        "if 'conn' in locals() and conn:\n",
        "    conn.close()\n",
        "\n",
        "print(\"Connections closed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
