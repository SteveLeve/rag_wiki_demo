{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4644e21",
   "metadata": {},
   "source": [
    "# RAG Tutorial with Wikipedia Dataset\n",
    "\n",
    "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) system using Simple Wikipedia articles. The dataset is configurable by size, making it easy to experiment with different amounts of data while staying within free tier limits.\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "Before running this notebook, you need to:\n",
    "\n",
    "1. Install Ollama from [ollama.com](https://ollama.com/)\n",
    "2. Download the required models by running these commands in your terminal:\n",
    "\n",
    "```bash\n",
    "ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\n",
    "ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n",
    "```\n",
    "\n",
    "3. Install the required Python packages:\n",
    "\n",
    "```bash\n",
    "pip install ollama datasets ipywidgets jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3198d7f",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1dd9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e96af9",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the target dataset size. The script will download articles until it reaches approximately this size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc326412",
   "metadata": {},
   "source": [
    "## Install Additional Dependencies\n",
    "\n",
    "If you plan to use PostgreSQL for persistent storage, install the additional dependency:\n",
    "\n",
    "```bash\n",
    "pip install psycopg2-binary\n",
    "```\n",
    "\n",
    "Or if you're using a virtual environment (recommended):\n",
    "\n",
    "```bash\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "pip install psycopg2-binary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target dataset size in MB (adjust as needed: 10, 20, 30, 40, 50)\n",
    "TARGET_SIZE_MB = 10\n",
    "\n",
    "# Maximum chunk size in characters (for splitting long articles)\n",
    "MAX_CHUNK_SIZE = 1000\n",
    "\n",
    "# Whether to save the dataset locally for reuse\n",
    "SAVE_LOCALLY = True\n",
    "LOCAL_DATASET_PATH = f'wikipedia_dataset_{TARGET_SIZE_MB}mb.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b99ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage backend configuration\n",
    "STORAGE_BACKEND = 'memory'  # Options: 'memory', 'json', 'postgresql'\n",
    "\n",
    "# PostgreSQL configuration (only used if STORAGE_BACKEND == 'postgresql')\n",
    "POSTGRES_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'rag_db',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "}\n",
    "\n",
    "# Table name for this embedding model (allows storing multiple models)\n",
    "# Table name will be: embeddings_{EMBEDDING_MODEL_ALIAS}\n",
    "EMBEDDING_MODEL_ALIAS = 'bge_base_en_v1.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_loading",
   "metadata": {},
   "source": [
    "## Load and Filter the Wikipedia Dataset\n",
    "\n",
    "We'll use Simple Wikipedia, which has cleaner, more concise articles. The dataset will be filtered to approximately your target size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc2a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load cached dataset from wikipedia_dataset_10mb.json...\n",
      "No cached dataset found, downloading from HuggingFace...\n",
      "Loading Simple Wikipedia dataset (this may take a minute)...\n",
      "\n",
      "Collecting articles (target: 10 MB)...\n",
      "\n",
      "Collecting articles (target: 10 MB)...\n",
      "  Progress: 0.28 MB (50 articles, 323 chunks)\n",
      "  Progress: 0.52 MB (100 articles, 589 chunks)\n",
      "  Progress: 0.75 MB (150 articles, 856 chunks)\n",
      "  Progress: 1.01 MB (200 articles, 1135 chunks)\n",
      "  Progress: 1.26 MB (250 articles, 1402 chunks)\n",
      "  Progress: 1.54 MB (300 articles, 1721 chunks)\n",
      "  Progress: 1.75 MB (350 articles, 1951 chunks)\n",
      "  Progress: 2.05 MB (400 articles, 2255 chunks)\n",
      "  Progress: 2.30 MB (450 articles, 2541 chunks)\n",
      "  Progress: 2.49 MB (500 articles, 2761 chunks)\n",
      "  Progress: 2.82 MB (550 articles, 3094 chunks)\n",
      "  Progress: 3.07 MB (600 articles, 3346 chunks)\n",
      "  Progress: 3.41 MB (650 articles, 3680 chunks)\n",
      "  Progress: 3.60 MB (700 articles, 3892 chunks)\n",
      "  Progress: 3.85 MB (750 articles, 4174 chunks)\n",
      "  Progress: 4.10 MB (800 articles, 4444 chunks)\n",
      "  Progress: 4.31 MB (850 articles, 4684 chunks)\n",
      "  Progress: 4.52 MB (900 articles, 4938 chunks)\n",
      "  Progress: 4.70 MB (950 articles, 5141 chunks)\n",
      "  Progress: 5.04 MB (1000 articles, 5432 chunks)\n",
      "  Progress: 5.58 MB (1050 articles, 5874 chunks)\n",
      "  Progress: 5.81 MB (1100 articles, 6127 chunks)\n",
      "  Progress: 6.07 MB (1150 articles, 6379 chunks)\n",
      "  Progress: 6.42 MB (1200 articles, 6733 chunks)\n",
      "  Progress: 6.68 MB (1250 articles, 6972 chunks)\n",
      "  Progress: 6.85 MB (1300 articles, 7172 chunks)\n",
      "  Progress: 7.11 MB (1350 articles, 7442 chunks)\n",
      "  Progress: 7.33 MB (1400 articles, 7670 chunks)\n",
      "  Progress: 7.85 MB (1450 articles, 8113 chunks)\n",
      "  Progress: 8.09 MB (1500 articles, 8377 chunks)\n",
      "  Progress: 8.39 MB (1550 articles, 8666 chunks)\n",
      "  Progress: 8.57 MB (1600 articles, 8860 chunks)\n",
      "  Progress: 8.67 MB (1650 articles, 8971 chunks)\n",
      "  Progress: 8.84 MB (1700 articles, 9151 chunks)\n",
      "  Progress: 9.09 MB (1750 articles, 9426 chunks)\n",
      "  Progress: 9.28 MB (1800 articles, 9637 chunks)\n",
      "  Progress: 9.48 MB (1850 articles, 9864 chunks)\n",
      "  Progress: 9.64 MB (1900 articles, 10053 chunks)\n",
      "  Progress: 9.88 MB (1950 articles, 10275 chunks)\n",
      "\n",
      "✓ Dataset loaded: 10402 chunks from 1993 articles\n",
      "  Estimated size: 10.00 MB\n",
      "\n",
      "Saving dataset to wikipedia_dataset_10mb.json...\n",
      "  Progress: 0.28 MB (50 articles, 323 chunks)\n",
      "  Progress: 0.52 MB (100 articles, 589 chunks)\n",
      "  Progress: 0.75 MB (150 articles, 856 chunks)\n",
      "  Progress: 1.01 MB (200 articles, 1135 chunks)\n",
      "  Progress: 1.26 MB (250 articles, 1402 chunks)\n",
      "  Progress: 1.54 MB (300 articles, 1721 chunks)\n",
      "  Progress: 1.75 MB (350 articles, 1951 chunks)\n",
      "  Progress: 2.05 MB (400 articles, 2255 chunks)\n",
      "  Progress: 2.30 MB (450 articles, 2541 chunks)\n",
      "  Progress: 2.49 MB (500 articles, 2761 chunks)\n",
      "  Progress: 2.82 MB (550 articles, 3094 chunks)\n",
      "  Progress: 3.07 MB (600 articles, 3346 chunks)\n",
      "  Progress: 3.41 MB (650 articles, 3680 chunks)\n",
      "  Progress: 3.60 MB (700 articles, 3892 chunks)\n",
      "  Progress: 3.85 MB (750 articles, 4174 chunks)\n",
      "  Progress: 4.10 MB (800 articles, 4444 chunks)\n",
      "  Progress: 4.31 MB (850 articles, 4684 chunks)\n",
      "  Progress: 4.52 MB (900 articles, 4938 chunks)\n",
      "  Progress: 4.70 MB (950 articles, 5141 chunks)\n",
      "  Progress: 5.04 MB (1000 articles, 5432 chunks)\n",
      "  Progress: 5.58 MB (1050 articles, 5874 chunks)\n",
      "  Progress: 5.81 MB (1100 articles, 6127 chunks)\n",
      "  Progress: 6.07 MB (1150 articles, 6379 chunks)\n",
      "  Progress: 6.42 MB (1200 articles, 6733 chunks)\n",
      "  Progress: 6.68 MB (1250 articles, 6972 chunks)\n",
      "  Progress: 6.85 MB (1300 articles, 7172 chunks)\n",
      "  Progress: 7.11 MB (1350 articles, 7442 chunks)\n",
      "  Progress: 7.33 MB (1400 articles, 7670 chunks)\n",
      "  Progress: 7.85 MB (1450 articles, 8113 chunks)\n",
      "  Progress: 8.09 MB (1500 articles, 8377 chunks)\n",
      "  Progress: 8.39 MB (1550 articles, 8666 chunks)\n",
      "  Progress: 8.57 MB (1600 articles, 8860 chunks)\n",
      "  Progress: 8.67 MB (1650 articles, 8971 chunks)\n",
      "  Progress: 8.84 MB (1700 articles, 9151 chunks)\n",
      "  Progress: 9.09 MB (1750 articles, 9426 chunks)\n",
      "  Progress: 9.28 MB (1800 articles, 9637 chunks)\n",
      "  Progress: 9.48 MB (1850 articles, 9864 chunks)\n",
      "  Progress: 9.64 MB (1900 articles, 10053 chunks)\n",
      "  Progress: 9.88 MB (1950 articles, 10275 chunks)\n",
      "\n",
      "✓ Dataset loaded: 10402 chunks from 1993 articles\n",
      "  Estimated size: 10.00 MB\n",
      "\n",
      "Saving dataset to wikipedia_dataset_10mb.json...\n",
      "✓ Dataset saved for future use\n",
      "\n",
      "Ready to build vector database with 10402 chunks!\n",
      "✓ Dataset saved for future use\n",
      "\n",
      "Ready to build vector database with 10402 chunks!\n"
     ]
    }
   ],
   "source": [
    "def estimate_size_mb(text):\n",
    "    \"\"\"Estimate the size of text in megabytes.\"\"\"\n",
    "    return sys.getsizeof(text) / (1024 * 1024)\n",
    "\n",
    "def chunk_text(text, max_size=1000):\n",
    "    \"\"\"Split text into chunks of approximately max_size characters.\n",
    "    \n",
    "    Tries to break at paragraph boundaries when possible.\n",
    "    \"\"\"\n",
    "    if len(text) <= max_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    current_chunk = ''\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # If adding this paragraph would exceed max_size\n",
    "        if len(current_chunk) + len(paragraph) > max_size:\n",
    "            if current_chunk:  # Save current chunk if not empty\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = ''\n",
    "            \n",
    "            # If single paragraph is too large, split it\n",
    "            if len(paragraph) > max_size:\n",
    "                sentences = paragraph.split('. ')\n",
    "                for sentence in sentences:\n",
    "                    if len(current_chunk) + len(sentence) > max_size:\n",
    "                        if current_chunk:\n",
    "                            chunks.append(current_chunk.strip())\n",
    "                        current_chunk = sentence + '. '\n",
    "                    else:\n",
    "                        current_chunk += sentence + '. '\n",
    "            else:\n",
    "                current_chunk = paragraph\n",
    "        else:\n",
    "            current_chunk += '\\n\\n' + paragraph if current_chunk else paragraph\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_wikipedia_dataset(target_size_mb, local_path=None):\n",
    "    \"\"\"Load and filter Wikipedia dataset to target size.\n",
    "    \n",
    "    Args:\n",
    "        target_size_mb: Target dataset size in megabytes\n",
    "        local_path: Path to save/load dataset locally\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Try to load from local cache first\n",
    "    if local_path:\n",
    "        try:\n",
    "            print(f'Attempting to load cached dataset from {local_path}...')\n",
    "            with open(local_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f'✓ Loaded {len(data[\"chunks\"])} chunks from cache')\n",
    "                print(f'  Estimated size: {data[\"size_mb\"]:.2f} MB')\n",
    "                return data['chunks']\n",
    "        except FileNotFoundError:\n",
    "            print('No cached dataset found, downloading from HuggingFace...')\n",
    "    \n",
    "    # Load Simple Wikipedia dataset\n",
    "    print('Loading Simple Wikipedia dataset (this may take a minute)...')\n",
    "    dataset = load_dataset('wikimedia/wikipedia', '20231101.simple', split='train', streaming=True)\n",
    "    \n",
    "    chunks = []\n",
    "    current_size_mb = 0\n",
    "    target_bytes = target_size_mb * 1024 * 1024\n",
    "    article_count = 0\n",
    "    \n",
    "    print(f'\\nCollecting articles (target: {target_size_mb} MB)...')\n",
    "    \n",
    "    for article in dataset:\n",
    "        # Skip very short articles\n",
    "        if len(article['text']) < 200:\n",
    "            continue\n",
    "        \n",
    "        # Create metadata-enriched chunks\n",
    "        article_chunks = chunk_text(article['text'], MAX_CHUNK_SIZE)\n",
    "        \n",
    "        for chunk in article_chunks:\n",
    "            # Add title context to help with retrieval\n",
    "            enriched_chunk = f\"Article: {article['title']}\\n\\n{chunk}\"\n",
    "            chunk_size = sys.getsizeof(enriched_chunk)\n",
    "            \n",
    "            chunks.append(enriched_chunk)\n",
    "            current_size_mb += chunk_size\n",
    "            \n",
    "            # Check if we've reached target size\n",
    "            if current_size_mb >= target_bytes:\n",
    "                break\n",
    "        \n",
    "        article_count += 1\n",
    "        \n",
    "        # Progress update every 50 articles\n",
    "        if article_count % 50 == 0:\n",
    "            print(f'  Progress: {current_size_mb / (1024*1024):.2f} MB ({article_count} articles, {len(chunks)} chunks)')\n",
    "        \n",
    "        if current_size_mb >= target_bytes:\n",
    "            break\n",
    "    \n",
    "    final_size_mb = current_size_mb / (1024 * 1024)\n",
    "    print(f'\\n✓ Dataset loaded: {len(chunks)} chunks from {article_count} articles')\n",
    "    print(f'  Estimated size: {final_size_mb:.2f} MB')\n",
    "    \n",
    "    # Save locally if requested\n",
    "    if local_path:\n",
    "        print(f'\\nSaving dataset to {local_path}...')\n",
    "        with open(local_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'size_mb': final_size_mb,\n",
    "                'chunk_count': len(chunks),\n",
    "                'article_count': article_count,\n",
    "                'chunks': chunks\n",
    "            }, f, ensure_ascii=False)\n",
    "        print('✓ Dataset saved for future use')\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_wikipedia_dataset(\n",
    "    TARGET_SIZE_MB, \n",
    "    LOCAL_DATASET_PATH if SAVE_LOCALLY else None\n",
    ")\n",
    "\n",
    "print(f'\\nReady to build vector database with {len(dataset)} chunks!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database helper functions for PostgreSQL storage\n",
    "\n",
    "class PostgreSQLVectorDB:\n",
    "    \"\"\"Helper class to manage embeddings in PostgreSQL with pgvector.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, table_name):\n",
    "        \"\"\"Initialize database connection.\n",
    "        \n",
    "        Args:\n",
    "            config: Dictionary with host, port, database, user, password\n",
    "            table_name: Name of the table for this embedding model\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.table_name = table_name\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "        self.setup_table()\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection.\"\"\"\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(\n",
    "                host=self.config['host'],\n",
    "                port=self.config['port'],\n",
    "                database=self.config['database'],\n",
    "                user=self.config['user'],\n",
    "                password=self.config['password']\n",
    "            )\n",
    "            print(f'✓ Connected to PostgreSQL at {self.config[\"host\"]}:{self.config[\"port\"]}')\n",
    "        except psycopg2.OperationalError as e:\n",
    "            print(f'✗ Failed to connect to PostgreSQL: {e}')\n",
    "            print('Make sure PostgreSQL is running with pgvector support.')\n",
    "            print('Start it with: docker run -d --name pgvector-rag \\\\')\n",
    "            print('  -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=rag_db \\\\')\n",
    "            print('  -p 5432:5432 -v pgvector_data:/var/lib/postgresql/data \\\\')\n",
    "            print('  pgvector/pgvector:pg16')\n",
    "            raise\n",
    "    \n",
    "    def setup_table(self):\n",
    "        \"\"\"Create table if it doesn't exist.\"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            # Enable pgvector extension\n",
    "            cur.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "            \n",
    "            # Create table with vector column\n",
    "            cur.execute(f'''\n",
    "                CREATE TABLE IF NOT EXISTS {self.table_name} (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    chunk_text TEXT NOT NULL,\n",
    "                    embedding vector(768),\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create index for fast similarity search\n",
    "            index_name = f'{self.table_name}_embedding_idx'\n",
    "            cur.execute(f'''\n",
    "                CREATE INDEX IF NOT EXISTS {index_name}\n",
    "                ON {self.table_name} USING hnsw (embedding vector_cosine_ops)\n",
    "            ''')\n",
    "            \n",
    "            self.conn.commit()\n",
    "            print(f'✓ Table \"{self.table_name}\" ready for embeddings')\n",
    "    \n",
    "    def insert_embedding(self, chunk, embedding):\n",
    "        \"\"\"Insert a chunk and its embedding into the database.\n",
    "        \n",
    "        Args:\n",
    "            chunk: The text chunk\n",
    "            embedding: The embedding vector (list of floats)\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                INSERT INTO {self.table_name} (chunk_text, embedding)\n",
    "                VALUES (%s, %s)\n",
    "            ''', (chunk, embedding))\n",
    "            self.conn.commit()\n",
    "    \n",
    "    def insert_batch(self, chunks_embeddings):\n",
    "        \"\"\"Batch insert multiple chunks and embeddings.\n",
    "        \n",
    "        Args:\n",
    "            chunks_embeddings: List of (chunk, embedding) tuples\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            execute_values(cur, f'''\n",
    "                INSERT INTO {self.table_name} (chunk_text, embedding)\n",
    "                VALUES %s\n",
    "            ''', chunks_embeddings, page_size=100)\n",
    "            self.conn.commit()\n",
    "    \n",
    "    def get_chunk_count(self):\n",
    "        \"\"\"Get the number of stored chunks.\"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n",
    "            return cur.fetchone()[0]\n",
    "    \n",
    "    def similarity_search(self, query_embedding, top_n=3):\n",
    "        \"\"\"Find most similar chunks using pgvector.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: The query embedding vector\n",
    "            top_n: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (chunk_text, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                SELECT chunk_text, \n",
    "                       1 - (embedding <=> %s::vector) as similarity\n",
    "                FROM {self.table_name}\n",
    "                ORDER BY embedding <=> %s::vector\n",
    "                LIMIT %s\n",
    "            ''', (embedding, embedding, top_n))\n",
    "            \n",
    "            results = cur.fetchall()\n",
    "            return [(chunk, score) for chunk, score in results]\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "\n",
    "def get_storage_backend(backend_type, config=None, table_name=None):\n",
    "    \"\"\"Factory function to get the appropriate storage backend.\n",
    "    \n",
    "    Args:\n",
    "        backend_type: 'memory', 'json', or 'postgresql'\n",
    "        config: PostgreSQL config dict (required if backend_type is 'postgresql')\n",
    "        table_name: Table name (required if backend_type is 'postgresql')\n",
    "    \n",
    "    Returns:\n",
    "        Storage backend instance\n",
    "    \"\"\"\n",
    "    if backend_type == 'postgresql':\n",
    "        if not config or not table_name:\n",
    "            raise ValueError('PostgreSQL backend requires config and table_name')\n",
    "        return PostgreSQLVectorDB(config, table_name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample_data",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's look at a few examples from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sample_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunks from the dataset:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Article: April\n",
      "\n",
      "April (Apr.) is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May. It is one of the four months to have 30 days.\n",
      "\n",
      "April always begins on the same day of the week as July, and additionally, January in leap years. April always ends on t...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Article: April\n",
      "\n",
      "In common years, April starts on the same day of the week as October of the previous year, and in leap years, May of the previous year. In common years, April finishes on the same day of the week as July of the previous year, and in leap years, February and October of the previous ye...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Article: April\n",
      "\n",
      "April is a spring month in the Northern Hemisphere and an autumn/fall month in the Southern Hemisphere. In each hemisphere, it is the seasonal equivalent of October in the other.\n",
      "\n",
      "It is unclear as to where April got its name. A common theory is that it comes from the Latin word \"aper...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Sample chunks from the dataset:\\n')\n",
    "for i, chunk in enumerate(dataset[:3]):\n",
    "    print(f'--- Chunk {i+1} ---')\n",
    "    print(chunk[:300] + '...' if len(chunk) > 300 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a326fc",
   "metadata": {},
   "source": [
    "## Configure Models\n",
    "\n",
    "We'll use two models:\n",
    "- **Embedding Model**: Converts text into vector representations\n",
    "- **Language Model**: Generates responses based on retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5aa78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a7821",
   "metadata": {},
   "source": [
    "## Implement the Vector Database\n",
    "\n",
    "### Indexing Phase\n",
    "\n",
    "In the indexing phase, we:\n",
    "1. Break the dataset into chunks (already done during loading)\n",
    "2. Calculate embedding vectors for each chunk\n",
    "3. Store chunks with their embeddings in our vector database\n",
    "\n",
    "Each element in `VECTOR_DB` will be a tuple: `(chunk, embedding)`\n",
    "\n",
    "The embedding is a list of floats, for example: `[0.1, 0.04, -0.34, 0.21, ...]`\n",
    "\n",
    "**Note**: This may take a few minutes depending on your dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "VECTOR_DB = []\n",
    "\n",
    "# Initialize storage backend if using PostgreSQL\n",
    "PG_DB = None\n",
    "if STORAGE_BACKEND == 'postgresql':\n",
    "    table_name = f'embeddings_{EMBEDDING_MODEL_ALIAS.replace(\".\", \"_\")}'\n",
    "    PG_DB = get_storage_backend('postgresql', POSTGRES_CONFIG, table_name)\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "    \"\"\"Add a chunk and its embedding to the vector database.\n",
    "    \n",
    "    Stores in memory and/or PostgreSQL depending on STORAGE_BACKEND.\n",
    "    \"\"\"\n",
    "    embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "    \n",
    "    if STORAGE_BACKEND == 'memory' or STORAGE_BACKEND == 'json':\n",
    "        VECTOR_DB.append((chunk, embedding))\n",
    "    elif STORAGE_BACKEND == 'postgresql':\n",
    "        PG_DB.insert_embedding(chunk, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df5ca8",
   "metadata": {},
   "source": [
    "Now let's populate our vector database with all chunks from the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4948f",
   "metadata": {},
   "source": [
    "## Optional: Persistent Storage with PostgreSQL & pgvector\n",
    "\n",
    "**Note on Performance**: Embedding generation takes significant time (~50 minutes for 10MB of data). Consider using PostgreSQL with pgvector for durable storage so you can reuse embeddings across multiple experiments without regenerating them.\n",
    "\n",
    "### Why PostgreSQL + pgvector?\n",
    "\n",
    "- **Reusable Embeddings**: Generate embeddings once, use them across multiple notebooks and experiments\n",
    "- **Multiple Models**: Store embeddings from different embedding models in separate tables for comparison\n",
    "- **Durable Storage**: Embeddings survive notebook restarts\n",
    "- **Scalability**: Move to production vector databases more easily\n",
    "\n",
    "### Quick Start with Docker\n",
    "\n",
    "1. Install [Docker Desktop](https://www.docker.com/products/docker-desktop) if you haven't already\n",
    "2. Run PostgreSQL with pgvector:\n",
    "\n",
    "```bash\n",
    "docker run --name pgvector-rag \\\n",
    "  -e POSTGRES_PASSWORD=postgres \\\n",
    "  -e POSTGRES_DB=rag_db \\\n",
    "  -p 5432:5432 \\\n",
    "  -v pgvector_data:/var/lib/postgresql/data \\\n",
    "  pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "This creates a persistent volume (`pgvector_data`) so your data survives container restarts.\n",
    "\n",
    "### Configuration for Persistent Storage\n",
    "\n",
    "Set the storage backend in the configuration section below. Choose:\n",
    "- `'memory'` - In-memory only (fast but lost on notebook restart)\n",
    "- `'json'` - Local JSON file (persists but slower for large datasets)\n",
    "- `'postgresql'` - PostgreSQL with pgvector (recommended for experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a7f2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vector database with 10402 chunks...\n",
      "This may take a few minutes...\n",
      "\n",
      "Embedded 50/10402 chunks (0.5%)\n",
      "Embedded 50/10402 chunks (0.5%)\n",
      "Embedded 100/10402 chunks (1.0%)\n",
      "Embedded 100/10402 chunks (1.0%)\n",
      "Embedded 150/10402 chunks (1.4%)\n",
      "Embedded 150/10402 chunks (1.4%)\n",
      "Embedded 200/10402 chunks (1.9%)\n",
      "Embedded 200/10402 chunks (1.9%)\n",
      "Embedded 250/10402 chunks (2.4%)\n",
      "Embedded 250/10402 chunks (2.4%)\n",
      "Embedded 300/10402 chunks (2.9%)\n",
      "Embedded 300/10402 chunks (2.9%)\n",
      "Embedded 350/10402 chunks (3.4%)\n",
      "Embedded 350/10402 chunks (3.4%)\n",
      "Embedded 400/10402 chunks (3.8%)\n",
      "Embedded 400/10402 chunks (3.8%)\n",
      "Embedded 450/10402 chunks (4.3%)\n",
      "Embedded 450/10402 chunks (4.3%)\n",
      "Embedded 500/10402 chunks (4.8%)\n",
      "Embedded 500/10402 chunks (4.8%)\n",
      "Embedded 550/10402 chunks (5.3%)\n",
      "Embedded 550/10402 chunks (5.3%)\n",
      "Embedded 600/10402 chunks (5.8%)\n",
      "Embedded 600/10402 chunks (5.8%)\n",
      "Embedded 650/10402 chunks (6.2%)\n",
      "Embedded 650/10402 chunks (6.2%)\n",
      "Embedded 700/10402 chunks (6.7%)\n",
      "Embedded 700/10402 chunks (6.7%)\n",
      "Embedded 750/10402 chunks (7.2%)\n",
      "Embedded 750/10402 chunks (7.2%)\n",
      "Embedded 800/10402 chunks (7.7%)\n",
      "Embedded 800/10402 chunks (7.7%)\n",
      "Embedded 850/10402 chunks (8.2%)\n",
      "Embedded 850/10402 chunks (8.2%)\n",
      "Embedded 900/10402 chunks (8.7%)\n",
      "Embedded 900/10402 chunks (8.7%)\n",
      "Embedded 950/10402 chunks (9.1%)\n",
      "Embedded 950/10402 chunks (9.1%)\n",
      "Embedded 1000/10402 chunks (9.6%)\n",
      "Embedded 1000/10402 chunks (9.6%)\n",
      "Embedded 1050/10402 chunks (10.1%)\n",
      "Embedded 1050/10402 chunks (10.1%)\n",
      "Embedded 1100/10402 chunks (10.6%)\n",
      "Embedded 1100/10402 chunks (10.6%)\n",
      "Embedded 1150/10402 chunks (11.1%)\n",
      "Embedded 1150/10402 chunks (11.1%)\n",
      "Embedded 1200/10402 chunks (11.5%)\n",
      "Embedded 1200/10402 chunks (11.5%)\n",
      "Embedded 1250/10402 chunks (12.0%)\n",
      "Embedded 1250/10402 chunks (12.0%)\n",
      "Embedded 1300/10402 chunks (12.5%)\n",
      "Embedded 1300/10402 chunks (12.5%)\n",
      "Embedded 1350/10402 chunks (13.0%)\n",
      "Embedded 1350/10402 chunks (13.0%)\n",
      "Embedded 1400/10402 chunks (13.5%)\n",
      "Embedded 1400/10402 chunks (13.5%)\n",
      "Embedded 1450/10402 chunks (13.9%)\n",
      "Embedded 1450/10402 chunks (13.9%)\n",
      "Embedded 1500/10402 chunks (14.4%)\n",
      "Embedded 1500/10402 chunks (14.4%)\n",
      "Embedded 1550/10402 chunks (14.9%)\n",
      "Embedded 1550/10402 chunks (14.9%)\n",
      "Embedded 1600/10402 chunks (15.4%)\n",
      "Embedded 1600/10402 chunks (15.4%)\n",
      "Embedded 1650/10402 chunks (15.9%)\n",
      "Embedded 1650/10402 chunks (15.9%)\n",
      "Embedded 1700/10402 chunks (16.3%)\n",
      "Embedded 1700/10402 chunks (16.3%)\n",
      "Embedded 1750/10402 chunks (16.8%)\n",
      "Embedded 1750/10402 chunks (16.8%)\n",
      "Embedded 1800/10402 chunks (17.3%)\n",
      "Embedded 1800/10402 chunks (17.3%)\n",
      "Embedded 1850/10402 chunks (17.8%)\n",
      "Embedded 1850/10402 chunks (17.8%)\n",
      "Embedded 1900/10402 chunks (18.3%)\n",
      "Embedded 1900/10402 chunks (18.3%)\n",
      "Embedded 1950/10402 chunks (18.7%)\n",
      "Embedded 1950/10402 chunks (18.7%)\n",
      "Embedded 2000/10402 chunks (19.2%)\n",
      "Embedded 2000/10402 chunks (19.2%)\n",
      "Embedded 2050/10402 chunks (19.7%)\n",
      "Embedded 2050/10402 chunks (19.7%)\n",
      "Embedded 2100/10402 chunks (20.2%)\n",
      "Embedded 2100/10402 chunks (20.2%)\n",
      "Embedded 2150/10402 chunks (20.7%)\n",
      "Embedded 2150/10402 chunks (20.7%)\n",
      "Embedded 2200/10402 chunks (21.1%)\n",
      "Embedded 2200/10402 chunks (21.1%)\n",
      "Embedded 2250/10402 chunks (21.6%)\n",
      "Embedded 2250/10402 chunks (21.6%)\n",
      "Embedded 2300/10402 chunks (22.1%)\n",
      "Embedded 2300/10402 chunks (22.1%)\n",
      "Embedded 2350/10402 chunks (22.6%)\n",
      "Embedded 2350/10402 chunks (22.6%)\n",
      "Embedded 2400/10402 chunks (23.1%)\n",
      "Embedded 2400/10402 chunks (23.1%)\n",
      "Embedded 2450/10402 chunks (23.6%)\n",
      "Embedded 2450/10402 chunks (23.6%)\n",
      "Embedded 2500/10402 chunks (24.0%)\n",
      "Embedded 2500/10402 chunks (24.0%)\n",
      "Embedded 2550/10402 chunks (24.5%)\n",
      "Embedded 2550/10402 chunks (24.5%)\n",
      "Embedded 2600/10402 chunks (25.0%)\n",
      "Embedded 2600/10402 chunks (25.0%)\n",
      "Embedded 2650/10402 chunks (25.5%)\n",
      "Embedded 2650/10402 chunks (25.5%)\n",
      "Embedded 2700/10402 chunks (26.0%)\n",
      "Embedded 2700/10402 chunks (26.0%)\n",
      "Embedded 2750/10402 chunks (26.4%)\n",
      "Embedded 2750/10402 chunks (26.4%)\n",
      "Embedded 2800/10402 chunks (26.9%)\n",
      "Embedded 2800/10402 chunks (26.9%)\n",
      "Embedded 2850/10402 chunks (27.4%)\n",
      "Embedded 2850/10402 chunks (27.4%)\n",
      "Embedded 2900/10402 chunks (27.9%)\n",
      "Embedded 2900/10402 chunks (27.9%)\n",
      "Embedded 2950/10402 chunks (28.4%)\n",
      "Embedded 2950/10402 chunks (28.4%)\n",
      "Embedded 3000/10402 chunks (28.8%)\n",
      "Embedded 3000/10402 chunks (28.8%)\n",
      "Embedded 3050/10402 chunks (29.3%)\n",
      "Embedded 3100/10402 chunks (29.8%)\n",
      "Embedded 3150/10402 chunks (30.3%)\n",
      "Embedded 3200/10402 chunks (30.8%)\n",
      "Embedded 3250/10402 chunks (31.2%)\n",
      "Embedded 3300/10402 chunks (31.7%)\n",
      "Embedded 3350/10402 chunks (32.2%)\n",
      "Embedded 3400/10402 chunks (32.7%)\n",
      "Embedded 3450/10402 chunks (33.2%)\n",
      "Embedded 3500/10402 chunks (33.6%)\n",
      "Embedded 3550/10402 chunks (34.1%)\n",
      "Embedded 3600/10402 chunks (34.6%)\n",
      "Embedded 3650/10402 chunks (35.1%)\n",
      "Embedded 3700/10402 chunks (35.6%)\n",
      "Embedded 3750/10402 chunks (36.1%)\n",
      "Embedded 3800/10402 chunks (36.5%)\n",
      "Embedded 3850/10402 chunks (37.0%)\n",
      "Embedded 3900/10402 chunks (37.5%)\n",
      "Embedded 3950/10402 chunks (38.0%)\n",
      "Embedded 4000/10402 chunks (38.5%)\n",
      "Embedded 4050/10402 chunks (38.9%)\n",
      "Embedded 4100/10402 chunks (39.4%)\n",
      "Embedded 4150/10402 chunks (39.9%)\n",
      "Embedded 4200/10402 chunks (40.4%)\n",
      "Embedded 4250/10402 chunks (40.9%)\n",
      "Embedded 4300/10402 chunks (41.3%)\n",
      "Embedded 4350/10402 chunks (41.8%)\n",
      "Embedded 4400/10402 chunks (42.3%)\n",
      "Embedded 4450/10402 chunks (42.8%)\n",
      "Embedded 4500/10402 chunks (43.3%)\n",
      "Embedded 4550/10402 chunks (43.7%)\n",
      "Embedded 4600/10402 chunks (44.2%)\n",
      "Embedded 4650/10402 chunks (44.7%)\n",
      "Embedded 4700/10402 chunks (45.2%)\n",
      "Embedded 4750/10402 chunks (45.7%)\n",
      "Embedded 4800/10402 chunks (46.1%)\n",
      "Embedded 4850/10402 chunks (46.6%)\n",
      "Embedded 4900/10402 chunks (47.1%)\n",
      "Embedded 4950/10402 chunks (47.6%)\n",
      "Embedded 5000/10402 chunks (48.1%)\n",
      "Embedded 5050/10402 chunks (48.5%)\n",
      "Embedded 5100/10402 chunks (49.0%)\n",
      "Embedded 5150/10402 chunks (49.5%)\n",
      "Embedded 5200/10402 chunks (50.0%)\n",
      "Embedded 5250/10402 chunks (50.5%)\n",
      "Embedded 5300/10402 chunks (51.0%)\n",
      "Embedded 5350/10402 chunks (51.4%)\n",
      "Embedded 5400/10402 chunks (51.9%)\n",
      "Embedded 5450/10402 chunks (52.4%)\n",
      "Embedded 5500/10402 chunks (52.9%)\n",
      "Embedded 5550/10402 chunks (53.4%)\n",
      "Embedded 5600/10402 chunks (53.8%)\n",
      "Embedded 5650/10402 chunks (54.3%)\n",
      "Embedded 5700/10402 chunks (54.8%)\n",
      "Embedded 5750/10402 chunks (55.3%)\n",
      "Embedded 5800/10402 chunks (55.8%)\n",
      "Embedded 5850/10402 chunks (56.2%)\n",
      "Embedded 5900/10402 chunks (56.7%)\n",
      "Embedded 5950/10402 chunks (57.2%)\n",
      "Embedded 6000/10402 chunks (57.7%)\n",
      "Embedded 6050/10402 chunks (58.2%)\n",
      "Embedded 6100/10402 chunks (58.6%)\n",
      "Embedded 6150/10402 chunks (59.1%)\n",
      "Embedded 6200/10402 chunks (59.6%)\n",
      "Embedded 6250/10402 chunks (60.1%)\n",
      "Embedded 6300/10402 chunks (60.6%)\n",
      "Embedded 6350/10402 chunks (61.0%)\n",
      "Embedded 6400/10402 chunks (61.5%)\n",
      "Embedded 6450/10402 chunks (62.0%)\n",
      "Embedded 6500/10402 chunks (62.5%)\n",
      "Embedded 6550/10402 chunks (63.0%)\n",
      "Embedded 6600/10402 chunks (63.4%)\n",
      "Embedded 6650/10402 chunks (63.9%)\n",
      "Embedded 6700/10402 chunks (64.4%)\n",
      "Embedded 6750/10402 chunks (64.9%)\n",
      "Embedded 6800/10402 chunks (65.4%)\n",
      "Embedded 6850/10402 chunks (65.9%)\n",
      "Embedded 6900/10402 chunks (66.3%)\n",
      "Embedded 6950/10402 chunks (66.8%)\n",
      "Embedded 7000/10402 chunks (67.3%)\n",
      "Embedded 7050/10402 chunks (67.8%)\n",
      "Embedded 7100/10402 chunks (68.3%)\n",
      "Embedded 7150/10402 chunks (68.7%)\n",
      "Embedded 7200/10402 chunks (69.2%)\n",
      "Embedded 7250/10402 chunks (69.7%)\n",
      "Embedded 7300/10402 chunks (70.2%)\n",
      "Embedded 7350/10402 chunks (70.7%)\n",
      "Embedded 7400/10402 chunks (71.1%)\n",
      "Embedded 7450/10402 chunks (71.6%)\n",
      "Embedded 7500/10402 chunks (72.1%)\n",
      "Embedded 7550/10402 chunks (72.6%)\n",
      "Embedded 7600/10402 chunks (73.1%)\n",
      "Embedded 7650/10402 chunks (73.5%)\n",
      "Embedded 7700/10402 chunks (74.0%)\n",
      "Embedded 7750/10402 chunks (74.5%)\n",
      "Embedded 7800/10402 chunks (75.0%)\n",
      "Embedded 7850/10402 chunks (75.5%)\n",
      "Embedded 7900/10402 chunks (75.9%)\n",
      "Embedded 7950/10402 chunks (76.4%)\n",
      "Embedded 8000/10402 chunks (76.9%)\n",
      "Embedded 8050/10402 chunks (77.4%)\n",
      "Embedded 8100/10402 chunks (77.9%)\n",
      "Embedded 8150/10402 chunks (78.4%)\n",
      "Embedded 8200/10402 chunks (78.8%)\n",
      "Embedded 8250/10402 chunks (79.3%)\n",
      "Embedded 8300/10402 chunks (79.8%)\n",
      "Embedded 8350/10402 chunks (80.3%)\n",
      "Embedded 8400/10402 chunks (80.8%)\n",
      "Embedded 8450/10402 chunks (81.2%)\n",
      "Embedded 8500/10402 chunks (81.7%)\n",
      "Embedded 8550/10402 chunks (82.2%)\n",
      "Embedded 8600/10402 chunks (82.7%)\n",
      "Embedded 8650/10402 chunks (83.2%)\n",
      "Embedded 8700/10402 chunks (83.6%)\n",
      "Embedded 8750/10402 chunks (84.1%)\n",
      "Embedded 8800/10402 chunks (84.6%)\n",
      "Embedded 8850/10402 chunks (85.1%)\n",
      "Embedded 8900/10402 chunks (85.6%)\n",
      "Embedded 8950/10402 chunks (86.0%)\n",
      "Embedded 9000/10402 chunks (86.5%)\n",
      "Embedded 9050/10402 chunks (87.0%)\n",
      "Embedded 9100/10402 chunks (87.5%)\n",
      "Embedded 9150/10402 chunks (88.0%)\n",
      "Embedded 9200/10402 chunks (88.4%)\n",
      "Embedded 9250/10402 chunks (88.9%)\n",
      "Embedded 9300/10402 chunks (89.4%)\n",
      "Embedded 9350/10402 chunks (89.9%)\n",
      "Embedded 9400/10402 chunks (90.4%)\n",
      "Embedded 9450/10402 chunks (90.8%)\n",
      "Embedded 9500/10402 chunks (91.3%)\n",
      "Embedded 9550/10402 chunks (91.8%)\n",
      "Embedded 9600/10402 chunks (92.3%)\n",
      "Embedded 9650/10402 chunks (92.8%)\n",
      "Embedded 9700/10402 chunks (93.3%)\n",
      "Embedded 9750/10402 chunks (93.7%)\n",
      "Embedded 9800/10402 chunks (94.2%)\n",
      "Embedded 9850/10402 chunks (94.7%)\n",
      "Embedded 9900/10402 chunks (95.2%)\n",
      "Embedded 9950/10402 chunks (95.7%)\n",
      "Embedded 10000/10402 chunks (96.1%)\n",
      "Embedded 10050/10402 chunks (96.6%)\n",
      "Embedded 10100/10402 chunks (97.1%)\n",
      "Embedded 10150/10402 chunks (97.6%)\n",
      "Embedded 10200/10402 chunks (98.1%)\n",
      "Embedded 10250/10402 chunks (98.5%)\n",
      "Embedded 10300/10402 chunks (99.0%)\n",
      "Embedded 10350/10402 chunks (99.5%)\n",
      "Embedded 10400/10402 chunks (100.0%)\n",
      "\n",
      "✓ Vector database ready with 10402 embeddings!\n"
     ]
    }
   ],
   "source": [
    "print(f'Building vector database with {len(dataset)} chunks...')\n",
    "print('This may take a few minutes...\\n')\n",
    "\n",
    "for i, chunk in enumerate(dataset):\n",
    "    add_chunk_to_database(chunk)\n",
    "    \n",
    "    # Progress update every 50 chunks\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f'Embedded {i+1}/{len(dataset)} chunks ({(i+1)/len(dataset)*100:.1f}%)')\n",
    "\n",
    "print(f'\\n✓ Vector database ready with {len(VECTOR_DB)} embeddings!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864c65a",
   "metadata": {},
   "source": [
    "## Implement the Retrieval Function\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "To find the most relevant chunks, we need to compare vector similarity. We'll use cosine similarity, which measures how \"close\" two vectors are in the vector space. Higher cosine similarity means more similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a204b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11352532",
   "metadata": {},
   "source": [
    "### Retrieval Function\n",
    "\n",
    "The retrieval function:\n",
    "1. Converts the query into an embedding vector\n",
    "2. Compares it against all vectors in the database\n",
    "3. Returns the top N most relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99953b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    \"\"\"Retrieve the top N most relevant chunks for a given query.\n",
    "    \n",
    "    Uses the configured storage backend (memory, JSON, or PostgreSQL).\n",
    "    \"\"\"\n",
    "    query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    \n",
    "    if STORAGE_BACKEND == 'postgresql':\n",
    "        # Use PostgreSQL pgvector for similarity search\n",
    "        return PG_DB.similarity_search(query_embedding, top_n)\n",
    "    else:\n",
    "        # Use in-memory cosine similarity\n",
    "        # temporary list to store (chunk, similarity) pairs\n",
    "        similarities = []\n",
    "        for chunk, embedding in VECTOR_DB:\n",
    "            similarity = cosine_similarity(query_embedding, embedding)\n",
    "            similarities.append((chunk, similarity))\n",
    "        # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        # finally, return the top N most relevant chunks\n",
    "        return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11d6bc",
   "metadata": {},
   "source": [
    "## Generation Phase\n",
    "\n",
    "In the generation phase, the chatbot generates a response based on the retrieved knowledge. We construct a prompt that includes the relevant chunks and instruct the model to only use that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2db2157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query, top_n=3, verbose=True):\n",
    "    \"\"\"Ask a question and get a response based on retrieved knowledge.\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask\n",
    "        top_n: Number of relevant chunks to retrieve\n",
    "        verbose: Whether to print retrieved knowledge\n",
    "    \n",
    "    Returns:\n",
    "        The chatbot's response as a string\n",
    "    \"\"\"\n",
    "    # Retrieve relevant knowledge\n",
    "    retrieved_knowledge = retrieve(query, top_n=top_n)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Retrieved knowledge:')\n",
    "        for i, (chunk, similarity) in enumerate(retrieved_knowledge):\n",
    "            # Extract title from chunk\n",
    "            title_line = chunk.split('\\n')[0]\n",
    "            preview = chunk[:200].replace('\\n', ' ') + '...' if len(chunk) > 200 else chunk\n",
    "            print(f'  [{i+1}] (similarity: {similarity:.3f}) {preview}')\n",
    "        print()\n",
    "    \n",
    "    # Construct the instruction prompt with retrieved context\n",
    "    instruction_prompt = f'''You are a helpful chatbot that answers questions based on Wikipedia articles.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information.\n",
    "If the context doesn't contain enough information to answer the question, say so.\n",
    "\n",
    "Context:\n",
    "{chr(10).join([f'{i+1}. {chunk.strip()}' for i, (chunk, _) in enumerate(retrieved_knowledge)])}\n",
    "'''\n",
    "    \n",
    "    # Generate response\n",
    "    stream = ollama.chat(\n",
    "        model=LANGUAGE_MODEL,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': instruction_prompt},\n",
    "            {'role': 'user', 'content': query},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    # Collect and print the response\n",
    "    if verbose:\n",
    "        print('Chatbot response:')\n",
    "    \n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        response += content\n",
    "        if verbose:\n",
    "            print(content, end='', flush=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n')  # ensure a newline after the streamed response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988ae81",
   "metadata": {},
   "source": [
    "## Try It Out!\n",
    "\n",
    "Now let's ask some questions. The quality of answers will depend on which articles were included in your dataset sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "179f3c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      "  [1] (similarity: 0.686) Article: Paris  Paris (nicknamed the \"City of light\") is the capital city of France, and the largest city in France. The area is , and around 2.15 million people live there. If suburbs are counted, th...\n",
      "  [2] (similarity: 0.644) Article: France  France ( or ; ), officially the French Republic (, ), is a country in Western Europe. It also includes various departments and territories of France overseas.   Mainland France extend...\n",
      "  [3] (similarity: 0.617) Article: France  France was one of the first members of the European Union, and has the largest land area of all members. It is also a founding member of the United Nations, and a member of the Franco...\n",
      "\n",
      "Chatbot response:\n",
      "The article does not directly state that Paris is the capital of France. It mentions \"the capital city of France\" but does not specify which one. However, it also states that the area around Paris is the largest in France and has a population of 10.7 million people, suggesting that Paris may be considered the capital due to its size and influence.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The article does not directly state that Paris is the capital of France. It mentions \"the capital city of France\" but does not specify which one. However, it also states that the area around Paris is the largest in France and has a population of 10.7 million people, suggesting that Paris may be considered the capital due to its size and influence.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71c4dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      "  [1] (similarity: 0.777) Article: Albert Einstein  Albert Einstein (14 March 1879 – 18 April 1955) was a German-born American scientist. He worked on theoretical physics. He developed the theory of relativity. He received the...\n",
      "  [2] (similarity: 0.764) Article: Albert Einstein  He is now thought to be one of the greatest scientists of all time.  His contributions helped lay the foundations for all modern branches of physics, including quantum mechan...\n",
      "  [3] (similarity: 0.760) Article: Albert Einstein  Later life  In spring of 1914, he moved back to Germany, and became ordinary member of the Prussian Academy and director of a newly established institute for physics of the K...\n",
      "\n",
      "Chatbot response:\n",
      "According to Wikipedia, Albert Einstein (1879-1955) was a renowned German-born physicist who is best known for his theory of relativity. Here are some key facts about his life:\n",
      "\n",
      "**Early Life**\n",
      "\n",
      "Einstein was born in Munich, Germany on March 14, 1879. He was the youngest of three children to Hermann and Pauline Einstein.\n",
      "\n",
      "**Education and Career**\n",
      "\n",
      "Einstein's early education was at a Catholic elementary school, followed by a private Jewish boarding school. In 1894, he enrolled at the Swiss Federal Polytechnic University in Zurich, where he studied physics and mathematics.\n",
      "\n",
      "In 1900, Einstein moved to Berlin, Germany to pursue his graduate studies at the University of Berlin. He earned his Ph.D. in 1905 while working as a patent clerk.\n",
      "\n",
      "**Theory of Relativity**\n",
      "\n",
      "Einstein's most famous contribution is his theory of relativity, which revolutionized our understanding of space and time. The theory consists of two main components:\n",
      "\n",
      "1. **Special Relativity**: Einstein proposed that the laws of physics are the same for all observers in uniform motion relative to one another.\n",
      "2. **General Relativity**: He later expanded his theory to include gravity by introducing the concept of curvature of spacetime.\n",
      "\n",
      "**Nobel Prize**\n",
      "\n",
      "In 1921, Einstein was awarded the Nobel Prize in Physics \"in recognition of his services to physics\" for his explanation of the photoelectric effect.\n",
      "\n",
      "**Other Achievements**\n",
      "\n",
      "Einstein's work extended beyond physics. He was a strong advocate for peace and civil rights, and he spoke out against racism and nationalism.\n",
      "\n",
      "**Later Life and Death**\n",
      "\n",
      "In 1914, Einstein moved back to Germany, where he became an ordinary member of the Prussian Academy and director of the Kaiser-Wilhelm-Gesellschaft institute for physics. In 1922, he received the Nobel Prize in Physics again. He continued to work as a professor at the University of Berlin until his retirement in 1933.\n",
      "\n",
      "Einstein died on April 18, 1955, due to complications from an abdominal aortic aneurysm (a burst blood vessel).\n",
      "\n",
      "**Legacy**\n",
      "\n",
      "Albert Einstein's legacy is immense. His theory of relativity has had a profound impact on our understanding of the universe, and he is widely regarded as one of the greatest scientists of all time.\n",
      "\n",
      "In 2010, the International Astronomical Union designated Einstein's speed limit in relativity to be approximately 299,792,458 meters per second (which we now know is incredibly close to the speed of light). This speed limit is often referred to as the \"Einstein limit.\"\n",
      "\n",
      "Overall, Albert Einstein was a towering figure in the world of science, and his work continues to inspire new generations of scientists and thinkers.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to Wikipedia, Albert Einstein (1879-1955) was a renowned German-born physicist who is best known for his theory of relativity. Here are some key facts about his life:\\n\\n**Early Life**\\n\\nEinstein was born in Munich, Germany on March 14, 1879. He was the youngest of three children to Hermann and Pauline Einstein.\\n\\n**Education and Career**\\n\\nEinstein\\'s early education was at a Catholic elementary school, followed by a private Jewish boarding school. In 1894, he enrolled at the Swiss Federal Polytechnic University in Zurich, where he studied physics and mathematics.\\n\\nIn 1900, Einstein moved to Berlin, Germany to pursue his graduate studies at the University of Berlin. He earned his Ph.D. in 1905 while working as a patent clerk.\\n\\n**Theory of Relativity**\\n\\nEinstein\\'s most famous contribution is his theory of relativity, which revolutionized our understanding of space and time. The theory consists of two main components:\\n\\n1. **Special Relativity**: Einstein proposed that the laws of physics are the same for all observers in uniform motion relative to one another.\\n2. **General Relativity**: He later expanded his theory to include gravity by introducing the concept of curvature of spacetime.\\n\\n**Nobel Prize**\\n\\nIn 1921, Einstein was awarded the Nobel Prize in Physics \"in recognition of his services to physics\" for his explanation of the photoelectric effect.\\n\\n**Other Achievements**\\n\\nEinstein\\'s work extended beyond physics. He was a strong advocate for peace and civil rights, and he spoke out against racism and nationalism.\\n\\n**Later Life and Death**\\n\\nIn 1914, Einstein moved back to Germany, where he became an ordinary member of the Prussian Academy and director of the Kaiser-Wilhelm-Gesellschaft institute for physics. In 1922, he received the Nobel Prize in Physics again. He continued to work as a professor at the University of Berlin until his retirement in 1933.\\n\\nEinstein died on April 18, 1955, due to complications from an abdominal aortic aneurysm (a burst blood vessel).\\n\\n**Legacy**\\n\\nAlbert Einstein\\'s legacy is immense. His theory of relativity has had a profound impact on our understanding of the universe, and he is widely regarded as one of the greatest scientists of all time.\\n\\nIn 2010, the International Astronomical Union designated Einstein\\'s speed limit in relativity to be approximately 299,792,458 meters per second (which we now know is incredibly close to the speed of light). This speed limit is often referred to as the \"Einstein limit.\"\\n\\nOverall, Albert Einstein was a towering figure in the world of science, and his work continues to inspire new generations of scientists and thinkers.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Tell me about Albert Einstein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f23221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      "  [1] (similarity: 0.766) Article: Programming language  A programming language is a type of written language that tells computers what to do. Examples are: Python, Ruby, Java, JavaScript, C, C++, and C#. Programming languages...\n",
      "  [2] (similarity: 0.714) Article: Programming language  Usually, the programming language uses real words for some of the commands (e.g. \"if... then... else...\", \"and\", \"or\"), so that the language is easier for a human to und...\n",
      "  [3] (similarity: 0.656) Article: Visual Basic  Visual Basic (VB) is a programming language developed by Microsoft for their operating system Windows. The BASIC language is said to be easier to read than other languages.   Vi...\n",
      "\n",
      "Chatbot response:\n",
      "Python is a popular programming language that tells computers what to do. It's like a set of commands that tell the computer how to do things. Python usually uses real words for some of the commands, making it easier for humans to understand.\n",
      "\n",
      "The article doesn't mention anything about the syntax or features of the Python programming language itself, but I can tell you that:\n",
      "\n",
      "* Python is often used for general-purpose programming and data analysis.\n",
      "* It's written using simple English-like words and syntax, which makes it easy for humans to read and write.\n",
      "* The language has a vast range of applications, including web development, scientific computing, artificial intelligence, and more.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Python is a popular programming language that tells computers what to do. It's like a set of commands that tell the computer how to do things. Python usually uses real words for some of the commands, making it easier for humans to understand.\\n\\nThe article doesn't mention anything about the syntax or features of the Python programming language itself, but I can tell you that:\\n\\n* Python is often used for general-purpose programming and data analysis.\\n* It's written using simple English-like words and syntax, which makes it easy for humans to read and write.\\n* The language has a vast range of applications, including web development, scientific computing, artificial intelligence, and more.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"What is Python programming language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "climate_question",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      "  [1] (similarity: 0.822) Article: Photosynthesis  Photosynthesis is how plants and some microorganisms make  carbohydrates. It is an endothermic (takes in heat) chemical process which uses sunlight to turn carbon dioxide into...\n",
      "  [2] (similarity: 0.784) Article: Photosynthesis  6 CO2(g) + 6 H2O + photons → C6H12O6(aq) + 6 O2(g) carbon dioxide + water + light energy → glucose + oxygen Carbon dioxide enters the leaf through the stomata by diffusion fro...\n",
      "  [3] (similarity: 0.775) Article: Photosynthesis  Glucose is used in respiration (to release energy in cells). It is stored in the form of starch (which is converted back to glucose for respiration in the dark). Glucose can a...\n",
      "\n",
      "Chatbot response:\n",
      "Photosynthesis works by using light energy from the sun to convert carbon dioxide (CO2) and water (H2O) into glucose (C6H12O6) and oxygen (O2). The process can be broken down into two main phases: light-dependent reactions and light-independent reactions.\n",
      "\n",
      "**Light-Dependent Reactions**\n",
      "\n",
      "1. Light energy from the sun is absorbed by pigments such as chlorophyll in the thylakoid membranes of chloroplasts.\n",
      "2. This energy excites electrons, which are then passed through a series of electron transport chains.\n",
      "3. The electrons ultimately reduce oxygen (O2) and water (H2O) to form a molecule called hydroquinone.\n",
      "\n",
      "**Light-Independent Reactions**\n",
      "\n",
      "1. Hydroquinone is converted into flavin mononucleotide (FMN) by the enzyme photosystem I.\n",
      "2. This process is known as electron transport, where energy from light is passed along a series of protein complexes and electron carriers.\n",
      "3. The electrons are then used to produce ATP (adenosine triphosphate), which is a molecule that provides energy for cellular processes.\n",
      "\n",
      "**Overall Equation**\n",
      "\n",
      "6 CO2(g) + 6 H2O(l) + photons → C6H12O6(aq) + 6 O2(g)\n",
      "\n",
      "In summary, photosynthesis occurs in two main phases:\n",
      "\n",
      "1. Light-dependent reactions: Energy from light is absorbed and used to split water molecules.\n",
      "2. Light-independent reactions: The energy from the split water molecules is used to produce ATP and NADPH (a molecule that plays a crucial role in cellular respiration).\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Photosynthesis works by using light energy from the sun to convert carbon dioxide (CO2) and water (H2O) into glucose (C6H12O6) and oxygen (O2). The process can be broken down into two main phases: light-dependent reactions and light-independent reactions.\\n\\n**Light-Dependent Reactions**\\n\\n1. Light energy from the sun is absorbed by pigments such as chlorophyll in the thylakoid membranes of chloroplasts.\\n2. This energy excites electrons, which are then passed through a series of electron transport chains.\\n3. The electrons ultimately reduce oxygen (O2) and water (H2O) to form a molecule called hydroquinone.\\n\\n**Light-Independent Reactions**\\n\\n1. Hydroquinone is converted into flavin mononucleotide (FMN) by the enzyme photosystem I.\\n2. This process is known as electron transport, where energy from light is passed along a series of protein complexes and electron carriers.\\n3. The electrons are then used to produce ATP (adenosine triphosphate), which is a molecule that provides energy for cellular processes.\\n\\n**Overall Equation**\\n\\n6 CO2(g) + 6 H2O(l) + photons → C6H12O6(aq) + 6 O2(g)\\n\\nIn summary, photosynthesis occurs in two main phases:\\n\\n1. Light-dependent reactions: Energy from light is absorbed and used to split water molecules.\\n2. Light-independent reactions: The energy from the split water molecules is used to produce ATP and NADPH (a molecule that plays a crucial role in cellular respiration).\\n\\nI hope this helps! Let me know if you have any further questions.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"How does photosynthesis work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370c0c5",
   "metadata": {},
   "source": [
    "## Interactive Chat\n",
    "\n",
    "You can also use this cell to ask your own questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "865c1651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      "  [1] (similarity: 0.777) Article: Solar System  The Solar System is the Sun and all the objects that orbit around it. The Sun is orbited by planets, asteroids, comets and other things.   The Solar System is about 4.568 billio...\n",
      "  [2] (similarity: 0.743) Article: Solar System  The Solar System also contains other things. There are asteroid belts, mostly between Mars and Jupiter. Further out than Neptune, there is the Kuiper belt and the scattered disc...\n",
      "  [3] (similarity: 0.741) Article: Solar System  There are eight planets in the Solar System. From closest to farthest from the Sun, they are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus and Neptune. The first four pl...\n",
      "\n",
      "Chatbot response:\n",
      "According to Article 2: The Solar System, the Solar System refers to:\n",
      "\n",
      "\"The entire system consisting of all objects that orbit around the Sun. It also includes the asteroid belt, the Kuiper belt and the scattered disc, as well as dwarf planets, moons, asteroids, comets, centaurs, and interplanetary dust.\"\n",
      "\n",
      "(Note: This definition is very concise and doesn't provide much detail about what makes up the Solar System.)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to Article 2: The Solar System, the Solar System refers to:\\n\\n\"The entire system consisting of all objects that orbit around the Sun. It also includes the asteroid belt, the Kuiper belt and the scattered disc, as well as dwarf planets, moons, asteroids, comets, centaurs, and interplanetary dust.\"\\n\\n(Note: This definition is very concise and doesn\\'t provide much detail about what makes up the Solar System.)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask your own question here\n",
    "your_question = \"What is the solar system?\"\n",
    "ask_question(your_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_cell",
   "metadata": {},
   "source": [
    "## Export Dataset for Other Platforms\n",
    "\n",
    "You can export the dataset for use with Neon (Vercel) or Cloudflare D1 with Vectorize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported 10402 chunks to wikipedia_vectorize_export.json\n",
      "  File size: 110.47 MB\n",
      "\n",
      "You can now use this file with:\n",
      "  - Neon (PostgreSQL with pgvector)\n",
      "  - Cloudflare D1 with Vectorize\n",
      "  - Any other vector database\n"
     ]
    }
   ],
   "source": [
    "def export_for_vectorize(output_path='wikipedia_export.json'):\n",
    "    \"\"\"Export dataset in a format ready for Cloudflare Vectorize or Neon.\n",
    "    \n",
    "    The output format includes:\n",
    "    - id: unique identifier\n",
    "    - text: the chunk content\n",
    "    - embedding: the vector (optional, can be generated on the platform)\n",
    "    \"\"\"\n",
    "    export_data = []\n",
    "    \n",
    "    for i, (chunk, embedding) in enumerate(VECTOR_DB):\n",
    "        # Extract title from chunk\n",
    "        lines = chunk.split('\\n')\n",
    "        title = lines[0].replace('Article: ', '') if lines[0].startswith('Article: ') else 'Unknown'\n",
    "        \n",
    "        export_data.append({\n",
    "            'id': f'chunk_{i}',\n",
    "            'text': chunk,\n",
    "            'title': title,\n",
    "            'embedding': embedding  # Include if you want pre-computed embeddings\n",
    "        })\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f'✓ Exported {len(export_data)} chunks to {output_path}')\n",
    "    print(f'  File size: {sys.getsizeof(json.dumps(export_data)) / (1024*1024):.2f} MB')\n",
    "    print('\\nYou can now use this file with:')\n",
    "    print('  - Neon (PostgreSQL with pgvector)')\n",
    "    print('  - Cloudflare D1 with Vectorize')\n",
    "    print('  - Any other vector database')\n",
    "\n",
    "# Uncomment to export:\n",
    "# export_for_vectorize('wikipedia_vectorize_export.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77f5ee",
   "metadata": {},
   "source": [
    "## Load Embeddings from PostgreSQL\n",
    "\n",
    "If you've previously generated embeddings and stored them in PostgreSQL, you can load them without regenerating:\n",
    "\n",
    "**Use this in a new notebook to:**\n",
    "- Run experiments with existing embeddings (avoiding 50+ minute regeneration)\n",
    "- Compare different embedding models stored in different tables\n",
    "- Analyze embedding quality without reprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ef9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_from_postgres(config, embedding_model_alias):\n",
    "    \"\"\"Load previously generated embeddings from PostgreSQL.\n",
    "    \n",
    "    Useful for running new experiments without regenerating embeddings.\n",
    "    \n",
    "    Args:\n",
    "        config: PostgreSQL connection config\n",
    "        embedding_model_alias: Alias used when the embeddings were generated\n",
    "    \n",
    "    Returns:\n",
    "        PostgreSQLVectorDB instance ready for retrieval\n",
    "    \"\"\"\n",
    "    table_name = f'embeddings_{embedding_model_alias.replace(\".\", \"_\")}'\n",
    "    \n",
    "    try:\n",
    "        db = PostgreSQLVectorDB(config, table_name)\n",
    "        count = db.get_chunk_count()\n",
    "        print(f'✓ Loaded {count} embeddings from table \"{table_name}\"')\n",
    "        return db\n",
    "    except psycopg2.ProgrammingError:\n",
    "        print(f'✗ Table \"{table_name}\" not found in database')\n",
    "        print('Run the main notebook first to generate and store embeddings.')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'✗ Error loading embeddings: {e}')\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example: Uncomment to load existing embeddings in a new notebook\n",
    "# loaded_db = load_embeddings_from_postgres(POSTGRES_CONFIG, 'bge_base_en_v1.5')\n",
    "# Then use: loaded_db.similarity_search(query_embedding, top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2d324",
   "metadata": {},
   "source": [
    "## Next Steps and Improvements\n",
    "\n",
    "### Migrate to Production Vector Databases\n",
    "\n",
    "**Neon (with Vercel):**\n",
    "```sql\n",
    "-- Create table with pgvector\n",
    "CREATE TABLE wikipedia_chunks (\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  title TEXT,\n",
    "  text TEXT,\n",
    "  embedding vector(768)  -- dimension depends on your model\n",
    ");\n",
    "\n",
    "-- Create index for fast similarity search\n",
    "CREATE INDEX ON wikipedia_chunks \n",
    "USING ivfflat (embedding vector_cosine_ops);\n",
    "```\n",
    "\n",
    "**Cloudflare D1 with Vectorize:**\n",
    "```javascript\n",
    "// Use Vectorize for embeddings, D1 for metadata\n",
    "await env.VECTORIZE.insert([\n",
    "  {\n",
    "    id: 'chunk_1',\n",
    "    values: embedding,\n",
    "    metadata: { title: 'Article Title', text: 'chunk text' }\n",
    "  }\n",
    "]);\n",
    "```\n",
    "\n",
    "### Other Improvements\n",
    "\n",
    "1. **Hybrid Search**: Combine vector similarity with keyword search (BM25) for better retrieval\n",
    "\n",
    "2. **Reranking**: Use a [reranking model](https://www.pinecone.io/learn/series/rag/rerankers/) to re-score retrieved chunks\n",
    "\n",
    "3. **Query Expansion**: Generate multiple variations of the user's question for better coverage\n",
    "\n",
    "4. **Metadata Filtering**: Filter by article categories, dates, or other metadata before similarity search\n",
    "\n",
    "5. **Better Chunking**: Implement semantic chunking that preserves context better\n",
    "\n",
    "6. **Citation Support**: Track which chunks were used and provide Wikipedia URLs as sources\n",
    "\n",
    "### Advanced RAG Architectures\n",
    "\n",
    "- **Graph RAG**: Build knowledge graphs from Wikipedia's link structure\n",
    "- **Hybrid RAG**: Combine vectors, graphs, and keyword search\n",
    "- **Agentic RAG**: Let the LLM decide when to retrieve more information\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "- **Batch Embeddings**: Embed multiple chunks at once for faster indexing\n",
    "- **Approximate Search**: Use FAISS, Annoy, or HNSW for faster similarity search\n",
    "- **Caching**: Cache frequent queries and their results\n",
    "\n",
    "Learn more about RAG patterns in the [HuggingFace RAG guide](https://huggingface.co/blog/ngxson/make-your-own-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_section",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "View statistics about your loaded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "stats_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "  Total chunks: 10,402\n",
      "  Unique articles: 1,993\n",
      "  Total characters: 7,968,175\n",
      "  Average chunk size: 766 characters\n",
      "  Estimated size: 30.98 MB\n",
      "\n",
      "  Embeddings in database: 10,402\n",
      "  Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_stats():\n",
    "    \"\"\"Print statistics about the current dataset.\"\"\"\n",
    "    total_chars = sum(len(chunk) for chunk in dataset)\n",
    "    avg_chunk_size = total_chars / len(dataset) if dataset else 0\n",
    "    \n",
    "    # Count unique articles\n",
    "    articles = set()\n",
    "    for chunk in dataset:\n",
    "        if chunk.startswith('Article: '):\n",
    "            title = chunk.split('\\n')[0].replace('Article: ', '')\n",
    "            articles.add(title)\n",
    "    \n",
    "    print('Dataset Statistics:')\n",
    "    print(f'  Total chunks: {len(dataset):,}')\n",
    "    print(f'  Unique articles: {len(articles):,}')\n",
    "    print(f'  Total characters: {total_chars:,}')\n",
    "    print(f'  Average chunk size: {avg_chunk_size:.0f} characters')\n",
    "    print(f'  Estimated size: {sys.getsizeof(str(dataset)) / (1024*1024):.2f} MB')\n",
    "    print(f'\\n  Embeddings in database: {len(VECTOR_DB):,}')\n",
    "    print(f'  Embedding dimension: {len(VECTOR_DB[0][1]) if VECTOR_DB else 0}')\n",
    "\n",
    "print_dataset_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
