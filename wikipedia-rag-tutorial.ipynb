{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4644e21",
   "metadata": {},
   "source": [
    "# RAG Tutorial with Wikipedia Dataset\n",
    "\n",
    "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) system using Simple Wikipedia articles. The dataset is configurable by size, making it easy to experiment with different amounts of data while staying within free tier limits.\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "Before running this notebook, you need to:\n",
    "\n",
    "1. Install Ollama from [ollama.com](https://ollama.com/)\n",
    "2. Download the required models by running these commands in your terminal:\n",
    "\n",
    "```bash\n",
    "ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\n",
    "ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n",
    "```\n",
    "\n",
    "3. Install the required Python packages:\n",
    "\n",
    "```bash\n",
    "pip install ollama datasets ipywidgets jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3198d7f",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e96af9",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the target dataset size. The script will download articles until it reaches approximately this size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc326412",
   "metadata": {},
   "source": [
    "## Install Additional Dependencies\n",
    "\n",
    "If you plan to use PostgreSQL for persistent storage, install the additional dependency:\n",
    "\n",
    "```bash\n",
    "pip install psycopg2-binary\n",
    "```\n",
    "\n",
    "Or if you're using a virtual environment (recommended):\n",
    "\n",
    "```bash\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "pip install psycopg2-binary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target dataset size in MB (adjust as needed: 10, 20, 30, 40, 50)\n",
    "TARGET_SIZE_MB = 10\n",
    "\n",
    "# Maximum chunk size in characters (for splitting long articles)\n",
    "MAX_CHUNK_SIZE = 1000\n",
    "\n",
    "# Whether to save the dataset locally for reuse\n",
    "SAVE_LOCALLY = True\n",
    "LOCAL_DATASET_PATH = f'wikipedia_dataset_{TARGET_SIZE_MB}mb.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b99ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage backend configuration\n",
    "STORAGE_BACKEND = 'postgresql'  # Options: 'memory', 'json', 'postgresql'\n",
    "\n",
    "# PostgreSQL configuration (only used if STORAGE_BACKEND == 'postgresql')\n",
    "POSTGRES_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'rag_db',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "}\n",
    "\n",
    "# Table name for this embedding model (allows storing multiple models)\n",
    "# Table name will be: embeddings_{EMBEDDING_MODEL_ALIAS}\n",
    "EMBEDDING_MODEL_ALIAS = 'bge_base_en_v1.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_loading",
   "metadata": {},
   "source": [
    "## Load and Filter the Wikipedia Dataset\n",
    "\n",
    "We'll use Simple Wikipedia, which has cleaner, more concise articles. The dataset will be filtered to approximately your target size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_size_mb(text):\n",
    "    \"\"\"Estimate the size of text in megabytes.\"\"\"\n",
    "    return sys.getsizeof(text) / (1024 * 1024)\n",
    "\n",
    "def chunk_text(text, max_size=1000):\n",
    "    \"\"\"Split text into chunks of approximately max_size characters.\n",
    "    \n",
    "    Tries to break at paragraph boundaries when possible.\n",
    "    \"\"\"\n",
    "    if len(text) <= max_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    current_chunk = ''\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # If adding this paragraph would exceed max_size\n",
    "        if len(current_chunk) + len(paragraph) > max_size:\n",
    "            if current_chunk:  # Save current chunk if not empty\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = ''\n",
    "            \n",
    "            # If single paragraph is too large, split it\n",
    "            if len(paragraph) > max_size:\n",
    "                sentences = paragraph.split('. ')\n",
    "                for sentence in sentences:\n",
    "                    if len(current_chunk) + len(sentence) > max_size:\n",
    "                        if current_chunk:\n",
    "                            chunks.append(current_chunk.strip())\n",
    "                        current_chunk = sentence + '. '\n",
    "                    else:\n",
    "                        current_chunk += sentence + '. '\n",
    "            else:\n",
    "                current_chunk = paragraph\n",
    "        else:\n",
    "            current_chunk += '\\n\\n' + paragraph if current_chunk else paragraph\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_wikipedia_dataset(target_size_mb, local_path=None):\n",
    "    \"\"\"Load and filter Wikipedia dataset to target size.\n",
    "    \n",
    "    Args:\n",
    "        target_size_mb: Target dataset size in megabytes\n",
    "        local_path: Path to save/load dataset locally\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Try to load from local cache first\n",
    "    if local_path:\n",
    "        try:\n",
    "            print(f'Attempting to load cached dataset from {local_path}...')\n",
    "            with open(local_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f'✓ Loaded {len(data[\"chunks\"])} chunks from cache')\n",
    "                print(f'  Estimated size: {data[\"size_mb\"]:.2f} MB')\n",
    "                return data['chunks']\n",
    "        except FileNotFoundError:\n",
    "            print('No cached dataset found, downloading from HuggingFace...')\n",
    "    \n",
    "    # Load Simple Wikipedia dataset\n",
    "    print('Loading Simple Wikipedia dataset (this may take a minute)...')\n",
    "    dataset = load_dataset('wikimedia/wikipedia', '20231101.simple', split='train', streaming=True)\n",
    "    \n",
    "    chunks = []\n",
    "    current_size_mb = 0\n",
    "    target_bytes = target_size_mb * 1024 * 1024\n",
    "    article_count = 0\n",
    "    \n",
    "    print(f'\\nCollecting articles (target: {target_size_mb} MB)...')\n",
    "    \n",
    "    for article in dataset:\n",
    "        # Skip very short articles\n",
    "        if len(article['text']) < 200:\n",
    "            continue\n",
    "        \n",
    "        # Create metadata-enriched chunks\n",
    "        article_chunks = chunk_text(article['text'], MAX_CHUNK_SIZE)\n",
    "        \n",
    "        for chunk in article_chunks:\n",
    "            # Add title context to help with retrieval\n",
    "            enriched_chunk = f\"Article: {article['title']}\\n\\n{chunk}\"\n",
    "            chunk_size = sys.getsizeof(enriched_chunk)\n",
    "            \n",
    "            chunks.append(enriched_chunk)\n",
    "            current_size_mb += chunk_size\n",
    "            \n",
    "            # Check if we've reached target size\n",
    "            if current_size_mb >= target_bytes:\n",
    "                break\n",
    "        \n",
    "        article_count += 1\n",
    "        \n",
    "        # Progress update every 50 articles\n",
    "        if article_count % 50 == 0:\n",
    "            print(f'  Progress: {current_size_mb / (1024*1024):.2f} MB ({article_count} articles, {len(chunks)} chunks)')\n",
    "        \n",
    "        if current_size_mb >= target_bytes:\n",
    "            break\n",
    "    \n",
    "    final_size_mb = current_size_mb / (1024 * 1024)\n",
    "    print(f'\\n✓ Dataset loaded: {len(chunks)} chunks from {article_count} articles')\n",
    "    print(f'  Estimated size: {final_size_mb:.2f} MB')\n",
    "    \n",
    "    # Save locally if requested\n",
    "    if local_path:\n",
    "        print(f'\\nSaving dataset to {local_path}...')\n",
    "        with open(local_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'size_mb': final_size_mb,\n",
    "                'chunk_count': len(chunks),\n",
    "                'article_count': article_count,\n",
    "                'chunks': chunks\n",
    "            }, f, ensure_ascii=False)\n",
    "        print('✓ Dataset saved for future use')\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_wikipedia_dataset(\n",
    "    TARGET_SIZE_MB, \n",
    "    LOCAL_DATASET_PATH if SAVE_LOCALLY else None\n",
    ")\n",
    "\n",
    "print(f'\\nReady to build vector database with {len(dataset)} chunks!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database helper functions for PostgreSQL storage\n",
    "\n",
    "class PostgreSQLVectorDB:\n",
    "    \"\"\"Helper class to manage embeddings in PostgreSQL with pgvector.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, table_name):\n",
    "        \"\"\"Initialize database connection.\n",
    "        \n",
    "        Args:\n",
    "            config: Dictionary with host, port, database, user, password\n",
    "            table_name: Name of the table for this embedding model\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.table_name = table_name\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "        self.setup_table()\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection.\"\"\"\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(\n",
    "                host=self.config['host'],\n",
    "                port=self.config['port'],\n",
    "                database=self.config['database'],\n",
    "                user=self.config['user'],\n",
    "                password=self.config['password']\n",
    "            )\n",
    "            print(f'✓ Connected to PostgreSQL at {self.config[\"host\"]}:{self.config[\"port\"]}')\n",
    "        except psycopg2.OperationalError as e:\n",
    "            print(f'✗ Failed to connect to PostgreSQL: {e}')\n",
    "            print('Make sure PostgreSQL is running with pgvector support.')\n",
    "            print('Start it with: docker run -d --name pgvector-rag \\\\')\n",
    "            print('  -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=rag_db \\\\')\n",
    "            print('  -p 5432:5432 -v pgvector_data:/var/lib/postgresql/data \\\\')\n",
    "            print('  pgvector/pgvector:pg16')\n",
    "            raise\n",
    "    \n",
    "    def setup_table(self):\n",
    "        \"\"\"Drop and recreate table for fresh experimental runs.\"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            # Enable pgvector extension\n",
    "            cur.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "            \n",
    "            # Drop existing table and its index\n",
    "            cur.execute(f'DROP TABLE IF EXISTS {self.table_name} CASCADE')\n",
    "            \n",
    "            # Create table with vector column\n",
    "            cur.execute(f'''\n",
    "                CREATE TABLE {self.table_name} (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    chunk_text TEXT NOT NULL,\n",
    "                    embedding vector(768),\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create index for fast similarity search\n",
    "            index_name = f'{self.table_name}_embedding_idx'\n",
    "            cur.execute(f'''\n",
    "                CREATE INDEX {index_name}\n",
    "                ON {self.table_name} USING hnsw (embedding vector_cosine_ops)\n",
    "            ''')\n",
    "            \n",
    "            self.conn.commit()\n",
    "            print(f'✓ Table \"{self.table_name}\" created (fresh start)')\n",
    "    \n",
    "    def insert_embedding(self, chunk, embedding):\n",
    "        \"\"\"Insert a chunk and its embedding into the database.\n",
    "        \n",
    "        Args:\n",
    "            chunk: The text chunk\n",
    "            embedding: The embedding vector (list of floats)\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                INSERT INTO {self.table_name} (chunk_text, embedding)\n",
    "                VALUES (%s, %s)\n",
    "            ''', (chunk, embedding))\n",
    "            self.conn.commit()\n",
    "    \n",
    "    def insert_batch(self, chunks_embeddings):\n",
    "        \"\"\"Batch insert multiple chunks and embeddings.\n",
    "        \n",
    "        Args:\n",
    "            chunks_embeddings: List of (chunk, embedding) tuples\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            execute_values(cur, f'''\n",
    "                INSERT INTO {self.table_name} (chunk_text, embedding)\n",
    "                VALUES %s\n",
    "            ''', chunks_embeddings, page_size=100)\n",
    "            self.conn.commit()\n",
    "    \n",
    "    def get_chunk_count(self):\n",
    "        \"\"\"Get the number of stored chunks.\"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'SELECT COUNT(*) FROM {self.table_name}')\n",
    "            return cur.fetchone()[0]\n",
    "    \n",
    "    def similarity_search(self, query_embedding, top_n=3):\n",
    "        \"\"\"Find most similar chunks using pgvector.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: The query embedding vector\n",
    "            top_n: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (chunk_text, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f'''\n",
    "                SELECT chunk_text, \n",
    "                       1 - (embedding <=> %s::vector) as similarity\n",
    "                FROM {self.table_name}\n",
    "                ORDER BY embedding <=> %s::vector\n",
    "                LIMIT %s\n",
    "            ''', (query_embedding, query_embedding, top_n))\n",
    "            \n",
    "            results = cur.fetchall()\n",
    "            return [(chunk, score) for chunk, score in results]\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "\n",
    "def get_storage_backend(backend_type, config=None, table_name=None):\n",
    "    \"\"\"Factory function to get the appropriate storage backend.\n",
    "    \n",
    "    Args:\n",
    "        backend_type: 'memory', 'json', or 'postgresql'\n",
    "        config: PostgreSQL config dict (required if backend_type is 'postgresql')\n",
    "        table_name: Table name (required if backend_type is 'postgresql')\n",
    "    \n",
    "    Returns:\n",
    "        Storage backend instance\n",
    "    \"\"\"\n",
    "    if backend_type == 'postgresql':\n",
    "        if not config or not table_name:\n",
    "            raise ValueError('PostgreSQL backend requires config and table_name')\n",
    "        return PostgreSQLVectorDB(config, table_name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample_data",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's look at a few examples from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample chunks from the dataset:\\n')\n",
    "for i, chunk in enumerate(dataset[:3]):\n",
    "    print(f'--- Chunk {i+1} ---')\n",
    "    print(chunk[:300] + '...' if len(chunk) > 300 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a326fc",
   "metadata": {},
   "source": [
    "## Configure Models\n",
    "\n",
    "We'll use two models:\n",
    "- **Embedding Model**: Converts text into vector representations\n",
    "- **Language Model**: Generates responses based on retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a7821",
   "metadata": {},
   "source": [
    "## Implement the Vector Database\n",
    "\n",
    "### Indexing Phase\n",
    "\n",
    "In the indexing phase, we:\n",
    "1. Break the dataset into chunks (already done during loading)\n",
    "2. Calculate embedding vectors for each chunk\n",
    "3. Store chunks with their embeddings in our vector database\n",
    "\n",
    "Each element in `VECTOR_DB` will be a tuple: `(chunk, embedding)`\n",
    "\n",
    "The embedding is a list of floats, for example: `[0.1, 0.04, -0.34, 0.21, ...]`\n",
    "\n",
    "**Note**: This may take a few minutes depending on your dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "VECTOR_DB = []\n",
    "\n",
    "# Initialize storage backend if using PostgreSQL\n",
    "PG_DB = None\n",
    "if STORAGE_BACKEND == 'postgresql':\n",
    "    table_name = f'embeddings_{EMBEDDING_MODEL_ALIAS.replace(\".\", \"_\")}'\n",
    "    PG_DB = get_storage_backend('postgresql', POSTGRES_CONFIG, table_name)\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "    \"\"\"Add a chunk and its embedding to the vector database.\n",
    "    \n",
    "    Stores in memory and/or PostgreSQL depending on STORAGE_BACKEND.\n",
    "    \"\"\"\n",
    "    embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "    \n",
    "    if STORAGE_BACKEND == 'memory' or STORAGE_BACKEND == 'json':\n",
    "        VECTOR_DB.append((chunk, embedding))\n",
    "    elif STORAGE_BACKEND == 'postgresql':\n",
    "        PG_DB.insert_embedding(chunk, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df5ca8",
   "metadata": {},
   "source": [
    "Now let's populate our vector database with all chunks from the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4948f",
   "metadata": {},
   "source": [
    "## Optional: Persistent Storage with PostgreSQL & pgvector\n",
    "\n",
    "**Note on Performance**: Embedding generation takes significant time (~50 minutes for 10MB of data). Consider using PostgreSQL with pgvector for durable storage so you can reuse embeddings across multiple experiments without regenerating them.\n",
    "\n",
    "### Why PostgreSQL + pgvector?\n",
    "\n",
    "- **Reusable Embeddings**: Generate embeddings once, use them across multiple notebooks and experiments\n",
    "- **Multiple Models**: Store embeddings from different embedding models in separate tables for comparison\n",
    "- **Durable Storage**: Embeddings survive notebook restarts\n",
    "- **Scalability**: Move to production vector databases more easily\n",
    "\n",
    "### Quick Start with Docker\n",
    "\n",
    "1. Install [Docker Desktop](https://www.docker.com/products/docker-desktop) if you haven't already\n",
    "2. Run PostgreSQL with pgvector:\n",
    "\n",
    "```bash\n",
    "docker run --name pgvector-rag \\\n",
    "  -e POSTGRES_PASSWORD=postgres \\\n",
    "  -e POSTGRES_DB=rag_db \\\n",
    "  -p 5432:5432 \\\n",
    "  -v pgvector_data:/var/lib/postgresql/data \\\n",
    "  pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "This creates a persistent volume (`pgvector_data`) so your data survives container restarts.\n",
    "\n",
    "### Configuration for Persistent Storage\n",
    "\n",
    "Set the storage backend in the configuration section below. Choose:\n",
    "- `'memory'` - In-memory only (fast but lost on notebook restart)\n",
    "- `'json'` - Local JSON file (persists but slower for large datasets)\n",
    "- `'postgresql'` - PostgreSQL with pgvector (recommended for experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Building vector database with {len(dataset)} chunks...')\n",
    "print('This may take a few minutes...\\n')\n",
    "\n",
    "for i, chunk in enumerate(dataset):\n",
    "    add_chunk_to_database(chunk)\n",
    "    \n",
    "    # Progress update every 50 chunks\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f'Embedded {i+1}/{len(dataset)} chunks ({(i+1)/len(dataset)*100:.1f}%)')\n",
    "\n",
    "# Get the correct count based on storage backend\n",
    "if STORAGE_BACKEND == 'postgresql':\n",
    "    embedding_count = PG_DB.get_chunk_count()\n",
    "else:\n",
    "    embedding_count = len(VECTOR_DB)\n",
    "\n",
    "print(f'\\n✓ Vector database ready with {embedding_count} embeddings!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864c65a",
   "metadata": {},
   "source": [
    "## Implement the Retrieval Function\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "To find the most relevant chunks, we need to compare vector similarity. We'll use cosine similarity, which measures how \"close\" two vectors are in the vector space. Higher cosine similarity means more similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11352532",
   "metadata": {},
   "source": [
    "### Retrieval Function\n",
    "\n",
    "The retrieval function:\n",
    "1. Converts the query into an embedding vector\n",
    "2. Compares it against all vectors in the database\n",
    "3. Returns the top N most relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99953b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    \"\"\"Retrieve the top N most relevant chunks for a given query.\n",
    "    \n",
    "    Uses the configured storage backend (memory, JSON, or PostgreSQL).\n",
    "    \"\"\"\n",
    "    query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "    \n",
    "    if STORAGE_BACKEND == 'postgresql':\n",
    "        # Use PostgreSQL pgvector for similarity search\n",
    "        return PG_DB.similarity_search(query_embedding, top_n)\n",
    "    else:\n",
    "        # Use in-memory cosine similarity\n",
    "        # temporary list to store (chunk, similarity) pairs\n",
    "        similarities = []\n",
    "        for chunk, embedding in VECTOR_DB:\n",
    "            similarity = cosine_similarity(query_embedding, embedding)\n",
    "            similarities.append((chunk, similarity))\n",
    "        # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        # finally, return the top N most relevant chunks\n",
    "        return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11d6bc",
   "metadata": {},
   "source": [
    "## Generation Phase\n",
    "\n",
    "In the generation phase, the chatbot generates a response based on the retrieved knowledge. We construct a prompt that includes the relevant chunks and instruct the model to only use that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query, top_n=3, verbose=True):\n",
    "    \"\"\"Ask a question and get a response based on retrieved knowledge.\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask\n",
    "        top_n: Number of relevant chunks to retrieve\n",
    "        verbose: Whether to print retrieved knowledge\n",
    "    \n",
    "    Returns:\n",
    "        The chatbot's response as a string\n",
    "    \"\"\"\n",
    "    # Retrieve relevant knowledge\n",
    "    retrieved_knowledge = retrieve(query, top_n=top_n)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Retrieved knowledge:')\n",
    "        for i, (chunk, similarity) in enumerate(retrieved_knowledge):\n",
    "            # Extract title from chunk\n",
    "            title_line = chunk.split('\\n')[0]\n",
    "            preview = chunk[:200].replace('\\n', ' ') + '...' if len(chunk) > 200 else chunk\n",
    "            print(f'  [{i+1}] (similarity: {similarity:.3f}) {preview}')\n",
    "        print()\n",
    "    \n",
    "    # Construct the instruction prompt with retrieved context\n",
    "    instruction_prompt = f'''You are a helpful chatbot that answers questions based on Wikipedia articles.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information.\n",
    "If the context doesn't contain enough information to answer the question, say so.\n",
    "\n",
    "Context:\n",
    "{chr(10).join([f'{i+1}. {chunk.strip()}' for i, (chunk, _) in enumerate(retrieved_knowledge)])}\n",
    "'''\n",
    "    \n",
    "    # Generate response\n",
    "    stream = ollama.chat(\n",
    "        model=LANGUAGE_MODEL,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': instruction_prompt},\n",
    "            {'role': 'user', 'content': query},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    # Collect and print the response\n",
    "    if verbose:\n",
    "        print('Chatbot response:')\n",
    "    \n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        response += content\n",
    "        if verbose:\n",
    "            print(content, end='', flush=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n')  # ensure a newline after the streamed response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988ae81",
   "metadata": {},
   "source": [
    "## Try It Out!\n",
    "\n",
    "Now let's ask some questions. The quality of answers will depend on which articles were included in your dataset sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"Tell me about Albert Einstein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"What is Python programming language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "climate_question",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"How does photosynthesis work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370c0c5",
   "metadata": {},
   "source": [
    "## Interactive Chat\n",
    "\n",
    "You can also use this cell to ask your own questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask your own question here\n",
    "your_question = \"What is the solar system?\"\n",
    "ask_question(your_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_cell",
   "metadata": {},
   "source": [
    "## Export Dataset for Other Platforms\n",
    "\n",
    "You can export the dataset for use with Neon (Vercel) or Cloudflare D1 with Vectorize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_for_vectorize(output_path='wikipedia_export.json'):\n",
    "    \"\"\"Export dataset in a format ready for Cloudflare Vectorize or Neon.\n",
    "    \n",
    "    The output format includes:\n",
    "    - id: unique identifier\n",
    "    - text: the chunk content\n",
    "    - embedding: the vector (optional, can be generated on the platform)\n",
    "    \"\"\"\n",
    "    export_data = []\n",
    "    \n",
    "    for i, (chunk, embedding) in enumerate(VECTOR_DB):\n",
    "        # Extract title from chunk\n",
    "        lines = chunk.split('\\n')\n",
    "        title = lines[0].replace('Article: ', '') if lines[0].startswith('Article: ') else 'Unknown'\n",
    "        \n",
    "        export_data.append({\n",
    "            'id': f'chunk_{i}',\n",
    "            'text': chunk,\n",
    "            'title': title,\n",
    "            'embedding': embedding  # Include if you want pre-computed embeddings\n",
    "        })\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f'✓ Exported {len(export_data)} chunks to {output_path}')\n",
    "    print(f'  File size: {sys.getsizeof(json.dumps(export_data)) / (1024*1024):.2f} MB')\n",
    "    print('\\nYou can now use this file with:')\n",
    "    print('  - Neon (PostgreSQL with pgvector)')\n",
    "    print('  - Cloudflare D1 with Vectorize')\n",
    "    print('  - Any other vector database')\n",
    "\n",
    "# Uncomment to export:\n",
    "# export_for_vectorize('wikipedia_vectorize_export.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77f5ee",
   "metadata": {},
   "source": [
    "## Load Embeddings from PostgreSQL\n",
    "\n",
    "If you've previously generated embeddings and stored them in PostgreSQL, you can load them without regenerating:\n",
    "\n",
    "**Use this in a new notebook to:**\n",
    "- Run experiments with existing embeddings (avoiding 50+ minute regeneration)\n",
    "- Compare different embedding models stored in different tables\n",
    "- Analyze embedding quality without reprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ef9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_from_postgres(config, embedding_model_alias):\n",
    "    \"\"\"Load previously generated embeddings from PostgreSQL.\n",
    "    \n",
    "    Useful for running new experiments without regenerating embeddings.\n",
    "    \n",
    "    Args:\n",
    "        config: PostgreSQL connection config\n",
    "        embedding_model_alias: Alias used when the embeddings were generated\n",
    "    \n",
    "    Returns:\n",
    "        PostgreSQLVectorDB instance ready for retrieval\n",
    "    \"\"\"\n",
    "    table_name = f'embeddings_{embedding_model_alias.replace(\".\", \"_\")}'\n",
    "    \n",
    "    try:\n",
    "        db = PostgreSQLVectorDB(config, table_name)\n",
    "        count = db.get_chunk_count()\n",
    "        print(f'✓ Loaded {count} embeddings from table \"{table_name}\"')\n",
    "        return db\n",
    "    except psycopg2.ProgrammingError:\n",
    "        print(f'✗ Table \"{table_name}\" not found in database')\n",
    "        print('Run the main notebook first to generate and store embeddings.')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'✗ Error loading embeddings: {e}')\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example: Uncomment to load existing embeddings in a new notebook\n",
    "# loaded_db = load_embeddings_from_postgres(POSTGRES_CONFIG, 'bge_base_en_v1.5')\n",
    "# Then use: loaded_db.similarity_search(query_embedding, top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2d324",
   "metadata": {},
   "source": [
    "## Next Steps and Improvements\n",
    "\n",
    "### Migrate to Production Vector Databases\n",
    "\n",
    "**Neon (with Vercel):**\n",
    "```sql\n",
    "-- Create table with pgvector\n",
    "CREATE TABLE wikipedia_chunks (\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  title TEXT,\n",
    "  text TEXT,\n",
    "  embedding vector(768)  -- dimension depends on your model\n",
    ");\n",
    "\n",
    "-- Create index for fast similarity search\n",
    "CREATE INDEX ON wikipedia_chunks \n",
    "USING ivfflat (embedding vector_cosine_ops);\n",
    "```\n",
    "\n",
    "**Cloudflare D1 with Vectorize:**\n",
    "```javascript\n",
    "// Use Vectorize for embeddings, D1 for metadata\n",
    "await env.VECTORIZE.insert([\n",
    "  {\n",
    "    id: 'chunk_1',\n",
    "    values: embedding,\n",
    "    metadata: { title: 'Article Title', text: 'chunk text' }\n",
    "  }\n",
    "]);\n",
    "```\n",
    "\n",
    "### Other Improvements\n",
    "\n",
    "1. **Hybrid Search**: Combine vector similarity with keyword search (BM25) for better retrieval\n",
    "\n",
    "2. **Reranking**: Use a [reranking model](https://www.pinecone.io/learn/series/rag/rerankers/) to re-score retrieved chunks\n",
    "\n",
    "3. **Query Expansion**: Generate multiple variations of the user's question for better coverage\n",
    "\n",
    "4. **Metadata Filtering**: Filter by article categories, dates, or other metadata before similarity search\n",
    "\n",
    "5. **Better Chunking**: Implement semantic chunking that preserves context better\n",
    "\n",
    "6. **Citation Support**: Track which chunks were used and provide Wikipedia URLs as sources\n",
    "\n",
    "### Advanced RAG Architectures\n",
    "\n",
    "- **Graph RAG**: Build knowledge graphs from Wikipedia's link structure\n",
    "- **Hybrid RAG**: Combine vectors, graphs, and keyword search\n",
    "- **Agentic RAG**: Let the LLM decide when to retrieve more information\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "- **Batch Embeddings**: Embed multiple chunks at once for faster indexing\n",
    "- **Approximate Search**: Use FAISS, Annoy, or HNSW for faster similarity search\n",
    "- **Caching**: Cache frequent queries and their results\n",
    "\n",
    "Learn more about RAG patterns in the [HuggingFace RAG guide](https://huggingface.co/blog/ngxson/make-your-own-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_section",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "View statistics about your loaded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats():\n",
    "    \"\"\"Print statistics about the current dataset.\"\"\"\n",
    "    total_chars = sum(len(chunk) for chunk in dataset)\n",
    "    avg_chunk_size = total_chars / len(dataset) if dataset else 0\n",
    "    \n",
    "    # Count unique articles\n",
    "    articles = set()\n",
    "    for chunk in dataset:\n",
    "        if chunk.startswith('Article: '):\n",
    "            title = chunk.split('\\n')[0].replace('Article: ', '')\n",
    "            articles.add(title)\n",
    "    \n",
    "    print('Dataset Statistics:')\n",
    "    print(f'  Total chunks: {len(dataset):,}')\n",
    "    print(f'  Unique articles: {len(articles):,}')\n",
    "    print(f'  Total characters: {total_chars:,}')\n",
    "    print(f'  Average chunk size: {avg_chunk_size:.0f} characters')\n",
    "    print(f'  Estimated size: {sys.getsizeof(str(dataset)) / (1024*1024):.2f} MB')\n",
    "    print(f'\\n  Embeddings in database: {len(VECTOR_DB):,}')\n",
    "    print(f'  Embedding dimension: {len(VECTOR_DB[0][1]) if VECTOR_DB else 0}')\n",
    "\n",
    "print_dataset_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
